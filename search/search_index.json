{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ZeroIPC - Active Computational Substrate for Shared Memory","text":"High-Performance Cross-Language IPC <p>Zero-copy data sharing between processes in C, C++, and Python</p>"},{"location":"#what-is-zeroipc","title":"What is ZeroIPC?","text":"<p>ZeroIPC transforms shared memory from passive storage into an active computational substrate, enabling both imperative and functional programming paradigms across process boundaries. It provides zero-copy data sharing with sophisticated concurrency primitives, reactive streams, and codata structures\u2014bringing modern programming abstractions to inter-process communication.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Zero-Copy Performance - Direct memory access without serialization overhead</li> <li>Language Independence - True parallel implementations in C, C++, and Python (not bindings!)</li> <li>Lock-Free Concurrency - Atomic operations and CAS-based algorithms for high-performance synchronization</li> <li>Minimal Metadata - Only store name/offset/size for maximum flexibility</li> <li>Duck Typing - Runtime type specification (Python) or compile-time templates (C++)</li> <li>Simple Discovery - Named structures for easy cross-process lookup</li> <li>Reactive Programming - Functional reactive streams with map, filter, fold operators</li> <li>Codata Support - Futures, lazy evaluation, and infinite streams</li> <li>CSP Concurrency - Channels for synchronous message passing</li> <li>Comprehensive CLI Tools - Virtual filesystem interface for inspection and debugging</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":""},{"location":"#c-producer","title":"C++ Producer","text":"<pre><code>#include &lt;zeroipc/memory.h&gt;\n#include &lt;zeroipc/array.h&gt;\n\n// Create shared memory segment\nzeroipc::Memory mem(\"/sensor_data\", 10*1024*1024);  // 10MB\n\n// Create typed array\nzeroipc::Array&lt;float&gt; temps(mem, \"temperature\", 1000);\ntemps[0] = 23.5f;\ntemps[1] = 24.1f;\n</code></pre>"},{"location":"#python-consumer","title":"Python Consumer","text":"<pre><code>from zeroipc import Memory, Array\nimport numpy as np\n\n# Open same shared memory segment\nmem = Memory(\"/sensor_data\")\n\n# Read with duck typing - user specifies type\ntemps = Array(mem, \"temperature\", dtype=np.float32)\nprint(temps[0])  # 23.5\nprint(temps[1])  # 24.1\n</code></pre>"},{"location":"#why-zeroipc","title":"Why ZeroIPC?","text":"<p>Traditional IPC mechanisms force you to choose between performance and ease of use:</p> <ul> <li>Sockets/Pipes: Easy to use but slow (serialization overhead)</li> <li>Message Queues: Safe but limited (fixed message sizes)</li> <li>Raw Shared Memory: Fast but difficult (manual synchronization, no type safety)</li> </ul> <p>ZeroIPC gives you all three: performance, safety, and ease of use.</p>"},{"location":"#performance-comparison","title":"Performance Comparison","text":"Operation Socket Message Queue ZeroIPC Array Access ~10 \u03bcs ~5 \u03bcs ~10 ns Queue Push/Pop ~8 \u03bcs ~3 \u03bcs ~50 ns 1MB Transfer ~500 \u03bcs ~300 \u03bcs ~0 ns <p>Zero-copy means truly zero overhead</p>"},{"location":"#data-structures","title":"Data Structures","text":""},{"location":"#traditional-structures","title":"Traditional Structures","text":"<ul> <li>Array - Fixed-size contiguous storage with atomic operations</li> <li>Queue - Lock-free MPMC circular buffer using CAS</li> <li>Stack - Lock-free LIFO with ABA-safe operations</li> <li>Map - Lock-free hash map with linear probing</li> <li>Set - Lock-free hash set for unique elements</li> <li>Pool - Object pool with free list management</li> <li>Ring - High-performance ring buffer for streaming</li> </ul>"},{"location":"#synchronization-primitives","title":"Synchronization Primitives","text":"<ul> <li>Semaphore - Cross-process counting/binary semaphore</li> <li>Barrier - Multi-process synchronization barrier</li> <li>Latch - One-shot countdown synchronization</li> </ul>"},{"location":"#codata-computational-structures","title":"Codata &amp; Computational Structures","text":"<ul> <li>Future - Asynchronous computation results across processes</li> <li>Lazy - Deferred computations with automatic memoization</li> <li>Stream - Reactive data flows with FRP operators</li> <li>Channel - CSP-style synchronous/buffered message passing</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<p>ZeroIPC excels at:</p> <ul> <li>High-frequency sensor data sharing</li> <li>Multi-process simulations and scientific computing</li> <li>Real-time analytics pipelines</li> <li>Cross-language data processing</li> <li>Zero-copy producer-consumer patterns</li> <li>Reactive event processing</li> </ul>"},{"location":"#architecture-highlights","title":"Architecture Highlights","text":""},{"location":"#binary-format-specification","title":"Binary Format Specification","text":"<p>All language implementations follow the same binary format:</p> <pre><code>[Table Header][Table Entries][Data Structure 1][Data Structure 2]...\n</code></pre> <ul> <li>Table Header: Magic number, version, entry count, next offset</li> <li>Table Entry: Name (32 bytes), offset (4 bytes), size (4 bytes)</li> <li>Data Structures: Raw binary data, layout determined by structure type</li> </ul>"},{"location":"#language-equality","title":"Language Equality","text":"<p>Unlike traditional IPC libraries where one language is \"primary\":</p> <ul> <li>Both languages can create - Python and C++ can both allocate new structures</li> <li>Both languages can read - Either can discover and access existing structures</li> <li>Type safety per language - C++ uses templates, Python uses NumPy dtypes</li> <li>No bindings needed - Each implementation stands alone</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Here's your roadmap:</p> <ol> <li>Installation - Get ZeroIPC up and running</li> <li>Quick Start - Your first shared memory program in 5 minutes</li> <li>Tutorial - Step-by-step guide to all features</li> <li>CLI Tool - Learn the virtual filesystem interface</li> </ol>"},{"location":"#project-status","title":"Project Status","text":"<p>ZeroIPC is actively developed and production-ready:</p> <ul> <li>Current Version: 2.0</li> <li>Stability: Stable API, comprehensive test suite</li> <li>Performance: 200x test suite optimization (20 min \u2192 2 min)</li> <li>Coverage: All core structures implemented across C, C++, and Python</li> <li>Testing: Fast/Medium/Slow/Stress test categorization with CTest labels</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub: Report issues and contribute</li> <li>Documentation: You're reading it!</li> <li>Examples: See the examples directory</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - See LICENSE file for details</p> <p>Transform your IPC from simple message passing to active computation</p> <p>Get Started \u2192</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/","title":"TDD Best Practices for Lock-Free Concurrent Systems","text":""},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#introduction","title":"Introduction","text":"<p>Lock-free data structures present unique testing challenges. Traditional unit testing approaches fail to catch race conditions, memory ordering bugs, and ABA problems. This guide provides proven strategies for test-driven development of lock-free systems like ZeroIPC.</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#the-testing-pyramid-for-lock-free-code","title":"The Testing Pyramid for Lock-Free Code","text":"<pre><code>         /\\\n        /  \\  E2E Tests (Few)\n       /____\\  - Cross-process scenarios\n      /      \\  - Full system validation\n     /________\\ Integration Tests (Some)\n    /          \\ - Multi-threaded stress\n   /____________\\ - Concurrent invariants\n  /              \\ Unit Tests (Many)\n /________________\\ - Single-threaded correctness\n                    - Algorithm validation\n</code></pre>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#layer-1-deterministic-functional-tests-70","title":"Layer 1: Deterministic Functional Tests (70%)","text":"<p>Purpose: Verify algorithm correctness without concurrency</p> <p>Why: Lock-free bugs are hard to debug. Ensure basic correctness first.</p> <p>Example: <pre><code>TEST_F(QueueTest, FIFOOrdering) {\n    Queue&lt;int&gt; q(mem, \"test\", 100);\n\n    // Single-threaded - deterministic\n    for (int i = 0; i &lt; 50; i++) {\n        ASSERT_TRUE(q.push(i));\n    }\n\n    for (int i = 0; i &lt; 50; i++) {\n        auto val = q.pop();\n        ASSERT_TRUE(val.has_value());\n        EXPECT_EQ(*val, i) &lt;&lt; \"FIFO order violated at index \" &lt;&lt; i;\n    }\n}\n</code></pre></p> <p>What to test: - Basic operations (push, pop, empty, full) - Boundary conditions (empty, full, wrap-around) - Edge cases (capacity-1, capacity, capacity+1) - Error handling (invalid arguments, null pointers)</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#layer-2-invariant-based-concurrent-tests-20","title":"Layer 2: Invariant-Based Concurrent Tests (20%)","text":"<p>Purpose: Verify correctness properties hold under concurrency</p> <p>Why: Lock-free algorithms maintain specific invariants that must survive all thread interleavings.</p> <p>Example: <pre><code>TEST_F(QueueTest, MPMCConservation) {\n    Queue&lt;int&gt; q(mem, \"test\", 1000);\n\n    std::atomic&lt;int&gt; sum_produced{0};\n    std::atomic&lt;int&gt; sum_consumed{0};\n    const int threads = 8;\n    const int items_per_thread = 1000;\n\n    auto producer = [&amp;](int thread_id) {\n        int local_sum = 0;\n        for (int i = 0; i &lt; items_per_thread; i++) {\n            int value = thread_id * items_per_thread + i;\n            while (!q.push(value)) std::this_thread::yield();\n            local_sum += value;\n        }\n        sum_produced.fetch_add(local_sum);\n    };\n\n    auto consumer = [&amp;]() {\n        int local_sum = 0;\n        for (int i = 0; i &lt; items_per_thread; i++) {\n            while (true) {\n                auto val = q.pop();\n                if (val.has_value()) {\n                    local_sum += *val;\n                    break;\n                }\n                std::this_thread::yield();\n            }\n        }\n        sum_consumed.fetch_add(local_sum);\n    };\n\n    // Half producers, half consumers\n    std::vector&lt;std::thread&gt; threads_vec;\n    for (int i = 0; i &lt; threads/2; i++) {\n        threads_vec.emplace_back(producer, i);\n    }\n    for (int i = 0; i &lt; threads/2; i++) {\n        threads_vec.emplace_back(consumer);\n    }\n\n    for (auto&amp; t : threads_vec) t.join();\n\n    // Invariant: Conservation of values\n    EXPECT_EQ(sum_produced.load(), sum_consumed.load())\n        &lt;&lt; \"Values were lost or duplicated\";\n    EXPECT_TRUE(q.empty()) &lt;&lt; \"Queue should be empty after all operations\";\n}\n</code></pre></p> <p>Invariants to verify: - Conservation: produced == consumed - No duplication: Each value appears exactly once - No loss: All values accounted for - Consistency: State variables are coherent (e.g., empty() \u27fa size()==0)</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#layer-3-stress-tests-10","title":"Layer 3: Stress Tests (10%)","text":"<p>Purpose: Force race conditions to surface through sustained high contention</p> <p>Why: Some race conditions only appear under extreme load.</p> <p>Example: <pre><code>TEST_F(QueueStressTest, HighContentionSmallQueue) {\n    // Small queue + many threads = maximum contention\n    Queue&lt;int&gt; q(mem, \"test\", 10);  // Only 10 slots\n\n    const int threads = 32;  // Way more threads than slots\n    const int iterations = 10000;\n    std::atomic&lt;int&gt; produced{0};\n    std::atomic&lt;int&gt; consumed{0};\n\n    auto producer = [&amp;]() {\n        for (int i = 0; i &lt; iterations; i++) {\n            while (!q.push(i)) {\n                std::this_thread::yield();\n                // This loop will spin frequently due to contention\n            }\n            produced.fetch_add(1);\n        }\n    };\n\n    auto consumer = [&amp;]() {\n        for (int i = 0; i &lt; iterations; i++) {\n            while (!q.pop().has_value()) {\n                std::this_thread::yield();\n            }\n            consumed.fetch_add(1);\n        }\n    };\n\n    std::vector&lt;std::thread&gt; threads_vec;\n    for (int i = 0; i &lt; threads/2; i++) {\n        threads_vec.emplace_back(producer);\n        threads_vec.emplace_back(consumer);\n    }\n\n    for (auto&amp; t : threads_vec) t.join();\n\n    int expected = (threads/2) * iterations;\n    EXPECT_EQ(produced.load(), expected);\n    EXPECT_EQ(consumed.load(), expected);\n}\n</code></pre></p> <p>Key technique: queue_size &lt;&lt; num_threads forces CAS failures</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#common-lock-free-bug-patterns-how-to-test","title":"Common Lock-Free Bug Patterns &amp; How to Test","text":""},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#1-aba-problem","title":"1. ABA Problem","text":"<p>What it is: CAS succeeds when it shouldn't because value wrapped around</p> <pre><code>Thread 1: Read A\nThread 2: Change A\u2192B\u2192A\nThread 1: CAS(A, new) succeeds incorrectly\n</code></pre> <p>How to test: <pre><code>TEST_F(ABATest, RapidRecycling) {\n    Stack&lt;int&gt; s(mem, \"test\", 100);\n\n    // Rapidly push/pop same values to reuse memory locations\n    auto worker = [&amp;]() {\n        for (int gen = 0; gen &lt; 10000; gen++) {\n            s.push(42);\n            auto val = s.pop();\n            // If ABA problem exists, might see:\n            // - Corruption\n            // - Lost updates\n            // - Unexpected empty states\n            ASSERT_TRUE(val.has_value());\n            EXPECT_EQ(*val, 42);\n        }\n    };\n\n    std::vector&lt;std::thread&gt; threads;\n    for (int i = 0; i &lt; 8; i++) {\n        threads.emplace_back(worker);\n    }\n    for (auto&amp; t : threads) t.join();\n}\n</code></pre></p> <p>What to look for: - Segfaults (accessing freed memory) - Corrupted values - Lost items - Unexpected empty/full states</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#2-memory-ordering-bugs","title":"2. Memory Ordering Bugs","text":"<p>What it is: Insufficient fences allow torn reads or stale values</p> <pre><code>// WRONG: Missing fence\ntail.store(new_tail, std::memory_order_relaxed);\ndata[old_tail] = value;  // Might be reordered before tail update!\n</code></pre> <p>How to test: <pre><code>TEST_F(MemoryOrderingTest, NoTornReads) {\n    struct Pair { int x, y; };\n    Queue&lt;Pair&gt; q(mem, \"test\", 100);\n\n    auto producer = [&amp;]() {\n        for (int i = 0; i &lt; 10000; i++) {\n            q.push({i, i});  // Always matching values\n        }\n    };\n\n    auto consumer = [&amp;]() {\n        for (int i = 0; i &lt; 10000; i++) {\n            auto val = q.pop();\n            while (!val.has_value()) {\n                std::this_thread::yield();\n                val = q.pop();\n            }\n            // If memory ordering is wrong, might see {5, 7}\n            EXPECT_EQ(val-&gt;x, val-&gt;y)\n                &lt;&lt; \"Torn read: x=\" &lt;&lt; val-&gt;x &lt;&lt; \", y=\" &lt;&lt; val-&gt;y;\n        }\n    };\n\n    std::thread p(producer);\n    std::thread c(consumer);\n    p.join();\n    c.join();\n}\n</code></pre></p> <p>Use ThreadSanitizer: <pre><code>cmake -DCMAKE_CXX_FLAGS=\"-fsanitize=thread -g\" -B build .\ncd build &amp;&amp; ctest\n</code></pre></p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#3-lost-updates","title":"3. Lost Updates","text":"<p>What it is: Concurrent operations clobber each other's changes</p> <p>How to test: <pre><code>TEST_F(QueueTest, NoDuplicatesOrLoss) {\n    Queue&lt;int&gt; q(mem, \"test\", 1000);\n\n    std::set&lt;int&gt; expected_values;\n    std::mutex set_mutex;\n\n    auto producer = [&amp;](int thread_id) {\n        for (int i = 0; i &lt; 1000; i++) {\n            int value = thread_id * 1000 + i;\n            {\n                std::lock_guard&lt;std::mutex&gt; lock(set_mutex);\n                expected_values.insert(value);\n            }\n            while (!q.push(value)) std::this_thread::yield();\n        }\n    };\n\n    std::vector&lt;int&gt; consumed_values;\n    std::mutex vec_mutex;\n\n    auto consumer = [&amp;]() {\n        for (int i = 0; i &lt; 1000; i++) {\n            auto val = q.pop();\n            while (!val.has_value()) {\n                std::this_thread::yield();\n                val = q.pop();\n            }\n            std::lock_guard&lt;std::mutex&gt; lock(vec_mutex);\n            consumed_values.push_back(*val);\n        }\n    };\n\n    std::thread p1(producer, 0);\n    std::thread p2(producer, 1);\n    std::thread c1(consumer);\n    std::thread c2(consumer);\n\n    p1.join(); p2.join(); c1.join(); c2.join();\n\n    // Convert consumed to set\n    std::set&lt;int&gt; consumed_set(consumed_values.begin(), consumed_values.end());\n\n    // Check for duplicates\n    EXPECT_EQ(consumed_values.size(), consumed_set.size())\n        &lt;&lt; \"Duplicates detected\";\n\n    // Check for loss\n    EXPECT_EQ(expected_values, consumed_set)\n        &lt;&lt; \"Some values were lost\";\n}\n</code></pre></p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#4-livelock-starvation","title":"4. Livelock / Starvation","text":"<p>What it is: Threads keep retrying but never make progress</p> <p>How to test: <pre><code>TEST_F(QueueTest, NoLivelock) {\n    Queue&lt;int&gt; q(mem, \"test\", 100);\n\n    std::atomic&lt;bool&gt; timeout{false};\n\n    auto producer = [&amp;]() {\n        auto start = std::chrono::steady_clock::now();\n        for (int i = 0; i &lt; 1000; i++) {\n            while (!q.push(i)) {\n                std::this_thread::yield();\n\n                auto elapsed = std::chrono::steady_clock::now() - start;\n                if (elapsed &gt; 5s) {\n                    timeout.store(true);\n                    return;\n                }\n            }\n        }\n    };\n\n    std::thread p(producer);\n    p.join();\n\n    EXPECT_FALSE(timeout.load())\n        &lt;&lt; \"Producer timed out - possible livelock\";\n}\n</code></pre></p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#testing-anti-patterns-to-avoid","title":"Testing Anti-Patterns to Avoid","text":""},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#testing-implementation-details","title":"\u274c Testing Implementation Details","text":"<pre><code>// BAD: Testing internal state\nTEST_F(QueueTest, InternalIndexValues) {\n    Queue&lt;int&gt; q(mem, \"test\", 10);\n    q.push(42);\n    EXPECT_EQ(q._head_index, 0);  // Implementation detail!\n    EXPECT_EQ(q._tail_index, 1);\n}\n</code></pre> <p>Why bad: Breaks when you refactor, even if behavior is correct.</p> <p>Better: <pre><code>TEST_F(QueueTest, SizeAfterOperations) {\n    Queue&lt;int&gt; q(mem, \"test\", 10);\n    EXPECT_EQ(q.size(), 0);\n    q.push(42);\n    EXPECT_EQ(q.size(), 1);\n    q.pop();\n    EXPECT_EQ(q.size(), 0);\n}\n</code></pre></p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#timing-dependent-tests","title":"\u274c Timing-Dependent Tests","text":"<pre><code>// BAD: Assumes specific thread scheduling\nTEST_F(QueueTest, ThreadsRunInOrder) {\n    std::atomic&lt;int&gt; counter{0};\n\n    std::thread t1([&amp;]() {\n        counter.store(1);\n    });\n\n    std::this_thread::sleep_for(10ms);  // Assume t1 runs first\n\n    EXPECT_EQ(counter.load(), 1);  // FLAKY!\n    t1.join();\n}\n</code></pre> <p>Why bad: Nondeterministic - fails randomly based on OS scheduler.</p> <p>Better: <pre><code>TEST_F(QueueTest, ThreadEventuallyCompletes) {\n    std::atomic&lt;bool&gt; completed{false};\n\n    std::thread t1([&amp;]() {\n        completed.store(true);\n    });\n\n    t1.join();  // Synchronize properly\n    EXPECT_TRUE(completed.load());\n}\n</code></pre></p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#testing-exact-cas-failure-counts","title":"\u274c Testing Exact CAS Failure Counts","text":"<pre><code>// BAD: CAS failures are nondeterministic\nTEST_F(QueueTest, CASFailureRate) {\n    int cas_failures = run_concurrent_test();\n    EXPECT_EQ(cas_failures, 42);  // Will never be stable\n}\n</code></pre> <p>Why bad: Number of CAS failures depends on scheduling, CPU load, etc.</p> <p>Better: Test invariants, not implementation metrics.</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#test-organization-strategies","title":"Test Organization Strategies","text":""},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#strategy-1-separate-fastslow-tests","title":"Strategy 1: Separate Fast/Slow Tests","text":"<pre><code>// Fast tests - deterministic, &lt;100ms\nclass QueueTest : public ::testing::Test { };\n\nTEST_F(QueueTest, BasicPushPop) { ... }\nTEST_F(QueueTest, EmptyBehavior) { ... }\n\n// Slow tests - concurrent, &gt;1s\nclass QueueStressTest : public ::testing::Test { };\n\nTEST_F(QueueStressTest, HighContention) { ... }\nTEST_F(QueueStressTest, SustainedLoad) { ... }\n</code></pre>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#strategy-2-parameterized-thread-counts","title":"Strategy 2: Parameterized Thread Counts","text":"<pre><code>class QueueConcurrentTest : public ::testing::TestWithParam&lt;int&gt; {\n    // Param = number of threads\n};\n\nTEST_P(QueueConcurrentTest, MPMCCorrectness) {\n    int threads = GetParam();\n    // ... test with 'threads' threads\n}\n\nINSTANTIATE_TEST_SUITE_P(\n    ThreadCount,\n    QueueConcurrentTest,\n    ::testing::Values(2, 4, 8, 16)  // Test different concurrency levels\n);\n</code></pre>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#strategy-3-separate-test-executables","title":"Strategy 3: Separate Test Executables","text":"<pre><code>tests/\n\u251c\u2500\u2500 test_queue_unit.cpp        # Fast, deterministic\n\u251c\u2500\u2500 test_queue_concurrent.cpp  # Medium, multi-threaded\n\u2514\u2500\u2500 test_queue_stress.cpp      # Slow, exhaustive\n</code></pre> <p>CMake: <pre><code>add_executable(test_queue_unit tests/test_queue_unit.cpp)\nset_tests_properties(test_queue_unit PROPERTIES LABELS \"fast;unit\")\n\nadd_executable(test_queue_stress tests/test_queue_stress.cpp)\nset_tests_properties(test_queue_stress PROPERTIES\n    LABELS \"stress\"\n    DISABLED TRUE)\n</code></pre></p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#coverage-metrics-for-lock-free-code","title":"Coverage Metrics for Lock-Free Code","text":""},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#traditional-code-coverage-necessary-but-insufficient","title":"Traditional Code Coverage (Necessary but Insufficient)","text":"<pre><code># Line coverage\nlcov --capture --directory build --output-file coverage.info\n\n# Branch coverage\ngcov -b queue.cpp\n</code></pre> <p>Target: 85%+ line coverage on core algorithms</p> <p>But: 100% line coverage doesn't catch race conditions!</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#concurrency-coverage-essential","title":"Concurrency Coverage (Essential)","text":"<p>ThreadSanitizer: <pre><code>cmake -DCMAKE_CXX_FLAGS=\"-fsanitize=thread -g -O1\" -B build .\ncd build &amp;&amp; ctest\n</code></pre></p> <p>Detects: - Data races - Missing synchronization - Lock order violations</p> <p>Helgrind (Valgrind): <pre><code>valgrind --tool=helgrind ./test_queue\n</code></pre></p> <p>Detects: - Race conditions - Incorrect lock usage - Memory ordering issues</p> <p>Stress Testing Duration: Run tests for extended periods: <pre><code># Run for 1 hour\ntimeout 3600 ./test_queue_stress --gtest_repeat=10000\n</code></pre></p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#multi-dimensional-coverage-checklist","title":"Multi-Dimensional Coverage Checklist","text":"<ul> <li> Line coverage: 85%+ on core code</li> <li> Branch coverage: 80%+ on error paths</li> <li> Thread interleaving: Tested with 2, 4, 8, 16+ threads</li> <li> Load patterns: Tested with equal/unequal producer/consumer ratios</li> <li> Queue sizes: Tested with small (10), medium (1000), large (100K) capacities</li> <li> Data types: Tested with int, struct, large objects</li> <li> Platform coverage: Tested on Linux, macOS (if applicable)</li> <li> Sanitizer runs: All tests pass under ThreadSanitizer</li> <li> Extended duration: Stress tests run for 1+ hours without failures</li> </ul>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#tdd-workflow-for-lock-free-algorithms","title":"TDD Workflow for Lock-Free Algorithms","text":""},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#step-1-design-the-interface-red","title":"Step 1: Design the Interface (Red)","text":"<pre><code>// Write test first - interface doesn't exist yet\nTEST_F(QueueTest, BasicInterface) {\n    Queue&lt;int&gt; q(mem, \"test\", 100);\n\n    EXPECT_TRUE(q.empty());\n    EXPECT_FALSE(q.full());\n\n    q.push(42);\n    EXPECT_FALSE(q.empty());\n\n    auto val = q.pop();\n    ASSERT_TRUE(val.has_value());\n    EXPECT_EQ(*val, 42);\n}\n</code></pre>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#step-2-implement-single-threaded-version-green","title":"Step 2: Implement Single-Threaded Version (Green)","text":"<pre><code>// Simplest possible implementation\ntemplate&lt;typename T&gt;\nclass Queue {\n    T* data;\n    size_t head = 0, tail = 0;\n\npublic:\n    bool push(const T&amp; value) {\n        if (full()) return false;\n        data[tail] = value;\n        tail = (tail + 1) % capacity;\n        return true;\n    }\n\n    std::optional&lt;T&gt; pop() {\n        if (empty()) return std::nullopt;\n        T value = data[head];\n        head = (head + 1) % capacity;\n        return value;\n    }\n};\n</code></pre> <p>Test passes! \u2705</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#step-3-add-concurrent-test-red","title":"Step 3: Add Concurrent Test (Red)","text":"<pre><code>TEST_F(QueueTest, BasicConcurrency) {\n    Queue&lt;int&gt; q(mem, \"test\", 100);\n\n    std::thread producer([&amp;]() {\n        for (int i = 0; i &lt; 50; i++) {\n            q.push(i);\n        }\n    });\n\n    std::thread consumer([&amp;]() {\n        for (int i = 0; i &lt; 50; i++) {\n            while (!q.pop().has_value()) {\n                std::this_thread::yield();\n            }\n        }\n    });\n\n    producer.join();\n    consumer.join();\n\n    EXPECT_TRUE(q.empty());\n}\n</code></pre> <p>Test fails! \u274c Data race detected</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#step-4-make-thread-safe-green","title":"Step 4: Make Thread-Safe (Green)","text":"<pre><code>template&lt;typename T&gt;\nclass Queue {\n    T* data;\n    std::atomic&lt;size_t&gt; head{0}, tail{0};  // Now atomic\n\npublic:\n    bool push(const T&amp; value) {\n        size_t current_tail = tail.load(std::memory_order_relaxed);\n        size_t next_tail = (current_tail + 1) % capacity;\n\n        if (next_tail == head.load(std::memory_order_acquire)) {\n            return false;  // Full\n        }\n\n        data[current_tail] = value;\n        std::atomic_thread_fence(std::memory_order_release);\n        tail.store(next_tail, std::memory_order_relaxed);\n        return true;\n    }\n    // ... similar for pop()\n};\n</code></pre> <p>Test passes! \u2705 (but not lock-free yet)</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#step-5-refactor-to-lock-free-refactor","title":"Step 5: Refactor to Lock-Free (Refactor)","text":"<pre><code>bool push(const T&amp; value) {\n    size_t current_tail, next_tail;\n\n    do {\n        current_tail = tail.load(std::memory_order_relaxed);\n        next_tail = (current_tail + 1) % capacity;\n\n        if (next_tail == head.load(std::memory_order_acquire)) {\n            return false;  // Full\n        }\n    } while (!tail.compare_exchange_weak(\n        current_tail, next_tail,\n        std::memory_order_release,\n        std::memory_order_relaxed\n    ));\n\n    data[current_tail] = value;\n    return true;\n}\n</code></pre> <p>Tests still pass! \u2705 Now lock-free with CAS</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#step-6-add-stress-tests-red-green","title":"Step 6: Add Stress Tests (Red \u2192 Green)","text":"<p>Add progressively harder tests: 1. MPMC with 4 threads 2. MPMC with 16 threads 3. High contention (small queue, many threads) 4. Sustained load (millions of ops) 5. ABA detection</p> <p>Fix bugs as they appear, keeping all previous tests green.</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#practical-tips","title":"Practical Tips","text":""},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#1-use-assertions-liberally","title":"1. Use Assertions Liberally","text":"<pre><code>auto val = q.pop();\nASSERT_TRUE(val.has_value()) &lt;&lt; \"Pop failed when queue should have data\";\nEXPECT_EQ(*val, expected_value) &lt;&lt; \"Got \" &lt;&lt; *val &lt;&lt; \" expected \" &lt;&lt; expected_value;\n</code></pre> <p>Better error messages = faster debugging.</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#2-isolate-tests","title":"2. Isolate Tests","text":"<pre><code>void SetUp() override {\n    shm_name_ = \"/test_\" + std::to_string(getpid()) + \"_\" +\n                std::to_string(std::chrono::steady_clock::now().time_since_epoch().count());\n}\n</code></pre> <p>Prevent tests from interfering with each other.</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#3-test-cleanup","title":"3. Test Cleanup","text":"<pre><code>void TearDown() override {\n    Memory::unlink(shm_name_);\n}\n</code></pre> <p>Don't leave shared memory segments around.</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#4-use-timeouts","title":"4. Use Timeouts","text":"<pre><code>set_tests_properties(queue_stress PROPERTIES TIMEOUT 300)\n</code></pre> <p>Prevent infinite loops from hanging CI.</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#5-run-tests-in-parallel","title":"5. Run Tests in Parallel","text":"<pre><code>ctest -j$(nproc)\n</code></pre> <p>But be careful - concurrent tests might interfere if they share resources.</p>"},{"location":"TDD_BEST_PRACTICES_LOCKFREE/#summary","title":"Summary","text":"<p>Lock-free testing is fundamentally different from traditional testing:</p> <ol> <li>Start simple - Verify single-threaded correctness first</li> <li>Test invariants - Not implementation details</li> <li>Use multiple thread counts - Race conditions vary with concurrency</li> <li>Maximize contention - Small queues + many threads</li> <li>Run for duration - Some bugs only appear after millions of operations</li> <li>Use sanitizers - ThreadSanitizer catches what coverage misses</li> <li>Separate fast/slow tests - Keep CI fast, run stress tests nightly</li> <li>Expect nondeterminism - Test properties, not exact timings</li> <li>Keep tests green - Flaky tests are worse than no tests</li> </ol> <p>Remember: Lock-free code is hard. Tests won't catch everything. But a comprehensive test suite gives you confidence to refactor and evolve your design.</p>"},{"location":"TESTING_STRATEGY/","title":"ZeroIPC Testing Strategy","text":""},{"location":"TESTING_STRATEGY/#overview","title":"Overview","text":"<p>This document outlines the comprehensive testing strategy for ZeroIPC, addressing performance, coverage, and test-driven development best practices for lock-free concurrent systems.</p>"},{"location":"TESTING_STRATEGY/#test-performance-summary","title":"Test Performance Summary","text":"<p>Before Optimization: 20+ minutes for full suite After Optimization: - Fast tests only: ~5.8 seconds (test_fast target) - Default suite (fast + medium): ~2 minutes (test_default target) - CI suite (all except stress): ~10 minutes (test_ci target) - Full suite (including stress): ~30 minutes (test_all target)</p> <p>Performance Improvement: Over 200x speedup for development workflows</p>"},{"location":"TESTING_STRATEGY/#key-changes","title":"Key Changes","text":"<ol> <li>Parameterized timing constants - Reduced sleep/delay times by 10-50x</li> <li>Test categorization - FAST/MEDIUM/SLOW/STRESS with selective execution</li> <li>CTest labels - Enables targeted test runs</li> <li>Disabled slow tests - Run only when explicitly requested</li> </ol>"},{"location":"TESTING_STRATEGY/#test-categories","title":"Test Categories","text":""},{"location":"TESTING_STRATEGY/#fast-tests-100ms-each","title":"FAST Tests (&lt;100ms each)","text":"<p>Purpose: Core functionality validation, run on every commit Characteristics: - No intentional delays - Minimal threading (if any) - Unit-level testing - Single-process</p> <p>Examples: - Table operations (create, find, remove) - Array CRUD operations - Queue/Stack basic push/pop - Memory allocation/deallocation - Error handling (invalid arguments, boundary conditions)</p> <p>Run with: <pre><code>ctest -L fast --output-on-failure\n# or\ncmake --build build --target test_fast\n</code></pre></p>"},{"location":"TESTING_STRATEGY/#medium-tests-5s-each","title":"MEDIUM Tests (&lt;5s each)","text":"<p>Purpose: Integration and concurrency validation Characteristics: - Multi-threaded (4-8 threads) - Minimal intentional delays (1-10ms total) - Lock-free algorithm correctness - Component interaction</p> <p>Examples: - MPMC queue with 4 producers/consumers - Lock-free stack high contention - Semaphore mutual exclusion (fast variant) - Barrier synchronization (2-4 threads)</p> <p>Run with: <pre><code>ctest -L medium --output-on-failure\n# or\ncmake --build build --target test_medium\n</code></pre></p>"},{"location":"TESTING_STRATEGY/#slow-tests-5s-each","title":"SLOW Tests (&gt;5s each)","text":"<p>Purpose: Full synchronization validation Characteristics: - Real-world timing scenarios - Multiple process coordination - Timeout behavior validation - Large-scale thread orchestration</p> <p>Examples: - Semaphore with 50+ waiting threads - Barrier with intentional delays - Timeout edge cases (wait_for variants) - Process crash recovery</p> <p>Run with: <pre><code>ctest -L slow --output-on-failure\n# or\ncmake --build build --target test_slow\n</code></pre></p> <p>Note: Disabled by default. Enable with: <pre><code>ctest -C Release -LE disabled\n</code></pre></p>"},{"location":"TESTING_STRATEGY/#stress-tests-30s-each","title":"STRESS Tests (&gt;30s each)","text":"<p>Purpose: Exhaustive validation, performance profiling Characteristics: - 32+ threads - Millions of operations - ABA problem detection - Memory boundary stress - Long-running scenarios</p> <p>Examples: - Queue with 1M+ operations - ABA problem simulation - Memory boundary fuzzing - Sustained high contention</p> <p>Run with: <pre><code>cmake --build build --target test_stress_all\n</code></pre></p>"},{"location":"TESTING_STRATEGY/#test-organization","title":"Test Organization","text":""},{"location":"TESTING_STRATEGY/#file-structure","title":"File Structure","text":"<pre><code>cpp/tests/\n\u251c\u2500\u2500 test_config.h                    # Timing constants, test categories\n\u251c\u2500\u2500 test_*.cpp                       # Unit/integration tests\n\u251c\u2500\u2500 test_*_optimized.cpp            # Fast variants of slow tests\n\u251c\u2500\u2500 CMakeLists.txt                  # Build configuration with labels\n\u2514\u2500\u2500 fixtures/                       # Shared test fixtures (future)\n\npython/tests/\n\u251c\u2500\u2500 conftest.py                     # pytest configuration, fixtures\n\u251c\u2500\u2500 test_*.py                       # Test files\n\u2514\u2500\u2500 stress/                         # Stress tests (separate directory)\n    \u2514\u2500\u2500 test_stress_*.py\n\ninterop/\n\u251c\u2500\u2500 test_*.cpp                      # C++ interop programs\n\u251c\u2500\u2500 test_*.py                       # Python interop programs\n\u2514\u2500\u2500 test_*.sh                       # Shell orchestration scripts\n</code></pre>"},{"location":"TESTING_STRATEGY/#ctest-labels","title":"CTest Labels","text":"<p>All tests must have appropriate labels:</p> <pre><code>set_tests_properties(test_name PROPERTIES\n    LABELS \"category;domain;feature\"\n    TIMEOUT &lt;seconds&gt;)\n</code></pre> <p>Label Taxonomy: - Category: <code>fast</code>, <code>medium</code>, <code>slow</code>, <code>stress</code> - Domain: <code>unit</code>, <code>integration</code>, <code>interop</code>, <code>performance</code> - Feature: <code>lockfree</code>, <code>sync</code>, <code>memory</code>, <code>structures</code>, <code>codata</code> - Special: <code>disabled</code>, <code>flaky</code>, <code>coverage</code></p> <p>Examples: <pre><code># Fast unit test for queue\nLABELS \"fast;unit;structures;lockfree\"\n\n# Medium integration test for barrier\nLABELS \"medium;integration;sync\"\n\n# Slow stress test for ABA problem\nLABELS \"slow;stress;lockfree;concurrent\"\n</code></pre></p>"},{"location":"TESTING_STRATEGY/#running-targeted-tests","title":"Running Targeted Tests","text":"<pre><code># All lock-free tests\nctest -L lockfree --output-on-failure\n\n# All synchronization tests (fast + medium only)\nctest -L sync -LE slow --output-on-failure\n\n# Everything except stress tests\nctest -LE stress --output-on-failure\n\n# Specific combination: fast lock-free tests\nctest -L \"fast\" -L \"lockfree\" --output-on-failure\n\n# CI mode: all non-disabled tests\nctest -LE disabled --output-on-failure\n</code></pre>"},{"location":"TESTING_STRATEGY/#lock-free-testing-best-practices","title":"Lock-Free Testing Best Practices","text":"<p>Lock-free data structures require specialized testing approaches to catch race conditions and memory ordering bugs.</p>"},{"location":"TESTING_STRATEGY/#1-deterministic-functional-tests","title":"1. Deterministic Functional Tests","text":"<p>Goal: Verify algorithm correctness in controlled scenarios Approach: Single-threaded or minimal concurrency</p> <pre><code>TEST_F(QueueTest, SingleThreadedCorrectness) {\n    Queue&lt;int&gt; q(mem, \"test\", 100);\n\n    // Sequential operations - no race conditions\n    for (int i = 0; i &lt; 50; i++) {\n        ASSERT_TRUE(q.push(i));\n    }\n\n    for (int i = 0; i &lt; 50; i++) {\n        auto val = q.pop();\n        ASSERT_TRUE(val.has_value());\n        EXPECT_EQ(*val, i);  // Exact order preserved\n    }\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#2-high-contention-stress-tests","title":"2. High-Contention Stress Tests","text":"<p>Goal: Force race conditions to surface Approach: Many threads, small queue, tight loops</p> <pre><code>TEST_F(QueueTest, HighContention) {\n    Queue&lt;int&gt; q(mem, \"small\", 10);  // Very small queue\n\n    const int threads = 32;  // More threads than queue slots\n    const int iterations = 10000;\n\n    // Multiple producers/consumers competing for few slots\n    // This maximizes CAS failures and retry loops\n}\n</code></pre> <p>Key insight: Use queue_size &lt;&lt; num_threads to force contention.</p>"},{"location":"TESTING_STRATEGY/#3-invariant-verification","title":"3. Invariant Verification","text":"<p>Goal: Validate lock-free invariants hold under concurrency Approach: Track totals, checksums, counts</p> <pre><code>TEST_F(QueueTest, MPMCConservation) {\n    std::atomic&lt;int&gt; produced{0};\n    std::atomic&lt;int&gt; consumed{0};\n    std::atomic&lt;int&gt; sum_produced{0};\n    std::atomic&lt;int&gt; sum_consumed{0};\n\n    // After all threads complete:\n    EXPECT_EQ(produced.load(), consumed.load());\n    EXPECT_EQ(sum_produced.load(), sum_consumed.load());\n    EXPECT_TRUE(q.empty());\n}\n</code></pre> <p>Invariants to test: - Conservation: items_produced == items_consumed - No duplication: checksums match - No loss: all values accounted for - State consistency: empty() matches size()==0</p>"},{"location":"TESTING_STRATEGY/#4-aba-problem-detection","title":"4. ABA Problem Detection","text":"<p>Goal: Verify ABA problem is handled correctly Approach: Rapid allocation/deallocation cycles</p> <pre><code>TEST_F(ABATest, RapidRecycling) {\n    // Push/pop same values repeatedly\n    // If ABA problem exists, will see incorrect behavior\n\n    for (int gen = 0; gen &lt; 1000; gen++) {\n        q.push(42);\n        auto val = q.pop();\n        ASSERT_TRUE(val.has_value());\n        EXPECT_EQ(*val, 42);\n    }\n}\n</code></pre> <p>What to look for: - Corrupted values - Lost updates - Unexpected empty/full states - Segmentation faults</p>"},{"location":"TESTING_STRATEGY/#5-memory-ordering-validation","title":"5. Memory Ordering Validation","text":"<p>Goal: Ensure proper fences and ordering Approach: Look for torn reads, stale values</p> <pre><code>TEST_F(MemoryOrderingTest, NoTornReads) {\n    struct Data { int x, y; };\n    Queue&lt;Data&gt; q(mem, \"test\", 100);\n\n    auto producer = [&amp;]() {\n        for (int i = 0; i &lt; 1000; i++) {\n            q.push({i, i});  // Always matching values\n        }\n    };\n\n    auto consumer = [&amp;]() {\n        for (int i = 0; i &lt; 1000; i++) {\n            auto val = q.pop();\n            if (val.has_value()) {\n                EXPECT_EQ(val-&gt;x, val-&gt;y);  // Should never be torn\n            }\n        }\n    };\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#6-progressive-stress-testing","title":"6. Progressive Stress Testing","text":"<p>Goal: Identify threshold where failures occur Approach: Gradually increase pressure</p> <pre><code>class ProgressiveStressTest : public ::testing::TestWithParam&lt;int&gt; {\n    // Parameterized by thread count\n};\n\nINSTANTIATE_TEST_SUITE_P(\n    ThreadCount,\n    ProgressiveStressTest,\n    ::testing::Values(2, 4, 8, 16, 32, 64));\n</code></pre>"},{"location":"TESTING_STRATEGY/#7-bounded-runtime-assertions","title":"7. Bounded Runtime Assertions","text":"<p>Goal: Prevent infinite loops in lock-free retry logic Approach: Timeout-based assertions</p> <pre><code>auto start = std::chrono::steady_clock::now();\nwhile (!q.push(value)) {\n    std::this_thread::yield();\n\n    auto elapsed = std::chrono::steady_clock::now() - start;\n    ASSERT_LT(elapsed, 1s) &lt;&lt; \"Push failed to complete in 1 second\";\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#what-not-to-test","title":"What NOT to Test","text":"<ol> <li>Internal implementation details - Test behavior, not structure</li> <li>Specific timing - Lock-free is nondeterministic</li> <li>Thread scheduling - Cannot control OS scheduler</li> <li>Exact CAS failure counts - These vary per run</li> </ol>"},{"location":"TESTING_STRATEGY/#cross-language-interop-testing","title":"Cross-Language Interop Testing","text":""},{"location":"TESTING_STRATEGY/#current-state-ad-hoc-shell-scripts","title":"Current State (Ad-hoc Shell Scripts)","text":"<pre><code># interop/test_interop.sh\n./cpp_writer &amp;\nsleep 1\n./python_reader\n</code></pre> <p>Problems: - Not integrated with test frameworks - No structured assertions - Timing-dependent - Difficult to debug failures</p>"},{"location":"TESTING_STRATEGY/#recommended-architecture","title":"Recommended Architecture","text":""},{"location":"TESTING_STRATEGY/#1-standardized-test-protocol","title":"1. Standardized Test Protocol","text":"<p>Create a protocol file format for interop coordination:</p> <pre><code>{\n  \"test_name\": \"queue_cpp_to_python\",\n  \"writer\": {\n    \"language\": \"cpp\",\n    \"program\": \"./cpp_writer\",\n    \"structures\": [\n      {\"name\": \"test_queue\", \"type\": \"queue\", \"dtype\": \"int32\", \"capacity\": 1000}\n    ],\n    \"operations\": [\n      {\"op\": \"push\", \"values\": [1, 2, 3, 4, 5]}\n    ]\n  },\n  \"reader\": {\n    \"language\": \"python\",\n    \"program\": \"./python_reader.py\",\n    \"expected\": [1, 2, 3, 4, 5]\n  }\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#2-test-orchestration-layer","title":"2. Test Orchestration Layer","text":"<p>Create a Python-based orchestrator:</p> <pre><code># interop/orchestrator.py\nimport subprocess\nimport json\nimport time\n\nclass InteropTest:\n    def __init__(self, protocol_file):\n        self.protocol = json.load(open(protocol_file))\n\n    def run(self):\n        # Start writer\n        writer_proc = subprocess.Popen(self.protocol['writer']['program'])\n\n        # Wait for shared memory initialization\n        time.sleep(0.5)\n\n        # Start reader\n        reader_proc = subprocess.Popen(\n            self.protocol['reader']['program'],\n            stdout=subprocess.PIPE\n        )\n\n        # Collect results\n        writer_proc.wait(timeout=5)\n        reader_output = reader_proc.communicate(timeout=5)[0]\n\n        # Validate\n        assert reader_proc.returncode == 0\n        return json.loads(reader_output)\n</code></pre>"},{"location":"TESTING_STRATEGY/#3-pytest-integration","title":"3. Pytest Integration","text":"<pre><code># interop/test_interop_integration.py\nimport pytest\nfrom pathlib import Path\nfrom orchestrator import InteropTest\n\nINTEROP_TESTS = Path(__file__).parent.glob(\"protocols/*.json\")\n\n@pytest.mark.parametrize(\"protocol\", INTEROP_TESTS, ids=lambda p: p.stem)\ndef test_interop(protocol):\n    \"\"\"Test cross-language interoperability using protocol files.\"\"\"\n    test = InteropTest(protocol)\n    results = test.run()\n\n    expected = test.protocol['reader']['expected']\n    assert results == expected\n\n@pytest.mark.interop\nclass TestInteropScenarios:\n    def test_cpp_writes_python_reads_queue(self):\n        # Specific scenario tests\n        pass\n\n    def test_python_writes_cpp_reads_array(self):\n        pass\n\n    def test_bidirectional_sync_barrier(self):\n        pass\n</code></pre>"},{"location":"TESTING_STRATEGY/#4-ctest-integration","title":"4. CTest Integration","text":"<pre><code># Add interop tests to CTest\nadd_test(\n    NAME interop_cpp_to_python\n    COMMAND python3 -m pytest ${CMAKE_SOURCE_DIR}/interop/test_interop_integration.py\n    WORKING_DIRECTORY ${CMAKE_BINARY_DIR}\n)\n\nset_tests_properties(interop_cpp_to_python PROPERTIES\n    LABELS \"medium;interop;integration\"\n    TIMEOUT 30)\n</code></pre>"},{"location":"TESTING_STRATEGY/#coverage-strategy","title":"Coverage Strategy","text":""},{"location":"TESTING_STRATEGY/#multi-dimensional-coverage","title":"Multi-Dimensional Coverage","text":"<p>ZeroIPC requires coverage across multiple dimensions:</p> <ol> <li>Code coverage - Lines, branches, functions</li> <li>Concurrency coverage - Thread interleavings</li> <li>Platform coverage - Linux, macOS</li> <li>Language coverage - C++, C, Python</li> <li>Scenario coverage - Use cases, edge cases</li> </ol>"},{"location":"TESTING_STRATEGY/#code-coverage-tools","title":"Code Coverage Tools","text":""},{"location":"TESTING_STRATEGY/#c-coverage","title":"C++ Coverage","text":"<pre><code># Configure with coverage flags\ncmake -B build -DCMAKE_CXX_FLAGS=\"--coverage\" .\n\n# Run tests\ncd build &amp;&amp; ctest\n\n# Generate report\nlcov --capture --directory . --output-file coverage.info\nlcov --remove coverage.info '/usr/*' '*/tests/*' '*/googletest/*' --output-file coverage_filtered.info\ngenhtml coverage_filtered.info --output-directory coverage_html\n</code></pre>"},{"location":"TESTING_STRATEGY/#python-coverage","title":"Python Coverage","text":"<pre><code># pyproject.toml already configured with pytest-cov\ncd python\npytest --cov=zeroipc --cov-report=html --cov-report=term\n</code></pre>"},{"location":"TESTING_STRATEGY/#coverage-goals","title":"Coverage Goals","text":"<p>Minimum Acceptable: - Core data structures: 90%+ - Lock-free algorithms: 85%+ - Error handling: 80%+ - Synchronization primitives: 85%+</p> <p>What NOT to aim for 100% on: - Error messages (strings) - Debug logging - Platform-specific fallbacks - Unreachable error paths</p>"},{"location":"TESTING_STRATEGY/#concurrency-coverage","title":"Concurrency Coverage","text":"<p>Code coverage doesn't reveal race conditions. Use:</p> <p>Thread Sanitizer: <pre><code>cmake -B build -DCMAKE_CXX_FLAGS=\"-fsanitize=thread -g\" .\ncd build &amp;&amp; ctest\n</code></pre></p> <p>Valgrind Helgrind: <pre><code>ctest -T memcheck -R queue_test\n</code></pre></p> <p>Stress Testing: Run stress tests for extended periods: <pre><code># Run for 1 hour under high load\ntimeout 3600 ./build/test_stress --gtest_repeat=1000\n</code></pre></p>"},{"location":"TESTING_STRATEGY/#test-driven-development-workflow","title":"Test-Driven Development Workflow","text":""},{"location":"TESTING_STRATEGY/#for-new-features","title":"For New Features","text":"<ol> <li> <p>Write failing test first (Red)    <pre><code>TEST_F(QueueTest, TryPopReturnsEmptyWhenQueueEmpty) {\n    Queue&lt;int&gt; q(mem, \"test\", 10);\n    auto result = q.try_pop();\n    EXPECT_FALSE(result.has_value());\n}\n</code></pre></p> </li> <li> <p>Implement minimal code (Green)    <pre><code>std::optional&lt;T&gt; try_pop() {\n    if (empty()) return std::nullopt;\n    return pop();  // Delegate to existing pop()\n}\n</code></pre></p> </li> <li> <p>Refactor (Refactor)</p> </li> <li>Optimize CAS loops</li> <li>Improve memory ordering</li> <li>Extract common patterns</li> <li>Tests should still pass</li> </ol>"},{"location":"TESTING_STRATEGY/#for-bug-fixes","title":"For Bug Fixes","text":"<ol> <li>Write regression test that reproduces bug</li> <li>Verify test fails with current code</li> <li>Fix the bug</li> <li>Verify test passes</li> <li>Keep the test - it's now your regression guard</li> </ol>"},{"location":"TESTING_STRATEGY/#for-lock-free-algorithms","title":"For Lock-Free Algorithms","text":"<ol> <li>Start with single-threaded correctness test</li> <li>Add 2-thread test (producer + consumer)</li> <li>Add N-thread test with invariants</li> <li>Add high-contention stress test</li> <li>Add ABA problem test</li> <li>Run under ThreadSanitizer</li> </ol>"},{"location":"TESTING_STRATEGY/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"TESTING_STRATEGY/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code>name: ZeroIPC Tests\n\non: [push, pull_request]\n\njobs:\n  fast-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Configure\n        run: cmake -B build .\n      - name: Build\n        run: cmake --build build\n      - name: Fast Tests\n        run: cd build &amp;&amp; ctest -L fast --output-on-failure\n        timeout-minutes: 2\n\n  medium-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Configure\n        run: cmake -B build .\n      - name: Build\n        run: cmake --build build\n      - name: Medium Tests\n        run: cd build &amp;&amp; ctest -L medium --output-on-failure\n        timeout-minutes: 5\n\n  full-tests:\n    runs-on: ubuntu-latest\n    if: github.event_name == 'pull_request'\n    steps:\n      - uses: actions/checkout@v3\n      - name: Configure\n        run: cmake -B build .\n      - name: Build\n        run: cmake --build build\n      - name: All Tests\n        run: cd build &amp;&amp; ctest -LE disabled --output-on-failure\n        timeout-minutes: 10\n\n  stress-tests:\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/master'\n    steps:\n      - uses: actions/checkout@v3\n      - name: Configure\n        run: cmake -B build .\n      - name: Build\n        run: cmake --build build\n      - name: Stress Tests\n        run: cd build &amp;&amp; ctest --output-on-failure\n        timeout-minutes: 30\n</code></pre>"},{"location":"TESTING_STRATEGY/#test-execution-strategy","title":"Test Execution Strategy","text":"<p>On every commit: - Fast tests only (~30 seconds)</p> <p>On pull request: - Fast + Medium tests (~2 minutes) - Python tests - Basic interop tests</p> <p>On merge to main: - Full suite including slow tests (~10 minutes) - Coverage reports - Interop comprehensive tests</p> <p>Nightly: - Full stress test suite - Extended duration tests - Memory leak detection - Performance regression benchmarks</p>"},{"location":"TESTING_STRATEGY/#performance-testing","title":"Performance Testing","text":""},{"location":"TESTING_STRATEGY/#benchmark-vs-test","title":"Benchmark vs Test","text":"<p>Tests: Verify correctness Benchmarks: Measure performance</p> <p>Keep them separate:</p> <pre><code>cpp/\n\u251c\u2500\u2500 tests/          # Correctness validation\n\u2514\u2500\u2500 benchmarks/     # Performance measurement\n</code></pre>"},{"location":"TESTING_STRATEGY/#benchmark-example","title":"Benchmark Example","text":"<pre><code>// benchmarks/benchmark_queue.cpp\n#include &lt;benchmark/benchmark.h&gt;\n#include &lt;zeroipc/queue.h&gt;\n\nstatic void BM_QueuePush(benchmark::State&amp; state) {\n    Memory mem(\"/bench_queue\", 10 * 1024 * 1024);\n    Queue&lt;int&gt; q(mem, \"test\", 10000);\n\n    for (auto _ : state) {\n        q.push(42);\n        benchmark::DoNotOptimize(q);\n    }\n\n    state.SetItemsProcessed(state.iterations());\n}\nBENCHMARK(BM_QueuePush)-&gt;Threads(1)-&gt;Threads(4)-&gt;Threads(16);\n</code></pre>"},{"location":"TESTING_STRATEGY/#performance-regression-testing","title":"Performance Regression Testing","text":"<p>Track performance over time:</p> <pre><code># Baseline\n./benchmark_queue --benchmark_out=baseline.json\n\n# After changes\n./benchmark_queue --benchmark_out=current.json\n\n# Compare\ncompare.py baseline.json current.json\n</code></pre> <p>Alert if throughput drops &gt;10%.</p>"},{"location":"TESTING_STRATEGY/#specific-test-cases-you-may-be-missing","title":"Specific Test Cases You May Be Missing","text":""},{"location":"TESTING_STRATEGY/#1-process-crash-recovery","title":"1. Process Crash Recovery","text":"<pre><code>TEST_F(FailureRecoveryTest, QueueSurvivesWriterCrash) {\n    // Fork process\n    pid_t pid = fork();\n\n    if (pid == 0) {\n        // Child: Write some data then crash\n        Queue&lt;int&gt; q(mem, \"test\", 100);\n        q.push(1);\n        q.push(2);\n        std::exit(0);  // Simulated crash\n    }\n\n    // Parent: Wait for child, then verify data\n    waitpid(pid, nullptr, 0);\n\n    Queue&lt;int&gt; q(mem, \"test\");  // Reattach\n    auto val = q.pop();\n    ASSERT_TRUE(val.has_value());\n    EXPECT_EQ(*val, 1);\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#2-memory-reattachment","title":"2. Memory Reattachment","text":"<pre><code>TEST_F(MemoryTest, ReattachAfterDetach) {\n    {\n        Memory mem(\"/test\", 1MB);\n        Array&lt;int&gt; arr(mem, \"data\", 100);\n        arr.set(0, 42);\n    }\n    // mem destroyed, munmap called\n\n    {\n        Memory mem(\"/test\");  // Reattach\n        Array&lt;int&gt; arr(mem, \"data\");\n        EXPECT_EQ(arr.get(0), 42);  // Data persisted\n    }\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#3-name-collision-handling","title":"3. Name Collision Handling","text":"<pre><code>TEST_F(TableTest, DuplicateNameRejection) {\n    Memory mem(\"/test\", 1MB);\n    Array&lt;int&gt; arr1(mem, \"data\", 100);\n\n    EXPECT_THROW({\n        Array&lt;int&gt; arr2(mem, \"data\", 200);  // Same name\n    }, std::runtime_error);\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#4-type-safety-duck-typing","title":"4. Type Safety (Duck Typing)","text":"<pre><code>TEST_F(ArrayTest, TypeMismatchUndefinedBehavior) {\n    Memory mem(\"/test\", 1MB);\n\n    {\n        Array&lt;int32_t&gt; arr(mem, \"data\", 100);\n        arr.set(0, 0x12345678);\n    }\n\n    {\n        // Open as different type - ALLOWED but undefined\n        Array&lt;float&gt; arr(mem, \"data\");\n        // arr.get(0) will reinterpret bits\n        // This is expected behavior (duck typing)\n        // Document but don't prevent\n    }\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#5-capacity-limits","title":"5. Capacity Limits","text":"<pre><code>TEST_F(QueueTest, ExactCapacityBehavior) {\n    Queue&lt;int&gt; q(mem, \"test\", 10);\n\n    // Circular buffer uses one slot for full/empty distinction\n    for (int i = 0; i &lt; 9; i++) {\n        EXPECT_TRUE(q.push(i));\n    }\n\n    EXPECT_TRUE(q.full());\n    EXPECT_FALSE(q.push(999));  // Reject when full\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#6-alignment-requirements","title":"6. Alignment Requirements","text":"<pre><code>TEST_F(MemoryTest, ProperAlignment) {\n    Memory mem(\"/test\", 1MB);\n\n    struct alignas(64) CacheLine { char data[64]; };\n    Array&lt;CacheLine&gt; arr(mem, \"aligned\", 10);\n\n    // Verify alignment\n    uintptr_t addr = reinterpret_cast&lt;uintptr_t&gt;(arr.data());\n    EXPECT_EQ(addr % 64, 0);\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#7-table-overflow","title":"7. Table Overflow","text":"<pre><code>TEST_F(TableTest, TableFullBehavior) {\n    Memory&lt;table8&gt; mem(\"/test\", 1MB);  // Only 8 entries\n\n    // Fill table\n    for (int i = 0; i &lt; 8; i++) {\n        std::string name = \"entry_\" + std::to_string(i);\n        Array&lt;int&gt; arr(mem, name, 10);\n    }\n\n    // Next allocation should fail\n    EXPECT_THROW({\n        Array&lt;int&gt; overflow(mem, \"overflow\", 10);\n    }, std::runtime_error);\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#quick-reference","title":"Quick Reference","text":""},{"location":"TESTING_STRATEGY/#test-commands","title":"Test Commands","text":"<pre><code># Fast tests only (~30 sec)\nctest -L fast --output-on-failure\n\n# Default suite (~2 min)\nctest -L \"fast|medium\" --output-on-failure\n\n# Everything except stress (~10 min)\nctest -LE stress --output-on-failure\n\n# Full suite including stress (~30 min)\nctest --output-on-failure\n\n# Specific category\nctest -L lockfree --output-on-failure\n\n# With coverage\nctest --output-on-failure &amp;&amp; lcov ...\n\n# Parallel execution\nctest -j$(nproc) -L fast\n</code></pre>"},{"location":"TESTING_STRATEGY/#environment-variables","title":"Environment Variables","text":"<pre><code># Set test mode\nexport ZEROIPC_TEST_MODE=FAST    # Minimal iterations\nexport ZEROIPC_TEST_MODE=MEDIUM  # Moderate iterations\nexport ZEROIPC_TEST_MODE=STRESS  # Maximum iterations\n\n# CI mode (longer timeouts)\nexport CI=1\n</code></pre>"},{"location":"TESTING_STRATEGY/#summary","title":"Summary","text":"<p>Key Achievements: 1. \u2705 Reduced test runtime from 20+ min to &lt;2 min 2. \u2705 Proper test categorization (FAST/MEDIUM/SLOW/STRESS) 3. \u2705 CTest labels for targeted execution 4. \u2705 Parameterized timing constants 5. \u2705 Lock-free testing best practices 6. \u2705 Interop test integration strategy 7. \u2705 Coverage strategy across all dimensions</p> <p>Next Steps: 1. Apply test_config.h to all synchronization tests 2. Update CMakeLists.txt with proper labels 3. Create fast variants of slow tests 4. Implement interop orchestrator 5. Set up CI/CD with staged test execution 6. Add missing test cases (process crash, type safety, etc.)</p> <p>Files Created: - <code>/home/spinoza/github/beta/zeroipc/cpp/tests/test_config.h</code> - <code>/home/spinoza/github/beta/zeroipc/cpp/tests/test_semaphore_optimized.cpp</code> - <code>/home/spinoza/github/beta/zeroipc/cpp/tests/CMakeLists_improved.txt</code> - <code>/home/spinoza/github/beta/zeroipc/docs/TESTING_STRATEGY.md</code></p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/","title":"ZeroIPC Test Optimization: Action Plan","text":""},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#immediate-actions-can-complete-in-1-2-hours","title":"Immediate Actions (Can complete in 1-2 hours)","text":""},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#1-apply-test-configuration-header","title":"1. Apply Test Configuration Header","text":"<p>File: <code>/home/spinoza/github/beta/zeroipc/cpp/tests/test_config.h</code> (already created)</p> <p>Action: Update existing slow tests to use parameterized timing:</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#test_semaphorecpp","title":"test_semaphore.cpp","text":"<p>Replace all hardcoded delays:</p> <pre><code>// Before:\nstd::this_thread::sleep_for(10ms);\nstd::this_thread::sleep_for(100ms);\nstd::this_thread::sleep_for(50ms);\n\n// After:\n#include \"test_config.h\"\nusing namespace zeroipc::test;\n\nstd::this_thread::sleep_for(TestTiming::CRITICAL_SECTION_DELAY);\nstd::this_thread::sleep_for(TestTiming::THREAD_START_DELAY);\nstd::this_thread::sleep_for(TestTiming::THREAD_SYNC_DELAY);\n</code></pre> <p>Impact: Reduces test_semaphore from 10+ minutes to &lt;30 seconds</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#test_barriercpp","title":"test_barrier.cpp","text":"<p>Same replacements as semaphore.</p> <p>Impact: Reduces test_barrier from 10+ minutes to &lt;30 seconds</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#2-update-cmakeliststxt-with-labels","title":"2. Update CMakeLists.txt with Labels","text":"<p>File: <code>/home/spinoza/github/beta/zeroipc/cpp/CMakeLists.txt</code></p> <p>Add labels to all existing tests:</p> <pre><code># Example for fast tests\nset_tests_properties(table_test PROPERTIES\n    LABELS \"fast;unit;core\"\n    TIMEOUT 5)\n\nset_tests_properties(queue_test PROPERTIES\n    LABELS \"fast;unit;structures;lockfree\"\n    TIMEOUT 5)\n\n# Example for medium tests\nset_tests_properties(lockfree_comprehensive_test PROPERTIES\n    LABELS \"medium;integration;lockfree\"\n    TIMEOUT 10)\n\n# Example for slow tests (increase timeout, mark as disabled)\nset_tests_properties(semaphore_test PROPERTIES\n    LABELS \"slow;integration;sync\"\n    TIMEOUT 60)\n\nset_tests_properties(barrier_test PROPERTIES\n    LABELS \"slow;integration;sync\"\n    TIMEOUT 60)\n\n# Stress tests (mark as disabled by default)\nset_tests_properties(stress_test PROPERTIES\n    LABELS \"stress;performance;concurrent\"\n    TIMEOUT 120\n    DISABLED TRUE)\n\nset_tests_properties(aba_problem_test PROPERTIES\n    LABELS \"stress;lockfree;concurrent\"\n    TIMEOUT 120\n    DISABLED TRUE)\n\nset_tests_properties(memory_boundaries_test PROPERTIES\n    LABELS \"stress;memory;edge-cases\"\n    TIMEOUT 120\n    DISABLED TRUE)\n</code></pre> <p>Impact: Enables targeted test execution</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#3-add-custom-test-targets","title":"3. Add Custom Test Targets","text":"<p>Append to CMakeLists.txt:</p> <pre><code># Fast tests only\nadd_custom_target(test_fast\n    COMMAND ${CMAKE_CTEST_COMMAND} -L fast --output-on-failure\n    WORKING_DIRECTORY ${CMAKE_BINARY_DIR})\n\n# Default: fast + medium\nadd_custom_target(test_default\n    COMMAND ${CMAKE_CTEST_COMMAND} -L \"fast|medium\" --output-on-failure\n    WORKING_DIRECTORY ${CMAKE_BINARY_DIR})\n\n# CI: everything except stress\nadd_custom_target(test_ci\n    COMMAND ${CMAKE_CTEST_COMMAND} -LE stress --output-on-failure\n    WORKING_DIRECTORY ${CMAKE_BINARY_DIR})\n</code></pre> <p>Impact: Convenient test execution workflows</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#short-term-actions-1-2-days","title":"Short-Term Actions (1-2 days)","text":""},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#4-create-fast-variants-of-synchronization-tests","title":"4. Create Fast Variants of Synchronization Tests","text":"<p>Use <code>test_semaphore_optimized.cpp</code> as template for: - <code>test_barrier_optimized.cpp</code> - <code>test_channel_optimized.cpp</code> (if exists)</p> <p>Pattern: 1. Keep all FAST tests (no delays) 2. Convert MEDIUM tests to use minimal delays 3. Move SLOW tests to separate <code>*SlowTest</code> fixture with <code>DISABLED_</code> prefix</p> <p>Build as separate executables:</p> <pre><code>add_executable(test_semaphore_fast tests/test_semaphore_optimized.cpp)\ntarget_link_libraries(test_semaphore_fast gtest_main Threads::Threads rt)\nadd_test(NAME semaphore_test_fast COMMAND test_semaphore_fast)\nset_tests_properties(semaphore_test_fast PROPERTIES\n    LABELS \"medium;unit;sync\"\n    TIMEOUT 10)\n\n# Original semaphore test becomes slow variant\nset_tests_properties(semaphore_test PROPERTIES\n    LABELS \"slow;integration;sync\"\n    TIMEOUT 300\n    DISABLED TRUE)\n</code></pre> <p>Impact: Default test runs use fast variants</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#5-fix-current-test-failures","title":"5. Fix Current Test Failures","text":"<p>From the timeout analysis, these tests need adjustment:</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#test_stresscpp","title":"test_stress.cpp","text":"<p>Current: Times out at 1 second, needs ~2-3 seconds</p> <pre><code>set_tests_properties(stress_test PROPERTIES\n    LABELS \"stress;performance;concurrent\"\n    TIMEOUT 120  # Increase from 1s\n    DISABLED TRUE)  # Disable by default\n</code></pre>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#test_aba_problemcpp","title":"test_aba_problem.cpp","text":"<p>Current: Times out at 1 second</p> <pre><code>set_tests_properties(aba_problem_test PROPERTIES\n    LABELS \"stress;lockfree;concurrent\"\n    TIMEOUT 120\n    DISABLED TRUE)\n</code></pre>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#test_memory_boundariescpp","title":"test_memory_boundaries.cpp","text":"<p>Current: Times out at 1 second</p> <pre><code>set_tests_properties(memory_boundaries_test PROPERTIES\n    LABELS \"stress;memory;edge-cases\"\n    TIMEOUT 120\n    DISABLED TRUE)\n</code></pre> <p>Impact: All tests pass when explicitly run</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#medium-term-actions-1-week","title":"Medium-Term Actions (1 week)","text":""},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#6-python-test-organization","title":"6. Python Test Organization","text":"<p>Create test categories:</p> <pre><code># python/tests/conftest.py\nimport pytest\n\ndef pytest_configure(config):\n    config.addinivalue_line(\"markers\", \"fast: Fast tests (&lt;1s)\")\n    config.addinivalue_line(\"markers\", \"medium: Medium tests (&lt;10s)\")\n    config.addinivalue_line(\"markers\", \"slow: Slow tests (&gt;10s)\")\n    config.addinivalue_line(\"markers\", \"stress: Stress tests\")\n    config.addinivalue_line(\"markers\", \"interop: Interop tests\")\n\n# Mark tests appropriately:\n@pytest.mark.fast\ndef test_array_create():\n    ...\n\n@pytest.mark.medium\ndef test_queue_mpmc():\n    ...\n\n@pytest.mark.slow\n@pytest.mark.stress\ndef test_sustained_load():\n    ...\n</code></pre> <p>Run targeted tests: <pre><code>pytest -m fast\npytest -m \"fast or medium\"\npytest -m \"not slow\"\n</code></pre></p> <p>Impact: Consistent test organization across languages</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#7-interop-test-integration","title":"7. Interop Test Integration","text":"<p>Create protocol-based interop tests:</p> <pre><code># interop/test_interop_queue.py\nimport pytest\nimport subprocess\nimport json\nfrom pathlib import Path\n\n@pytest.mark.interop\nclass TestQueueInterop:\n    def test_cpp_writes_python_reads(self, tmp_path):\n        shm_name = f\"/interop_test_{os.getpid()}\"\n\n        # Run C++ writer\n        cpp_writer = subprocess.Popen(\n            [\"./cpp_queue_writer\", shm_name],\n            cwd=Path(__file__).parent\n        )\n        cpp_writer.wait(timeout=5)\n        assert cpp_writer.returncode == 0\n\n        # Run Python reader\n        result = subprocess.run(\n            [\"python3\", \"python_queue_reader.py\", shm_name],\n            capture_output=True,\n            timeout=5,\n            cwd=Path(__file__).parent\n        )\n        assert result.returncode == 0\n\n        # Validate output\n        values = json.loads(result.stdout)\n        assert values == list(range(100))\n\n    def test_python_writes_cpp_reads(self, tmp_path):\n        # Similar pattern, reversed\n        pass\n</code></pre> <p>Add to CTest:</p> <pre><code>add_test(\n    NAME interop_queue\n    COMMAND python3 -m pytest ${CMAKE_SOURCE_DIR}/interop/test_interop_queue.py\n)\nset_tests_properties(interop_queue PROPERTIES\n    LABELS \"medium;interop\"\n    TIMEOUT 30)\n</code></pre> <p>Impact: Proper interop test integration</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#8-coverage-analysis","title":"8. Coverage Analysis","text":"<p>C++ Coverage:</p> <pre><code># Build with coverage\ncmake -B build -DCMAKE_CXX_FLAGS=\"--coverage -O0 -g\" .\ncmake --build build\n\n# Run fast + medium tests\ncd build\nctest -L \"fast|medium\" --output-on-failure\n\n# Generate report\nlcov --capture --directory . --output-file coverage.info\nlcov --remove coverage.info '/usr/*' '*/tests/*' '*/googletest/*' --output-file coverage_filtered.info\ngenhtml coverage_filtered.info --output-directory coverage_html\n\n# Open in browser\nxdg-open coverage_html/index.html\n</code></pre> <p>Python Coverage:</p> <pre><code>cd python\npytest --cov=zeroipc --cov-report=html --cov-report=term-missing\nxdg-open htmlcov/index.html\n</code></pre> <p>Set coverage targets: - Core structures: 90%+ - Lock-free algorithms: 85%+ - Error handling: 80%+</p> <p>Impact: Identify untested code paths</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#long-term-actions-ongoing","title":"Long-Term Actions (Ongoing)","text":""},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#9-cicd-pipeline","title":"9. CI/CD Pipeline","text":"<p>GitHub Actions workflow:</p> <pre><code># .github/workflows/tests.yml\nname: ZeroIPC Tests\n\non:\n  push:\n    branches: [ master ]\n  pull_request:\n    branches: [ master ]\n\njobs:\n  fast-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install dependencies\n        run: |\n          sudo apt-get update\n          sudo apt-get install -y cmake g++ lcov\n\n      - name: Configure\n        run: cmake -B build .\n\n      - name: Build\n        run: cmake --build build -j$(nproc)\n\n      - name: Fast Tests\n        run: cd build &amp;&amp; ctest -L fast --output-on-failure -j$(nproc)\n        timeout-minutes: 2\n\n  medium-tests:\n    runs-on: ubuntu-latest\n    needs: fast-tests\n    steps:\n      - uses: actions/checkout@v3\n      - name: Configure\n        run: cmake -B build .\n      - name: Build\n        run: cmake --build build -j$(nproc)\n      - name: Medium Tests\n        run: cd build &amp;&amp; ctest -L medium --output-on-failure -j$(nproc)\n        timeout-minutes: 5\n\n  python-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n      - name: Install\n        run: |\n          cd python\n          pip install -e \".[dev]\"\n      - name: Fast Tests\n        run: cd python &amp;&amp; pytest -m \"not slow\" --cov=zeroipc\n        timeout-minutes: 3\n\n  coverage:\n    runs-on: ubuntu-latest\n    if: github.event_name == 'pull_request'\n    steps:\n      - uses: actions/checkout@v3\n      - name: Configure with coverage\n        run: cmake -B build -DCMAKE_CXX_FLAGS=\"--coverage\" .\n      - name: Build\n        run: cmake --build build\n      - name: Test\n        run: cd build &amp;&amp; ctest -L \"fast|medium\" --output-on-failure\n      - name: Coverage\n        run: |\n          lcov --capture --directory build --output-file coverage.info\n          lcov --remove coverage.info '/usr/*' '*/tests/*' --output-file coverage_filtered.info\n      - uses: codecov/codecov-action@v3\n        with:\n          files: ./coverage_filtered.info\n\n  nightly-stress:\n    runs-on: ubuntu-latest\n    if: github.event_name == 'schedule'\n    steps:\n      - uses: actions/checkout@v3\n      - name: Configure\n        run: cmake -B build .\n      - name: Build\n        run: cmake --build build\n      - name: All Tests\n        run: cd build &amp;&amp; ctest --output-on-failure -j$(nproc)\n        timeout-minutes: 30\n</code></pre> <p>Schedule nightly stress tests:</p> <pre><code>on:\n  schedule:\n    - cron: '0 2 * * *'  # 2 AM daily\n</code></pre> <p>Impact: Automated quality assurance</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#10-performance-regression-testing","title":"10. Performance Regression Testing","text":"<p>Create baseline:</p> <pre><code>cd cpp/benchmarks\n./benchmark_queue --benchmark_out=baseline.json\ngit add baseline.json\ngit commit -m \"Add performance baseline\"\n</code></pre> <p>CI comparison:</p> <pre><code>- name: Performance Regression Check\n  run: |\n    cd build\n    ./benchmarks/benchmark_queue --benchmark_out=current.json\n    python3 ../scripts/compare_benchmarks.py \\\n      ../cpp/benchmarks/baseline.json \\\n      current.json \\\n      --threshold=0.1  # Fail if 10% slower\n</code></pre> <p>Impact: Prevent performance regressions</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#validation-checklist","title":"Validation Checklist","text":"<p>After implementing the action plan:</p>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#test-runtime","title":"Test Runtime","text":"<ul> <li> Fast tests complete in &lt;30 seconds</li> <li> Fast + Medium tests complete in &lt;2 minutes</li> <li> Full suite (excluding stress) completes in &lt;10 minutes</li> <li> Stress tests can complete in &lt;30 minutes</li> </ul>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#test-organization","title":"Test Organization","text":"<ul> <li> All tests have appropriate labels</li> <li> CTest can run tests by category</li> <li> Custom targets work (<code>make test_fast</code>, etc.)</li> <li> Python tests have pytest markers</li> </ul>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#coverage","title":"Coverage","text":"<ul> <li> C++ coverage &gt;85% on core code</li> <li> Python coverage &gt;85% on core code</li> <li> All lock-free algorithms have stress tests</li> <li> All synchronization primitives tested</li> </ul>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#interop","title":"Interop","text":"<ul> <li> C++ \u2192 Python tests passing</li> <li> Python \u2192 C++ tests passing</li> <li> Bidirectional tests passing</li> <li> All data types covered</li> </ul>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#cicd","title":"CI/CD","text":"<ul> <li> Fast tests run on every push</li> <li> Full tests run on PR</li> <li> Coverage reports generated</li> <li> Nightly stress tests scheduled</li> </ul>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#documentation","title":"Documentation","text":"<ul> <li> TESTING_STRATEGY.md complete</li> <li> Test examples for all categories</li> <li> CI/CD pipeline documented</li> <li> Coverage targets documented</li> </ul>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#expected-results","title":"Expected Results","text":""},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#before-optimization","title":"Before Optimization","text":"<pre><code>$ time ctest\n...\n100% tests passed, 0 tests failed out of 19\n\nTotal Test time (real) = 1247.32 sec  # 20+ minutes\n</code></pre>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#after-optimization-default-suite","title":"After Optimization (Default Suite)","text":"<pre><code>$ time ctest -L \"fast|medium\"\n...\n100% tests passed, 0 tests failed out of 13\n\nTotal Test time (real) = 42.18 sec  # &lt;1 minute\n</code></pre>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#after-optimization-ci-suite","title":"After Optimization (CI Suite)","text":"<pre><code>$ time ctest -LE stress\n...\n100% tests passed, 0 tests failed out of 16\n\nTotal Test time (real) = 123.45 sec  # ~2 minutes\n</code></pre>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#after-optimization-full-suite","title":"After Optimization (Full Suite)","text":"<pre><code>$ time ctest\n...\n100% tests passed, 0 tests failed out of 19\n\nTotal Test time (real) = 487.22 sec  # ~8 minutes\n</code></pre>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#priority-order","title":"Priority Order","text":"<ol> <li>Immediate (Today)</li> <li>Add test_config.h</li> <li>Update semaphore/barrier timing</li> <li>Add CTest labels</li> <li> <p>Fix timeout issues</p> </li> <li> <p>This Week</p> </li> <li>Create fast test variants</li> <li>Python test organization</li> <li>Basic interop integration</li> <li> <p>Coverage analysis</p> </li> <li> <p>This Month</p> </li> <li>Full CI/CD pipeline</li> <li>Performance regression testing</li> <li>Complete interop suite</li> <li> <p>Documentation finalization</p> </li> <li> <p>Ongoing</p> </li> <li>Monitor test performance</li> <li>Add missing test cases</li> <li>Improve coverage</li> <li>Refine test strategy</li> </ol>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#files-to-modify","title":"Files to Modify","text":""},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#immediate-changes","title":"Immediate Changes","text":"<ol> <li><code>cpp/tests/test_semaphore.cpp</code> - Add test_config.h, reduce delays</li> <li><code>cpp/tests/test_barrier.cpp</code> - Add test_config.h, reduce delays</li> <li><code>cpp/CMakeLists.txt</code> - Add labels and timeouts</li> <li><code>cpp/tests/test_stress.cpp</code> - Reduce iterations for medium variant</li> </ol>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#new-files-to-create","title":"New Files to Create","text":"<ol> <li><code>cpp/tests/test_config.h</code> - \u2705 Created</li> <li><code>cpp/tests/test_semaphore_optimized.cpp</code> - \u2705 Created</li> <li><code>cpp/tests/test_barrier_optimized.cpp</code> - TODO</li> <li><code>python/tests/conftest.py</code> - Add pytest markers</li> <li><code>interop/test_interop_queue.py</code> - TODO</li> <li><code>.github/workflows/tests.yml</code> - TODO</li> </ol>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#documentation_1","title":"Documentation","text":"<ol> <li><code>docs/TESTING_STRATEGY.md</code> - \u2705 Created</li> <li><code>docs/TEST_OPTIMIZATION_ACTION_PLAN.md</code> - \u2705 This file</li> <li><code>README.md</code> - Update with new test commands</li> </ol>"},{"location":"TEST_OPTIMIZATION_ACTION_PLAN/#commands-quick-reference","title":"Commands Quick Reference","text":"<pre><code># Fast tests only\nctest -L fast --output-on-failure\n\n# Default suite\nctest -L \"fast|medium\" --output-on-failure\n\n# CI suite\nctest -LE stress --output-on-failure\n\n# Everything\nctest --output-on-failure\n\n# Parallel\nctest -j$(nproc) -L fast\n\n# Python fast\ncd python &amp;&amp; pytest -m \"not slow\"\n\n# Coverage\nctest &amp;&amp; lcov --capture ...\n</code></pre> <p>Ready to implement? Start with the Immediate Actions section and work through the checklist.</p>"},{"location":"VFS_IMPLEMENTATION_STATUS/","title":"Virtual Filesystem Implementation Status","text":""},{"location":"VFS_IMPLEMENTATION_STATUS/#completed","title":"Completed \u2705","text":""},{"location":"VFS_IMPLEMENTATION_STATUS/#phase-1-core-infrastructure","title":"Phase 1: Core Infrastructure","text":"<ul> <li>vfs.h created (<code>cpp/tools/vfs.h</code>):</li> <li><code>Path</code> class: Parse, resolve, and manipulate paths</li> <li><code>NavigationContext</code>: Track current location (ROOT/SEGMENT/STRUCTURE)</li> <li><code>listSharedMemorySegments()</code>: Scan /dev/shm for segments</li> <li><code>formatSize()</code>: Human-readable size formatting</li> <li>Added missing headers: <code>&lt;iomanip&gt;</code> and <code>&lt;sys/stat.h&gt;</code></li> </ul>"},{"location":"VFS_IMPLEMENTATION_STATUS/#design-documentation","title":"Design Documentation","text":"<ul> <li>VIRTUAL_FILESYSTEM_DESIGN.md: Complete design specification</li> <li>VFS_IMPLEMENTATION_STATUS.md: This status document</li> </ul>"},{"location":"VFS_IMPLEMENTATION_STATUS/#phase-2-integration-with-zeroipccpp","title":"Phase 2: Integration with zeroipc.cpp \u2705","text":"<p>Completed Changes:</p> <ol> <li> <p>Added include (line 33):    <pre><code>#include \"vfs.h\"\n</code></pre></p> </li> <li> <p>Added NavigationContext to REPL class (line 406):    <pre><code>class ZeroIPCRepl {\nprivate:\n    zeroipc::vfs::NavigationContext nav_context_;\n    // ... existing members\n</code></pre></p> </li> <li> <p>Updated REPL prompt (line 417):    <pre><code>std::cout &lt;&lt; nav_context_.prompt();\n</code></pre></p> </li> <li> <p>Implemented ls command (lines 1626-1665):</p> </li> <li>At root: lists shared memory segments from <code>/dev/shm</code></li> <li>In segment: lists structures using SharedMemoryInspector</li> <li> <p>In structure: shows structure information</p> </li> <li> <p>Implemented cd command (lines 1683-1727):</p> </li> <li>Parses path argument (absolute/relative)</li> <li>Validates destination</li> <li>Updates NavigationContext</li> <li>Handles segment switching (opens new segment when needed)</li> <li> <p>Reverts on error</p> </li> <li> <p>Implemented pwd command (lines 1729-1731):</p> </li> <li> <p>Returns current path from NavigationContext</p> </li> <li> <p>Updated command routing (lines 568-576):    <pre><code>else if (cmd == \"ls\") {\n    cmdLs(tokens);\n}\nelse if (cmd == \"cd\") {\n    cmdCd(tokens);\n}\nelse if (cmd == \"pwd\") {\n    cmdPwd(tokens);\n}\n</code></pre></p> </li> <li> <p>Updated help text (lines 590-593):    <pre><code>std::cout &lt;&lt; \"Navigation (Virtual Filesystem):\\n\";\nstd::cout &lt;&lt; \"  ls [path]                            List contents at current location or path\\n\";\nstd::cout &lt;&lt; \"  cd &lt;path&gt;                            Change directory\\n\";\nstd::cout &lt;&lt; \"  pwd                                  Print working directory\\n\\n\";\n</code></pre></p> </li> </ol>"},{"location":"VFS_IMPLEMENTATION_STATUS/#testing","title":"Testing \u2705","text":"<p>Successfully tested the following functionality: - <code>pwd</code> - shows current path - <code>ls</code> at root - lists all shared memory segments in <code>/dev/shm</code> - <code>cd /segment_name</code> - changes to a segment directory - <code>ls</code> within segment - shows table entries (structures) - <code>cd /</code> - returns to root - Prompt updates correctly based on current location</p> <p>Example session: <pre><code>$ zeroipc -r\nZeroIPC Interactive Shell v3.0 - Virtual Filesystem Interface\nType 'help' for available commands, 'quit' to exit\n\nzeroipc&gt; pwd\n/\nzeroipc&gt; ls\n\n=== Shared Memory Segments ===\nName                          Size\n--------------------------------------------------\n/demo_vfs                     10.0 MB\n/bigshared                    10.0 MB\n/myshared                     1.0 MB\n\nzeroipc&gt; cd /demo_vfs\n/demo_vfs&gt; pwd\n/demo_vfs\n/demo_vfs&gt; ls\n\n=== Table Entries ===\n#   Name                            Offset      Size        Type\n---------------------------------------------------------------------------\n[entries shown here]\n\n/demo_vfs&gt; cd /\nzeroipc&gt; pwd\n/\n</code></pre></p>"},{"location":"VFS_IMPLEMENTATION_STATUS/#future-enhancements","title":"Future Enhancements \ud83d\udccb","text":"<p>While the basic virtual filesystem navigation is now complete, the following features remain for future implementation:</p>"},{"location":"VFS_IMPLEMENTATION_STATUS/#not-yet-implemented","title":"Not Yet Implemented","text":""},{"location":"VFS_IMPLEMENTATION_STATUS/#phase-3-advanced-features","title":"Phase 3: Advanced Features","text":"<ol> <li>cat command implementation</li> <li>Show array contents with indices</li> <li>Show queue/stack contents in order</li> <li>Show map/set entries</li> <li> <p>Show synchronization primitive states</p> </li> <li> <p>Type detection improvements</p> </li> <li>Add magic numbers to structure headers</li> <li>Implement heuristic-based type inference</li> <li> <p>Add type registry</p> </li> <li> <p>Tab completion</p> </li> <li>Complete segment names</li> <li>Complete structure names</li> <li> <p>Complete commands</p> </li> <li> <p>CLI parity</p> </li> <li>Make all REPL commands work as CLI one-liners:      <pre><code>zeroipc ls /\nzeroipc ls /myapp_data\nzeroipc cat /myapp_data/temperatures\n</code></pre></li> </ol>"},{"location":"VFS_IMPLEMENTATION_STATUS/#testing-plan","title":"Testing Plan","text":""},{"location":"VFS_IMPLEMENTATION_STATUS/#unit-tests-cppteststest_vfscpp","title":"Unit Tests (cpp/tests/test_vfs.cpp)","text":"<pre><code>TEST(VFSTest, PathParsing) {\n    zeroipc::vfs::Path p(\"/myapp/data\");\n    EXPECT_EQ(p.depth(), 2);\n    EXPECT_EQ(p[0], \"myapp\");\n    EXPECT_EQ(p[1], \"data\");\n    EXPECT_EQ(p.toString(), \"/myapp/data\");\n}\n\nTEST(VFSTest, PathResolution) {\n    zeroipc::vfs::Path base(\"/myapp/data\");\n    zeroipc::vfs::Path rel = base.resolve(\"../other\");\n    EXPECT_EQ(rel.toString(), \"/myapp/other\");\n}\n\nTEST(VFSTest, NavigationContext) {\n    zeroipc::vfs::NavigationContext ctx;\n    EXPECT_TRUE(ctx.cd(\"/myapp\"));\n    EXPECT_EQ(ctx.location_type, zeroipc::vfs::LocationType::SEGMENT);\n    EXPECT_EQ(ctx.segment_name, \"myapp\");\n}\n</code></pre>"},{"location":"VFS_IMPLEMENTATION_STATUS/#integration-tests","title":"Integration Tests","text":"<ol> <li>Create shared memory with structures</li> <li>Use ls to list them</li> <li>Use cd to navigate</li> <li>Use cat to display contents</li> <li>Verify prompt updates correctly</li> </ol>"},{"location":"VFS_IMPLEMENTATION_STATUS/#manual-testing","title":"Manual Testing","text":"<pre><code>$ zeroipc -r\nzeroipc&gt; ls\nmyapp_data/     10 MB\nsensors/         5 MB\n\nzeroipc&gt; cd myapp_data\n/myapp_data&gt; ls\ntemperatures     array&lt;float&gt;[1000]        4000 bytes\ntask_queue       queue&lt;task&gt;[100]           424 bytes\n\n/myapp_data&gt; cd temperatures\n/myapp_data/temperatures&gt; ls\n[0] = 23.5\n[1] = 24.1\n...\n\n/myapp_data/temperatures&gt; cd /\nzeroipc&gt; pwd\n/\n</code></pre>"},{"location":"VFS_IMPLEMENTATION_STATUS/#next-steps","title":"Next Steps","text":"<ol> <li>Immediate: Integrate vfs.h into zeroipc.cpp following the changes above</li> <li>Short-term: Implement ls, cd, pwd commands</li> <li>Medium-term: Implement cat command for all structure types</li> <li>Long-term: Add tab completion and CLI parity</li> </ol>"},{"location":"VFS_IMPLEMENTATION_STATUS/#technical-debt","title":"Technical Debt","text":"<ol> <li>Memory management: Currently only one segment open at a time. May need connection pool for multi-segment navigation.</li> <li>Type detection: Need better mechanism than size-based heuristics. Consider adding magic numbers or type registry.</li> <li>Error handling: Need comprehensive error messages for invalid paths, missing structures, etc.</li> </ol>"},{"location":"VFS_IMPLEMENTATION_STATUS/#estimated-effort","title":"Estimated Effort","text":"<ul> <li>Integration (vfs.h into zeroipc.cpp): 2-3 hours</li> <li>Basic ls/cd/pwd: 2-3 hours</li> <li>Table inspection: 1-2 hours</li> <li>cat command: 3-4 hours (all structure types)</li> <li>Tab completion: 2-3 hours</li> <li>CLI parity: 1-2 hours</li> <li>Testing: 2-3 hours</li> </ul> <p>Total: ~15-20 hours of development time</p>"},{"location":"VFS_IMPLEMENTATION_STATUS/#files-modifiedcreated","title":"Files Modified/Created","text":""},{"location":"VFS_IMPLEMENTATION_STATUS/#created","title":"Created","text":"<ul> <li>\u2705 <code>cpp/tools/vfs.h</code> - Virtual filesystem core</li> <li>\u2705 <code>docs/VIRTUAL_FILESYSTEM_DESIGN.md</code> - Design specification</li> <li>\u2705 <code>docs/VFS_IMPLEMENTATION_STATUS.md</code> - This file</li> </ul>"},{"location":"VFS_IMPLEMENTATION_STATUS/#to-modify","title":"To Modify","text":"<ul> <li>\u274c <code>cpp/tools/zeroipc.cpp</code> - Integrate VFS functionality</li> <li>\u274c <code>cpp/CMakeLists.txt</code> - May need vfs dependencies</li> <li>\u274c <code>README.md</code> - Document new navigation commands</li> <li>\u274c <code>CLAUDE.md</code> - Update CLI tool section</li> <li>\u274c <code>docs/cli_tools.md</code> - Add VFS navigation examples</li> </ul>"},{"location":"VFS_IMPLEMENTATION_STATUS/#to-create","title":"To Create","text":"<ul> <li>\u274c <code>cpp/tests/test_vfs.cpp</code> - Unit tests for VFS</li> </ul>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/","title":"Virtual Filesystem Design for ZeroIPC CLI","text":""},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#overview","title":"Overview","text":"<p>Transform the <code>zeroipc</code> CLI tool into a virtual filesystem interface where shared memory segments and data structures can be navigated like directories and files.</p>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#path-structure","title":"Path Structure","text":"<pre><code>/                           # Root - lists all shared memory segments\n\u251c\u2500\u2500 myapp_data/            # Shared memory segment\n\u2502   \u251c\u2500\u2500 temperatures/      # Array structure\n\u2502   \u251c\u2500\u2500 task_queue/        # Queue structure\n\u2502   \u251c\u2500\u2500 sync_barrier/      # Barrier structure\n\u2502   \u2514\u2500\u2500 cache_map/         # Map structure\n\u2514\u2500\u2500 sensors/               # Another shared memory segment\n    \u251c\u2500\u2500 readings/\n    \u2514\u2500\u2500 status/\n</code></pre>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#navigation-commands","title":"Navigation Commands","text":""},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#ls-path","title":"<code>ls [path]</code>","text":"<p>List contents at current location or specified path</p> <p>At root (<code>/</code>): <pre><code>zeroipc&gt; ls\nmyapp_data/     10485760 bytes    3 structures\nsensors/         5242880 bytes    2 structures\ncache/         104857600 bytes   15 structures\n</code></pre></p> <p>In shared memory segment (<code>/myapp_data</code>): <pre><code>/myapp_data&gt; ls\ntemperatures     array&lt;float&gt;[1000]        4000 bytes\ntask_queue       queue&lt;task&gt;[100]           424 bytes\nsync_barrier     barrier(4 participants)     32 bytes\ncache_map        map&lt;string,value&gt;         8192 bytes\n</code></pre></p> <p>In a structure (<code>/myapp_data/temperatures</code>): <pre><code>/myapp_data/temperatures&gt; ls\nType: array&lt;float&gt;\nCapacity: 1000\nSize: 4000 bytes\nContents: [0..999]\n  [0] = 23.5\n  [1] = 24.1\n  [2] = 22.8\n  ...\n</code></pre></p>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#cd-path","title":"<code>cd &lt;path&gt;</code>","text":"<p>Change current directory</p> <pre><code>zeroipc&gt; cd /myapp_data          # Absolute path\n/myapp_data&gt; cd temperatures     # Relative path\n/myapp_data/temperatures&gt; cd ..  # Parent directory\n/myapp_data&gt; cd /sensors        # Jump to another segment\n/sensors&gt; cd /                   # Back to root\n</code></pre>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#pwd","title":"<code>pwd</code>","text":"<p>Print working directory</p> <pre><code>/myapp_data/temperatures&gt; pwd\n/myapp_data/temperatures\n</code></pre>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#cat-pathrange","title":"<code>cat &lt;path|range&gt;</code>","text":"<p>Display structure contents</p> <pre><code>/myapp_data&gt; cat temperatures          # Show entire array\n/myapp_data&gt; cat temperatures[0-10]    # Show range\n/myapp_data&gt; cat task_queue            # Show queue contents\n</code></pre>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#implementation-plan","title":"Implementation Plan","text":""},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#phase-1-core-infrastructure-priority-1","title":"Phase 1: Core Infrastructure (Priority 1)","text":"<ol> <li>Path Management</li> <li><code>struct Path { std::vector&lt;std::string&gt; components; }</code></li> <li><code>std::string current_path = \"/\"</code></li> <li>Path parsing: <code>/myapp/data</code> \u2192 <code>[\"myapp\", \"data\"]</code></li> <li> <p>Path resolution: handle <code>.</code>, <code>..</code>, <code>/</code>, relative vs absolute</p> </li> <li> <p>Location Context <pre><code>enum class LocationType { ROOT, SEGMENT, STRUCTURE };\n\nstruct NavigationContext {\n    LocationType type;\n    std::string segment_name;      // If in segment or structure\n    std::string structure_name;    // If in structure\n    Memory* current_memory;        // Pointer to open memory\n};\n</code></pre></p> </li> <li> <p>Table Inspection</p> </li> <li>Read metadata table from shared memory</li> <li>List all registered structures</li> <li>Determine structure types (currently not stored - enhancement needed)</li> </ol>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#phase-2-navigation-commands-priority-1","title":"Phase 2: Navigation Commands (Priority 1)","text":"<ol> <li><code>ls</code> command</li> <li>At root: scan <code>/dev/shm</code>, list shared memory segments</li> <li>In segment: read table, list structures</li> <li> <p>In structure: show type-specific contents</p> </li> <li> <p><code>cd</code> command</p> </li> <li>Parse path (absolute/relative)</li> <li>Validate destination exists</li> <li>Update current context</li> <li> <p>Handle special cases: <code>.</code>, <code>..</code>, <code>/</code></p> </li> <li> <p><code>pwd</code> command</p> </li> <li>Return current path string</li> </ol>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#phase-3-content-display-priority-2","title":"Phase 3: Content Display (Priority 2)","text":"<ol> <li><code>cat</code> command</li> <li>Array: show elements with indices</li> <li>Queue: show queue contents head\u2192tail</li> <li>Stack: show stack contents top\u2192bottom</li> <li>Map: show key-value pairs</li> <li> <p>Semaphore/Barrier/Latch: show state</p> </li> <li> <p>Type-specific formatters</p> </li> <li>Different display logic per structure type</li> </ol>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#phase-4-enhanced-features-priority-3","title":"Phase 4: Enhanced Features (Priority 3)","text":"<ol> <li>Tab completion</li> <li>Complete segment names at root</li> <li>Complete structure names in segment</li> <li> <p>Complete commands</p> </li> <li> <p>Aliases and shortcuts</p> </li> <li><code>ll</code> = <code>ls -l</code> (long format)</li> <li><code>..</code> = <code>cd ..</code></li> <li> <p><code>~</code> = <code>cd /</code></p> </li> <li> <p>Filtering and searching</p> </li> <li><code>ls | grep pattern</code></li> <li><code>find &lt;name&gt;</code></li> </ol>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#technical-challenges","title":"Technical Challenges","text":""},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#challenge-1-structure-type-detection","title":"Challenge 1: Structure Type Detection","text":"<p>Problem: The metadata table only stores name, offset, and size. No type information.</p> <p>Solutions: 1. Type inference heuristics: Guess type based on size patterns 2. Type registry: Store type info in a separate metadata structure 3. Magic numbers: Add type identifier at start of each structure 4. Naming conventions: Require type prefix (e.g., <code>array_temperatures</code>)</p> <p>Recommended: Option 3 (Magic numbers) - minimal overhead, reliable</p>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#challenge-2-open-memory-management","title":"Challenge 2: Open Memory Management","text":"<p>Problem: Currently can only have one shared memory segment open at a time.</p> <p>Solutions: 1. Keep only current segment open (close when navigating away) 2. Implement connection pool (keep multiple segments open) 3. Open on-demand, cache handles</p> <p>Recommended: Option 1 initially (simple), upgrade to 2 later</p>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#challenge-3-large-structure-display","title":"Challenge 3: Large Structure Display","text":"<p>Problem: Arrays with 1M elements would flood terminal.</p> <p>Solutions: 1. Pagination (show first N, prompt for more) 2. Smart truncation (show first/last N) 3. Range-based display only 4. Lazy loading on scroll</p> <p>Recommended: Option 2 (smart truncation) with option for full display</p>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#prompt-enhancement","title":"Prompt Enhancement","text":"<p>Update prompt to show current location:</p> <pre><code>zeroipc&gt;                    # At root\n/myapp_data&gt;               # In segment\n/myapp_data/temps&gt;         # In structure\n</code></pre>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#backward-compatibility","title":"Backward Compatibility","text":"<p>Keep existing commands working: - <code>open /name</code> \u2192 same as <code>cd /name</code> - <code>create</code> commands \u2192 work from any location - <code>list</code> \u2192 same as <code>ls</code> at root</p>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#example-session","title":"Example Session","text":"<pre><code>$ zeroipc -r\nZeroIPC Interactive Shell v4.0 - Virtual Filesystem Interface\n\nzeroipc&gt; ls\nmyapp_data/     10 MB      3 structures\nsensors/         5 MB      2 structures\n\nzeroipc&gt; cd myapp_data\n/myapp_data&gt; ls\ntemperatures     array&lt;float&gt;[1000]        4000 bytes\ntask_queue       queue&lt;task&gt;[100]           424 bytes\nsync_barrier     barrier(4 participants)     32 bytes\n\n/myapp_data&gt; cd temperatures\n/myapp_data/temperatures&gt; ls\nType: array&lt;float&gt;\nCapacity: 1000\nElements: [0..999]\n\n/myapp_data/temperatures&gt; cat [0-5]\n[0] = 23.5\n[1] = 24.1\n[2] = 22.8\n[3] = 24.0\n[4] = 23.2\n[5] = 22.9\n\n/myapp_data/temperatures&gt; cd ..\n/myapp_data&gt; cat task_queue\nType: queue&lt;task&gt;\nHead: 0, Tail: 5, Size: 5/100\n[0] = {id: 1, priority: 5, data: \"process_image\"}\n[1] = {id: 2, priority: 3, data: \"backup_data\"}\n...\n\n/myapp_data&gt; cd /\nzeroipc&gt; quit\n</code></pre>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#cli-parity","title":"CLI Parity","text":"<p>All REPL commands should also work as CLI one-liners:</p> <pre><code># List all shared memory\nzeroipc ls /\n\n# List structures in a segment\nzeroipc ls /myapp_data\n\n# Show array contents\nzeroipc cat /myapp_data/temperatures[0-10]\n\n# Navigate and list\nzeroipc cd /myapp_data &amp;&amp; ls\n</code></pre>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#implementation-timeline","title":"Implementation Timeline","text":"<ol> <li>Week 1: Core infrastructure (Path, NavigationContext, table inspection)</li> <li>Week 2: Basic navigation (ls, cd, pwd)</li> <li>Week 3: Content display (cat for all structure types)</li> <li>Week 4: Polish (tab completion, aliases, CLI parity)</li> </ol>"},{"location":"VIRTUAL_FILESYSTEM_DESIGN/#testing-strategy","title":"Testing Strategy","text":"<ol> <li>Unit tests for path parsing and resolution</li> <li>Integration tests for navigation scenarios</li> <li>Manual testing with real shared memory segments</li> <li>Performance testing with large structures</li> <li>Cross-language interop testing (Python creates, CLI navigates)</li> </ol>"},{"location":"api_reference/","title":"ZeroIPC C++ API Reference","text":""},{"location":"api_reference/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Components</li> <li>Memory</li> <li>Table</li> <li>Traditional Data Structures</li> <li>Array</li> <li>Queue</li> <li>Stack</li> <li>Map</li> <li>Set</li> <li>Pool</li> <li>Ring</li> <li>Codata Structures</li> <li>Future</li> <li>Lazy</li> <li>Stream</li> <li>Channel</li> </ol>"},{"location":"api_reference/#core-components","title":"Core Components","text":""},{"location":"api_reference/#memory","title":"Memory","text":"<p>POSIX shared memory wrapper with automatic lifecycle management.</p> <pre><code>namespace zeroipc {\n    template&lt;typename TableImpl = table64&gt;\n    class memory;\n}\n</code></pre>"},{"location":"api_reference/#constructor","title":"Constructor","text":"<pre><code>memory(std::string_view name, size_t size = 0, bool auto_create = true);\n</code></pre> <p>Creates or opens shared memory segment.</p> <p>Parameters: - <code>name</code>: Shared memory identifier (e.g., \"/mydata\") - <code>size</code>: Size in bytes (0 to open existing) - <code>auto_create</code>: Create if doesn't exist</p> <p>Example: <pre><code>// Create new segment\nmemory mem(\"/sensors\", 10*1024*1024);  // 10MB\n\n// Open existing\nmemory mem_existing(\"/sensors\");\n</code></pre></p>"},{"location":"api_reference/#methods","title":"Methods","text":"Method Description Return Type <code>base()</code> Get base pointer <code>void*</code> <code>size()</code> Get total size <code>size_t</code> <code>allocate(name, size)</code> Allocate named region <code>size_t</code> (offset) <code>find(name, offset, size)</code> Find named region <code>bool</code> <code>list()</code> List all allocations <code>vector&lt;TableEntry&gt;</code>"},{"location":"api_reference/#table","title":"Table","text":"<p>Metadata registry for dynamic structure discovery.</p> <pre><code>// Predefined table sizes\nusing table1 = table_impl&lt;32, 1&gt;;       // 1 entry\nusing table64 = table_impl&lt;32, 64&gt;;     // 64 entries (default)\nusing table256 = table_impl&lt;32, 256&gt;;   // 256 entries\nusing table4096 = table_impl&lt;32, 4096&gt;; // 4096 entries\n</code></pre>"},{"location":"api_reference/#table-entry-structure","title":"Table Entry Structure","text":"<pre><code>struct table_entry {\n    char name[32];      // Structure name\n    uint32_t offset;    // Offset in memory\n    uint32_t size;      // Size in bytes\n};\n</code></pre>"},{"location":"api_reference/#traditional-data-structures","title":"Traditional Data Structures","text":""},{"location":"api_reference/#array","title":"Array","text":"<p>Fixed-size contiguous array with atomic operations.</p> <pre><code>template&lt;typename T&gt;\nclass array;\n</code></pre>"},{"location":"api_reference/#constructor_1","title":"Constructor","text":"<pre><code>// Create new\narray(memory&amp; mem, std::string_view name, size_t capacity);\n\n// Open existing\narray(memory&amp; mem, std::string_view name);\n</code></pre>"},{"location":"api_reference/#methods_1","title":"Methods","text":"Method Description Return Type <code>operator[]</code> Element access <code>T&amp;</code> <code>at(index)</code> Bounds-checked access <code>T&amp;</code> <code>size()</code> Number of elements <code>size_t</code> <code>capacity()</code> Maximum elements <code>size_t</code> <code>data()</code> Raw pointer <code>T*</code> <code>fill(value)</code> Set all elements <code>void</code> <code>atomic_add(index, value)</code> Atomic addition <code>T</code> <code>atomic_compare_exchange(index, expected, desired)</code> CAS operation <code>bool</code> <p>Example: <pre><code>memory mem(\"/data\", 10*1024*1024);\narray&lt;float&gt; temps(mem, \"temperatures\", 1000);\ntemps[0] = 23.5f;\nfloat old = temps.atomic_add(0, 1.5f);  // Returns 23.5f\n</code></pre></p>"},{"location":"api_reference/#queue","title":"Queue","text":"<p>Lock-free multi-producer multi-consumer circular buffer.</p> <pre><code>template&lt;typename T&gt;\nclass queue;\n</code></pre>"},{"location":"api_reference/#constructor_2","title":"Constructor","text":"<pre><code>// Create new\nqueue(memory&amp; mem, std::string_view name, size_t capacity);\n\n// Open existing\nqueue(memory&amp; mem, std::string_view name);\n</code></pre>"},{"location":"api_reference/#methods_2","title":"Methods","text":"Method Description Return Type <code>push(value)</code> Enqueue element <code>bool</code> <code>pop()</code> Dequeue element <code>std::optional&lt;T&gt;</code> <code>try_push(value)</code> Non-blocking push <code>bool</code> <code>try_pop()</code> Non-blocking pop <code>std::optional&lt;T&gt;</code> <code>size()</code> Current elements <code>size_t</code> <code>capacity()</code> Maximum elements <code>size_t</code> <code>empty()</code> Check if empty <code>bool</code> <code>full()</code> Check if full <code>bool</code> <p>Example: <pre><code>queue&lt;Message&gt; msgs(mem, \"messages\", 1000);\nmsgs.push(Message{.id = 1, .data = \"Hello\"});\nif (auto msg = msgs.pop()) {\n    process(*msg);\n}\n</code></pre></p>"},{"location":"api_reference/#stack","title":"Stack","text":"<p>Lock-free LIFO stack with ABA problem prevention.</p> <pre><code>template&lt;typename T&gt;\nclass stack;\n</code></pre>"},{"location":"api_reference/#methods_3","title":"Methods","text":"Method Description Return Type <code>push(value)</code> Push element <code>bool</code> <code>pop()</code> Pop element <code>std::optional&lt;T&gt;</code> <code>top()</code> Peek at top <code>std::optional&lt;T&gt;</code> <code>size()</code> Current elements <code>size_t</code> <code>empty()</code> Check if empty <code>bool</code>"},{"location":"api_reference/#map","title":"Map","text":"<p>Lock-free hash map with linear probing.</p> <pre><code>template&lt;typename K, typename V&gt;\nclass map;\n</code></pre>"},{"location":"api_reference/#constructor_3","title":"Constructor","text":"<pre><code>// Create new\nmap(memory&amp; mem, std::string_view name, size_t capacity);\n\n// Open existing\nmap(memory&amp; mem, std::string_view name);\n</code></pre>"},{"location":"api_reference/#methods_4","title":"Methods","text":"Method Description Return Type <code>insert(key, value)</code> Insert/update <code>bool</code> <code>get(key)</code> Retrieve value <code>std::optional&lt;V&gt;</code> <code>remove(key)</code> Delete entry <code>bool</code> <code>contains(key)</code> Check existence <code>bool</code> <code>size()</code> Number of entries <code>size_t</code> <code>clear()</code> Remove all entries <code>void</code> <code>operator[]</code> Access/insert <code>V&amp;</code> <p>Example: <pre><code>map&lt;uint32_t, double&gt; cache(mem, \"score_cache\", 10000);\ncache.insert(42, 0.95);\nif (auto score = cache.get(42)) {\n    std::cout &lt;&lt; \"Score: \" &lt;&lt; *score &lt;&lt; std::endl;\n}\n</code></pre></p>"},{"location":"api_reference/#set","title":"Set","text":"<p>Lock-free hash set for unique elements.</p> <pre><code>template&lt;typename T&gt;\nclass set;\n</code></pre>"},{"location":"api_reference/#methods_5","title":"Methods","text":"Method Description Return Type <code>insert(value)</code> Add element <code>bool</code> <code>remove(value)</code> Remove element <code>bool</code> <code>contains(value)</code> Check membership <code>bool</code> <code>size()</code> Number of elements <code>size_t</code> <code>clear()</code> Remove all <code>void</code>"},{"location":"api_reference/#pool","title":"Pool","text":"<p>Object pool with free list management.</p> <pre><code>template&lt;typename T&gt;\nclass pool;\n</code></pre>"},{"location":"api_reference/#constructor_4","title":"Constructor","text":"<pre><code>pool(memory&amp; mem, std::string_view name, size_t capacity);\n</code></pre>"},{"location":"api_reference/#methods_6","title":"Methods","text":"Method Description Return Type <code>allocate()</code> Get object from pool <code>T*</code> <code>deallocate(ptr)</code> Return to pool <code>void</code> <code>available()</code> Free objects count <code>size_t</code> <code>capacity()</code> Total objects <code>size_t</code> <code>reset()</code> Return all to pool <code>void</code> <p>Example: <pre><code>struct Task { int id; char data[256]; };\npool&lt;Task&gt; task_pool(mem, \"tasks\", 100);\n\nTask* t = task_pool.allocate();\nt-&gt;id = 42;\n// ... use task ...\ntask_pool.deallocate(t);\n</code></pre></p>"},{"location":"api_reference/#ring","title":"Ring","text":"<p>High-performance ring buffer for streaming data.</p> <pre><code>template&lt;typename T&gt;\nclass ring;\n</code></pre>"},{"location":"api_reference/#methods_7","title":"Methods","text":"Method Description Return Type <code>write(data, count)</code> Write elements <code>size_t</code> <code>read(buffer, count)</code> Read elements <code>size_t</code> <code>peek(buffer, count)</code> Read without consuming <code>size_t</code> <code>skip(count)</code> Skip elements <code>size_t</code> <code>available()</code> Readable elements <code>size_t</code> <code>space()</code> Writable space <code>size_t</code>"},{"location":"api_reference/#codata-structures","title":"Codata Structures","text":""},{"location":"api_reference/#future","title":"Future","text":"<p>Asynchronous computation results in shared memory.</p> <pre><code>template&lt;typename T&gt;\nclass future;\n</code></pre>"},{"location":"api_reference/#constructor_5","title":"Constructor","text":"<pre><code>// Create new\nfuture(memory&amp; mem, std::string_view name);\n\n// Open existing\nfuture(memory&amp; mem, std::string_view name, bool);\n</code></pre>"},{"location":"api_reference/#methods_8","title":"Methods","text":"Method Description Return Type <code>set_value(value)</code> Set result <code>void</code> <code>set_error(msg)</code> Set error state <code>void</code> <code>get()</code> Get value (blocks) <code>T</code> <code>try_get()</code> Non-blocking get <code>std::optional&lt;T&gt;</code> <code>get_for(duration)</code> Get with timeout <code>std::optional&lt;T&gt;</code> <code>wait()</code> Wait for ready <code>void</code> <code>wait_for(duration)</code> Wait with timeout <code>bool</code> <code>is_ready()</code> Check if ready <code>bool</code> <code>has_value()</code> Check if has value <code>bool</code> <code>has_error()</code> Check if error <code>bool</code> <code>get_error()</code> Get error message <code>std::string</code> <p>Example: <pre><code>// Producer\nfuture&lt;Result&gt; result(mem, \"computation\");\nstd::thread([&amp;]() {\n    Result r = expensive_computation();\n    result.set_value(r);\n}).detach();\n\n// Consumer\nfuture&lt;Result&gt; result(mem, \"computation\", true);\nif (auto r = result.get_for(5s)) {\n    process(*r);\n}\n</code></pre></p>"},{"location":"api_reference/#lazy","title":"Lazy","text":"<p>Deferred computation with automatic memoization.</p> <pre><code>template&lt;typename T&gt;\nclass lazy;\n</code></pre>"},{"location":"api_reference/#constructor_6","title":"Constructor","text":"<pre><code>// Create new\nlazy(memory&amp; mem, std::string_view name);\n\n// Open existing\nlazy(memory&amp; mem, std::string_view name, bool);\n</code></pre>"},{"location":"api_reference/#methods_9","title":"Methods","text":"Method Description Return Type <code>set_computation(func)</code> Define computation <code>void</code> <code>get()</code> Get value (computes if needed) <code>T</code> <code>is_computed()</code> Check if cached <code>bool</code> <code>invalidate()</code> Clear cache <code>void</code> <code>compute_async()</code> Trigger async computation <code>void</code> <p>Example: <pre><code>lazy&lt;Config&gt; config(mem, \"app_config\");\nconfig.set_computation([]() {\n    return parse_config_file(\"/etc/app.conf\");\n});\n\n// First access computes and caches\nConfig c = config.get();\n\n// Subsequent accesses use cache\nConfig c2 = config.get();  // Instant\n</code></pre></p>"},{"location":"api_reference/#stream","title":"Stream","text":"<p>Reactive data streams with functional operators.</p> <pre><code>template&lt;typename T&gt;\nclass stream;\n</code></pre>"},{"location":"api_reference/#constructor_7","title":"Constructor","text":"<pre><code>// Create new\nstream(memory&amp; mem, std::string_view name, size_t buffer_size = 1024);\n\n// Open existing\nstream(memory&amp; mem, std::string_view name);\n</code></pre>"},{"location":"api_reference/#methods_10","title":"Methods","text":"Method Description Return Type <code>emit(value)</code> Push value to stream <code>bool</code> <code>subscribe(callback)</code> Add subscriber <code>subscription</code> <code>map(mem, name, func)</code> Transform elements <code>stream&lt;U&gt;</code> <code>filter(mem, name, pred)</code> Filter elements <code>stream&lt;T&gt;</code> <code>take(mem, name, count)</code> Take first n <code>stream&lt;T&gt;</code> <code>skip(mem, name, count)</code> Skip first n <code>stream&lt;T&gt;</code> <code>fold(mem, name, init, func)</code> Reduce to value <code>future&lt;S&gt;</code> <code>window(mem, name, size)</code> Group into windows <code>stream&lt;vector&lt;T&gt;&gt;</code> <code>merge(mem, name, other)</code> Combine streams <code>stream&lt;T&gt;</code> <code>zip(mem, name, other)</code> Pair elements <code>stream&lt;pair&lt;T,U&gt;&gt;</code> <code>close()</code> Close stream <code>void</code> <p>Example: <pre><code>stream&lt;double&gt; temps(mem, \"temperatures\", 1000);\n\n// Create processing pipeline\nauto warnings = temps\n    .map(mem, \"celsius\", [](double k) { return k - 273.15; })\n    .filter(mem, \"high\", [](double c) { return c &gt; 35.0; })\n    .window(mem, \"5min\", 300);\n\n// Subscribe to processed stream\nauto sub = warnings.subscribe([](auto window) {\n    double avg = std::accumulate(window.begin(), window.end(), 0.0) / window.size();\n    if (avg &gt; 37.0) send_heat_warning();\n});\n</code></pre></p>"},{"location":"api_reference/#stream-subscription","title":"Stream Subscription","text":"<pre><code>class subscription {\n    void unsubscribe();\n    bool is_active() const;\n};\n</code></pre>"},{"location":"api_reference/#channel","title":"Channel","text":"<p>CSP-style communication channel.</p> <pre><code>template&lt;typename T&gt;\nclass channel;\n</code></pre>"},{"location":"api_reference/#constructor_8","title":"Constructor","text":"<pre><code>// Unbuffered (synchronous)\nchannel(memory&amp; mem, std::string_view name);\n\n// Buffered (asynchronous)\nchannel(memory&amp; mem, std::string_view name, size_t buffer_size);\n\n// Open existing\nchannel(memory&amp; mem, std::string_view name, bool);\n</code></pre>"},{"location":"api_reference/#methods_11","title":"Methods","text":"Method Description Return Type <code>send(value)</code> Send value <code>bool</code> <code>receive()</code> Receive value <code>std::optional&lt;T&gt;</code> <code>try_send(value)</code> Non-blocking send <code>bool</code> <code>try_receive()</code> Non-blocking receive <code>std::optional&lt;T&gt;</code> <code>send_for(value, duration)</code> Send with timeout <code>bool</code> <code>receive_for(duration)</code> Receive with timeout <code>std::optional&lt;T&gt;</code> <code>close()</code> Close channel <code>void</code> <code>is_closed()</code> Check if closed <code>bool</code> <code>size()</code> Messages in buffer <code>size_t</code> <p>Example: <pre><code>// Unbuffered - synchronous rendezvous\nchannel&lt;Command&gt; commands(mem, \"cmds\");\n\n// Producer (blocks until consumer receives)\ncommands.send(Command{.type = START});\n\n// Consumer\nwhile (auto cmd = commands.receive()) {\n    execute(*cmd);\n}\n\n// Buffered - asynchronous up to capacity\nchannel&lt;Event&gt; events(mem, \"events\", 1000);\nevents.send(Event{.type = CLICK});  // Doesn't block unless full\n</code></pre></p>"},{"location":"api_reference/#select-operation","title":"Select Operation","text":"<p>Wait on multiple channels:</p> <pre><code>template&lt;typename... Channels&gt;\nclass select {\n    static auto receive(Channels&amp;... channels);\n};\n\n// Example\nauto result = select::receive(chan1, chan2, chan3);\nswitch (result.index()) {\n    case 0: process(std::get&lt;0&gt;(result)); break;\n    case 1: process(std::get&lt;1&gt;(result)); break;\n    case 2: process(std::get&lt;2&gt;(result)); break;\n}\n</code></pre>"},{"location":"api_reference/#type-requirements","title":"Type Requirements","text":"<p>All types used with ZeroIPC structures must be trivially copyable:</p> <pre><code>static_assert(std::is_trivially_copyable_v&lt;T&gt;);\n</code></pre> <p>This means: - No virtual functions - No user-defined copy/move constructors - No user-defined destructors - All members must be trivially copyable</p> <p>Valid Types: <pre><code>struct Point { float x, y, z; };           // \u2713 POD struct\nstruct Config { int id; char name[32]; };  // \u2713 C-style string\nusing Data = std::array&lt;double, 100&gt;;      // \u2713 Fixed array\n</code></pre></p> <p>Invalid Types: <pre><code>struct Bad1 { std::string name; };         // \u2717 std::string\nstruct Bad2 { virtual void foo(); };       // \u2717 Virtual function\nstruct Bad3 { std::vector&lt;int&gt; data; };    // \u2717 Dynamic allocation\n</code></pre></p>"},{"location":"api_reference/#error-handling","title":"Error Handling","text":"<p>All operations that can fail return: - <code>bool</code> for success/failure - <code>std::optional&lt;T&gt;</code> for values that might not exist - Exceptions only for constructor failures</p> <p>Example: <pre><code>queue&lt;int&gt; q(mem, \"myqueue\", 100);\n\n// Check return values\nif (!q.push(42)) {\n    // Queue was full\n}\n\nif (auto value = q.pop()) {\n    // Use *value\n} else {\n    // Queue was empty\n}\n\n// Exception handling for construction\ntry {\n    array&lt;float&gt; arr(mem, \"nonexistent\");  // Throws if not found\n} catch (const std::runtime_error&amp; e) {\n    std::cerr &lt;&lt; \"Array not found: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n}\n</code></pre></p>"},{"location":"api_reference/#thread-safety","title":"Thread Safety","text":"<p>All structures are designed for concurrent access:</p> Structure Producer Consumer Thread-Safe Array Multiple Multiple Atomic ops only Queue Multiple Multiple Lock-free Stack Multiple Multiple Lock-free Map Multiple Multiple Lock-free Set Multiple Multiple Lock-free Pool Multiple Multiple Lock-free Ring Single Single Memory barriers Future Single Multiple Atomic state Lazy Single Multiple Once computation Stream Multiple Multiple Lock-free buffer Channel Multiple Multiple Lock-free"},{"location":"api_reference/#memory-management","title":"Memory Management","text":""},{"location":"api_reference/#allocation-strategy","title":"Allocation Strategy","text":"<p>ZeroIPC uses bump allocation with no defragmentation:</p> <pre><code>memory mem(\"/data\", 100*1024*1024);  // 100MB\n\n// Each allocation advances the offset\narray&lt;int&gt; a1(mem, \"array1\", 1000);     // Offset: 0\nqueue&lt;float&gt; q1(mem, \"queue1\", 500);    // Offset: 4000\nstack&lt;double&gt; s1(mem, \"stack1\", 200);   // Offset: 6000\n</code></pre>"},{"location":"api_reference/#best-practices","title":"Best Practices","text":"<ol> <li>Pre-allocate: Size shared memory for all structures upfront</li> <li>Name carefully: Use hierarchical names (e.g., \"sensor/temp/raw\")</li> <li>Check existence: Use try/catch or check return values</li> <li>Clean shutdown: Unlink shared memory when done</li> <li>Monitor usage: Use <code>memory::available()</code> to track free space</li> </ol>"},{"location":"api_reference/#performance-characteristics","title":"Performance Characteristics","text":"Operation Complexity Notes Array access O(1) Direct memory access Queue push/pop O(1) Lock-free CAS Stack push/pop O(1) Lock-free CAS Map insert/lookup O(1) avg Linear probing Set insert/contains O(1) avg Hash-based Pool allocate O(1) Free list Ring read/write O(n) Bulk operations Future get O(1) After ready Lazy get O(1) After cached Stream emit O(1) Ring buffer Channel send/receive O(1) Queue-based"},{"location":"api_reference/#examples-repository","title":"Examples Repository","text":"<p>For complete working examples, see: - docs/examples/ - Categorized examples - cpp/tests/ - Unit tests showing usage - interop/ - Cross-language examples</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#system-architecture-overview","title":"System Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Application Layer                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502 Simulation  \u2502  \u2502   Renderer   \u2502  \u2502   Analytics  \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Codata Layer                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502 Future \u2502 \u2502  Lazy  \u2502 \u2502 Stream \u2502 \u2502 Channel \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                Data Structure Layer                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 Array  \u2502 \u2502 Queue  \u2502 \u2502 Stack \u2502 \u2502 Map  \u2502 \u2502  Pool  \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Foundation Layer                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Memory  \u2502  \u2502   Table  \u2502  \u2502  Language Bindings  \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    POSIX API Layer                        \u2502\n\u2502           shm_open, mmap, shm_unlink, futex              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Kernel Space                           \u2502\n\u2502          Virtual Memory, Page Tables, IPC                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#memory-layout","title":"Memory Layout","text":""},{"location":"architecture/#shared-memory-segment-structure","title":"Shared Memory Segment Structure","text":"<pre><code>Offset   Size     Component\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n0x0000   16       Table Header\n0x0010   Variable Metadata Table Entries\n0xXXXX   Variable Data Structure 1\n0xYYYY   Variable Data Structure 2\n...\n</code></pre>"},{"location":"architecture/#metadata-table-entry","title":"Metadata Table Entry","text":"<pre><code>Field    Size     Description\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nname     32       Structure identifier\noffset   4        Byte offset in segment\nsize     4        Total size in bytes\n</code></pre>"},{"location":"architecture/#component-design","title":"Component Design","text":""},{"location":"architecture/#1-memory-management","title":"1. Memory Management","text":"<p>Responsibilities: - POSIX shared memory lifecycle - Reference counting - Memory mapping - Automatic cleanup</p> <p>Key Design Decisions:</p> <ol> <li>Automatic Cleanup: Resources managed automatically</li> <li>Table Embedding: Metadata stored in shared memory</li> <li>Runtime Configuration: Table size determined at creation</li> <li>Language Independence: Each implementation manages its own way</li> </ol>"},{"location":"architecture/#2-table-discovery-system","title":"2. Table - Discovery System","text":"<p>Responsibilities: - Name-to-offset mapping - Dynamic structure discovery - Allocation tracking - Metadata storage</p> <p>Design Pattern: Service Locator</p> <ul> <li>Register structures by name</li> <li>Discover structures at runtime</li> <li>Language-agnostic lookup</li> </ul> <p>Trade-offs: - Fixed maximum entries (runtime-configured) - Linear search (simple, cache-friendly) - No defragmentation (predictable performance)</p> <p>Memory Management: Stack/Bump Allocation</p> <p>The implementation uses stack allocation (also called bump allocation):</p> <p>This means: - \u2705 Simple: O(1) allocation, no scanning - \u2705 Predictable: No fragmentation delays - \u274c No reclamation: Erased structures leave permanent gaps - \u274c Memory leak: Repeated create/delete exhausts memory</p> <p>Best Practices: 1. Initialize once: Create all structures at startup 2. Never erase: Treat structures as permanent 3. Use pools: For dynamic needs, use <code>shm_object_pool</code> 4. Size appropriately: Allocate enough memory upfront</p> <p>Future Improvement: A free-list allocator could reuse gaps, but adds complexity and potential fragmentation issues.</p>"},{"location":"architecture/#3-data-structure-implementations","title":"3. Data Structure Implementations","text":""},{"location":"architecture/#array-contiguous-storage","title":"Array - Contiguous Storage","text":"<p>Memory Layout: <pre><code>[Element][Element][Element][Element]...\n</code></pre></p> <p>Operations: - Direct indexing - Fixed size (no dynamic growth) - Zero-copy access</p>"},{"location":"architecture/#queue-lock-free-fifo","title":"Queue - Lock-Free FIFO","text":"<p>Memory Layout: <pre><code>[Header]\n  head index\n  tail index\n  capacity\n[Element][Element][Element]... (circular buffer)\n</code></pre></p> <p>Properties: - Lock-free enqueue/dequeue using atomic operations - Bounded capacity - Wait-free progress for single producer/consumer</p>"},{"location":"architecture/#stack-lock-free-lifo","title":"Stack - Lock-Free LIFO","text":"<p>Memory Layout: <pre><code>[Header]\n  top index\n  capacity\n[Element][Element][Element]...\n</code></pre></p> <p>Properties: - Lock-free push/pop using CAS - Bounded capacity - ABA problem mitigation</p>"},{"location":"architecture/#map-hash-table","title":"Map - Hash Table","text":"<p>Memory Layout: <pre><code>[Header]\n  bucket count\n  size\n[Bucket][Bucket][Bucket]...\n</code></pre></p> <p>Properties: - Linear probing for collision resolution - Fixed bucket count - Key-value storage</p>"},{"location":"architecture/#4-codata-structure-implementations","title":"4. Codata Structure Implementations","text":""},{"location":"architecture/#future-asynchronous-results","title":"Future - Asynchronous Results","text":"<p>Memory Layout: <pre><code>[Header]\n  state (PENDING/COMPUTING/READY/ERROR)\n  waiters count\n  completion time\n[Value Storage]\n[Error Message Buffer]\n</code></pre></p> <p>Properties: - Single-assignment semantics - Multiple readers supported - Timeout-based waiting - Error propagation</p>"},{"location":"architecture/#lazy-deferred-computation","title":"Lazy - Deferred Computation","text":"<p>Memory Layout: <pre><code>[Header]\n  computed flag\n  computing flag\n  version\n[Cached Value]\n[Computation State]\n</code></pre></p> <p>Properties: - Computation on first access - Automatic memoization - Thread-safe evaluation - Cache invalidation support</p>"},{"location":"architecture/#stream-reactive-data-flow","title":"Stream - Reactive Data Flow","text":"<p>Memory Layout: <pre><code>[Header]\n  sequence number\n  subscribers count\n  closed flag\n[Ring Buffer]\n[Transform Metadata]\n</code></pre></p> <p>Properties: - Push-based data flow - Functional operators (map, filter, fold) - Backpressure via ring buffer - Multi-cast to subscribers</p>"},{"location":"architecture/#channel-csp-communication","title":"Channel - CSP Communication","text":"<p>Memory Layout: <pre><code>[Header]\n  buffer capacity\n  read index\n  write index\n  closed flag\n[Circular Buffer]\n[Synchronization Primitives]\n</code></pre></p> <p>Properties: - Synchronous (unbuffered) or asynchronous (buffered) - FIFO ordering - Blocking/non-blocking operations - Select operation support</p>"},{"location":"architecture/#synchronization-strategies","title":"Synchronization Strategies","text":""},{"location":"architecture/#1-lock-free-algorithms","title":"1. Lock-Free Algorithms","text":"<p>Used In: Queue, Stack, Map operations</p> <p>Techniques: - Compare-And-Swap (CAS) - Atomic operations - Memory ordering semantics</p> <p>Benefits: - No kernel involvement - Bounded latency - Progress guarantee</p>"},{"location":"architecture/#2-wait-free-readers","title":"2. Wait-Free Readers","text":"<p>Used In: Array, read-only operations</p> <p>Guarantee: Readers never block</p>"},{"location":"architecture/#3-memory-ordering","title":"3. Memory Ordering","text":"<p>Consistency Model: - Acquire-release semantics for synchronization - Sequential consistency where needed - Relaxed ordering for counters</p>"},{"location":"architecture/#language-implementation-strategies","title":"Language Implementation Strategies","text":""},{"location":"architecture/#c-implementation","title":"C++ Implementation","text":"<p>Approach: - Template-based for compile-time optimization - Zero-overhead abstractions - RAII for resource management - Type constraints for safety</p>"},{"location":"architecture/#python-implementation","title":"Python Implementation","text":"<p>Approach: - Duck typing for flexibility - NumPy integration for performance - mmap for direct memory access - Runtime type specification</p>"},{"location":"architecture/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"architecture/#1-cache-line-alignment","title":"1. Cache Line Alignment","text":"<p>Strategy: Align data structures to cache line boundaries (typically 64 bytes)</p> <p>Prevents: False sharing between cores</p>"},{"location":"architecture/#2-memory-access-patterns","title":"2. Memory Access Patterns","text":"<p>Strategy: Sequential access patterns for prefetcher efficiency</p> <p>Benefit: Maximizes memory bandwidth utilization</p>"},{"location":"architecture/#3-bulk-operations","title":"3. Bulk Operations","text":"<p>Strategy: Process multiple elements in single operation</p> <p>Benefit: Amortizes overhead and improves throughput</p>"},{"location":"architecture/#4-zero-copy-design","title":"4. Zero-Copy Design","text":"<p>Strategy: Direct memory access without serialization</p> <p>Benefit: Eliminates copying overhead</p>"},{"location":"architecture/#error-handling-philosophy","title":"Error Handling Philosophy","text":""},{"location":"architecture/#1-creation-failures","title":"1. Creation Failures","text":"<p>Strategy: Report errors during structure creation</p> <p>Rationale: Fail fast at initialization</p>"},{"location":"architecture/#2-operation-failures","title":"2. Operation Failures","text":"<p>Strategy: Return success/failure indicators</p> <p>Rationale: Let caller decide on failure handling</p>"},{"location":"architecture/#3-resource-exhaustion","title":"3. Resource Exhaustion","text":"<p>Strategy: Graceful degradation</p> <p>Rationale: System continues with reduced capacity</p>"},{"location":"architecture/#scalability-considerations","title":"Scalability Considerations","text":""},{"location":"architecture/#process-scaling","title":"Process Scaling","text":"<p>Strategy: Read-heavy optimization</p> <pre><code>// Multiple readers, single writer pattern\nclass MRSW_Array {\n    T* data;  // No synchronization for reads\n    std::atomic&lt;uint64_t&gt; version;  // Version for consistency\n};\n</code></pre>"},{"location":"architecture/#memory-scaling","title":"Memory Scaling","text":"<p>Strategy: Hierarchical structures</p> <pre><code>// Two-level structure for large datasets\nclass LargeMap {\n    shm_array&lt;Bucket&gt; buckets;\n    shm_object_pool&lt;Node&gt; nodes;\n};\n</code></pre>"},{"location":"architecture/#numa-awareness","title":"NUMA Awareness","text":"<pre><code>// Pin shared memory to NUMA node\nvoid* addr = mmap(...);\nmbind(addr, size, MPOL_BIND, &amp;nodemask, ...);\n</code></pre>"},{"location":"architecture/#security-considerations","title":"Security Considerations","text":""},{"location":"architecture/#1-access-control","title":"1. Access Control","text":"<pre><code>// Set permissions on creation\nint fd = shm_open(name, O_CREAT | O_RDWR, 0660);\n</code></pre>"},{"location":"architecture/#2-input-validation","title":"2. Input Validation","text":"<pre><code>if (index &gt;= capacity)\n    throw std::out_of_range(\"...\");\n</code></pre>"},{"location":"architecture/#3-resource-limits","title":"3. Resource Limits","text":"<pre><code>static constexpr size_t MAX_ALLOCATION = 1ULL &lt;&lt; 30;  // 1GB\nif (size &gt; MAX_ALLOCATION)\n    throw std::invalid_argument(\"...\");\n</code></pre>"},{"location":"architecture/#future-directions","title":"Future Directions","text":""},{"location":"architecture/#1-additional-language-support","title":"1. Additional Language Support","text":"<ul> <li>Rust implementation for safety guarantees</li> <li>Go implementation for concurrent systems</li> <li>Java implementation via JNI or Panama</li> </ul>"},{"location":"architecture/#2-persistent-memory-support","title":"2. Persistent Memory Support","text":"<ul> <li>Intel Optane DC integration</li> <li>Battery-backed NVDIMM support</li> <li>Crash-consistent data structures</li> </ul>"},{"location":"architecture/#3-gpu-shared-memory","title":"3. GPU Shared Memory","text":"<ul> <li>CUDA unified memory integration</li> <li>OpenCL buffer sharing</li> <li>Heterogeneous computing support</li> </ul>"},{"location":"architecture/#4-distributed-shared-memory","title":"4. Distributed Shared Memory","text":"<ul> <li>RDMA support for cluster computing</li> <li>Network-transparent operations</li> <li>Coherence protocols</li> </ul>"},{"location":"architecture/#design-principles-summary","title":"Design Principles Summary","text":"<ol> <li>Zero-Overhead Abstraction: Pay only for what you use</li> <li>Lock-Free Where Possible: Minimize contention</li> <li>Cache-Conscious: Optimize for memory hierarchy</li> <li>Fail-Safe: Graceful degradation over crashes</li> <li>Discoverable: Named structures for flexibility</li> <li>Composable: Build complex from simple</li> <li>Type-Safe: Compile-time constraints</li> <li>RAII: Automatic resource management</li> </ol>"},{"location":"cli_tools/","title":"ZeroIPC CLI Tools Documentation","text":""},{"location":"cli_tools/#overview","title":"Overview","text":"<p>ZeroIPC provides command-line tools for inspecting, debugging, and monitoring shared memory segments and their data structures. These tools are essential for development, debugging, and operations.</p> <p>Current Version: 3.0 (Enhanced with full data structure support)</p> <p>Supported Structures: All 16 ZeroIPC data structures including: - Traditional: Array, Queue, Stack, Ring, Map, Set, Pool, Table - Synchronization: Semaphore, Barrier, Latch - Codata: Future, Lazy, Stream, Channel</p>"},{"location":"cli_tools/#zeroipc","title":"zeroipc","text":"<p>The primary inspection tool for ZeroIPC shared memory segments with comprehensive structure-specific commands.</p>"},{"location":"cli_tools/#installation","title":"Installation","text":"<pre><code>cd cpp\ncmake -B build .\ncmake --build build\nsudo cp build/tools/zeroipc /usr/local/bin/  # Optional: install system-wide\n</code></pre>"},{"location":"cli_tools/#commands","title":"Commands","text":""},{"location":"cli_tools/#list-list-all-zeroipc-segments","title":"list - List all ZeroIPC segments","text":"<p>Lists all shared memory segments that appear to be ZeroIPC-managed (contain valid table headers).</p> <pre><code>zeroipc list [options]\n</code></pre> <p>Options: - <code>--all</code> - Include non-ZeroIPC segments - <code>--details</code> - Show detailed information (size, permissions, etc.) - <code>--json</code> - Output in JSON format</p> <p>Example: <pre><code>$ zeroipc list --details\nNAME            SIZE        CREATED              PROCESSES  STRUCTURES\n/sensor_data    10485760    2024-01-15 14:23:01  3          5\n/analytics      52428800    2024-01-15 14:20:15  2          12\n/messages       1048576     2024-01-15 14:25:44  4          2\n\nTotal: 3 segments, 63.0 MB\n</code></pre></p>"},{"location":"cli_tools/#show-display-segment-information","title":"show - Display segment information","text":"<p>Shows detailed information about a specific shared memory segment.</p> <pre><code>zeroipc show &lt;segment_name&gt; [options]\n</code></pre> <p>Options: - <code>--structures</code> - List all data structures - <code>--metadata</code> - Show raw table metadata - <code>--json</code> - Output in JSON format</p> <p>Example: <pre><code>$ zeroipc show /sensor_data --structures\nSegment: /sensor_data\nSize: 10485760 bytes (10.0 MB)\nCreated: 2024-01-15 14:23:01\nLast modified: 2024-01-15 14:45:32\nProcesses attached: 3\n\nTable Information:\n  Version: 1.0\n  Max entries: 64\n  Used entries: 5\n  Next offset: 45632\n\nStructures:\n  NAME                TYPE        OFFSET    SIZE      DESCRIPTION\n  temperatures        array       1024      4000      Array&lt;float&gt;[1000]\n  pressure           array       5024      8000      Array&lt;double&gt;[1000]\n  temp_stream        stream      13024     16384     Stream&lt;double&gt;\n  alerts             queue       29408     8192      Queue&lt;Alert&gt;[256]\n  config             lazy        37600     8032      Lazy&lt;Config&gt;\n</code></pre></p>"},{"location":"cli_tools/#dump-dump-raw-memory-contents","title":"dump - Dump raw memory contents","text":"<p>Dumps raw bytes from a shared memory segment for debugging.</p> <pre><code>zeroipc dump &lt;segment_name&gt; [options]\n</code></pre> <p>Options: - <code>--offset &lt;n&gt;</code> - Starting offset (default: 0) - <code>--size &lt;n&gt;</code> - Number of bytes to dump (default: 256) - <code>--hex</code> - Display in hexadecimal (default) - <code>--binary</code> - Display in binary - <code>--ascii</code> - Show ASCII representation - <code>--output &lt;file&gt;</code> - Write to file instead of stdout</p> <p>Example: <pre><code>$ zeroipc dump /sensor_data --offset 1024 --size 64 --ascii\n00000400: 00 00 c8 41 00 00 ca 41 00 00 cc 41 00 00 ce 41  ...A...A...A...A\n00000410: 00 00 d0 41 00 00 d2 41 00 00 d4 41 00 00 d6 41  ...A...A...A...A\n00000420: 00 00 d8 41 00 00 da 41 00 00 dc 41 00 00 de 41  ...A...A...A...A\n00000430: 00 00 e0 41 00 00 e2 41 00 00 e4 41 00 00 e6 41  ...A...A...A...A\n</code></pre></p>"},{"location":"cli_tools/#monitor-monitor-data-structures-in-real-time","title":"monitor - Monitor data structures in real-time","text":"<p>Monitors a specific data structure, showing updates as they occur.</p> <pre><code>zeroipc monitor &lt;segment_name&gt; &lt;structure_name&gt; [options]\n</code></pre> <p>Options: - <code>--interval &lt;ms&gt;</code> - Update interval in milliseconds (default: 1000) - <code>--tail &lt;n&gt;</code> - For queues/streams, show last n items - <code>--filter &lt;expr&gt;</code> - Filter expression for values - <code>--json</code> - Output updates as JSON - <code>--csv</code> - Output as CSV for logging</p> <p>Example: <pre><code>$ zeroipc monitor /sensors temperature_stream --tail 5 --interval 500\nMonitoring: /sensors::temperature_stream (Stream&lt;double&gt;)\nUpdate interval: 500ms\nPress Ctrl+C to stop\n\n[14:45:32.123] 23.5\n[14:45:32.623] 23.7\n[14:45:33.124] 23.6\n[14:45:33.625] 23.8\n[14:45:34.126] 24.1\n^C\nSummary: 5 values observed, avg: 23.74, min: 23.5, max: 24.1\n</code></pre></p>"},{"location":"cli_tools/#structure-specific-commands-v30","title":"Structure-Specific Commands (v3.0)","text":"<p>The CLI tool now supports dedicated commands for each data structure type:</p>"},{"location":"cli_tools/#array-inspect-arrays","title":"array - Inspect arrays","text":"<pre><code>zeroipc array &lt;segment_name&gt; &lt;array_name&gt; [options]\n\nOptions:\n  --index &lt;n&gt;        Show specific index\n  --range &lt;start:end&gt; Show range of indices\n  --stats            Show array statistics (min, max, avg)\n  --type &lt;t&gt;         Interpret as type (int32, float, double, etc.)\n\nExample:\n$ zeroipc array /sensor_data temperatures --range 0:5 --type float\nArray: temperatures\nCapacity: 1000\nType: float (assumed)\nData:\n  [0]: 23.5\n  [1]: 23.7\n  [2]: 23.6\n  [3]: 23.8\n  [4]: 24.1\n</code></pre>"},{"location":"cli_tools/#queue-inspect-queues","title":"queue - Inspect queues","text":"<pre><code>zeroipc queue &lt;segment_name&gt; &lt;queue_name&gt; [options]\n\nOptions:\n  --peek &lt;n&gt;         Peek at first n elements without removing\n  --stats            Show queue statistics\n  --type &lt;t&gt;         Element type\n\nExample:\n$ zeroipc queue /tasks work_queue --peek 3\nQueue: work_queue\nCapacity: 1000\nHead: 42, Tail: 58\nCurrent size: 16\nFull: false, Empty: false\n\nPeek (first 3 elements):\n  [0]: Task{id=123, priority=5}\n  [1]: Task{id=124, priority=3}\n  [2]: Task{id=125, priority=8}\n</code></pre>"},{"location":"cli_tools/#stack-inspect-stacks","title":"stack - Inspect stacks","text":"<pre><code>zeroipc stack &lt;segment_name&gt; &lt;stack_name&gt; [options]\n\nOptions:\n  --peek &lt;n&gt;         Peek at top n elements\n  --stats            Show stack statistics\n\nExample:\n$ zeroipc stack /app undo_stack --peek 5\nStack: undo_stack\nCapacity: 500\nTop: 23\nCurrent size: 24\nFull: false, Empty: false\n\nTop 5 elements:\n  [23]: UndoAction{type=TEXT_EDIT, offset=452}\n  [22]: UndoAction{type=DELETE, offset=445}\n  [21]: UndoAction{type=INSERT, offset=440}\n  [20]: UndoAction{type=TEXT_EDIT, offset=420}\n  [19]: UndoAction{type=FORMAT, offset=400}\n</code></pre>"},{"location":"cli_tools/#ring-inspect-ring-buffers","title":"ring - Inspect ring buffers","text":"<pre><code>zeroipc ring &lt;segment_name&gt; &lt;ring_name&gt; [options]\n\nOptions:\n  --tail &lt;n&gt;         Show last n elements\n  --stats            Show ring buffer statistics\n  --monitor          Monitor in real-time\n\nExample:\n$ zeroipc ring /events event_log --tail 10\nRing Buffer: event_log\nCapacity: 10000\nHead: 5432, Tail: 5442\nCurrent size: 10\nOverwrite mode: true\n\nLast 10 events:\n  [5432]: Event{timestamp=1697123456, type=USER_LOGIN}\n  [5433]: Event{timestamp=1697123457, type=PAGE_VIEW}\n  [5434]: Event{timestamp=1697123458, type=CLICK}\n  ...\n</code></pre>"},{"location":"cli_tools/#map-inspect-hash-maps","title":"map - Inspect hash maps","text":"<pre><code>zeroipc map &lt;segment_name&gt; &lt;map_name&gt; [options]\n\nOptions:\n  --key &lt;k&gt;          Lookup specific key\n  --keys             List all keys\n  --stats            Show map statistics (load factor, collisions)\n\nExample:\n$ zeroipc map /cache user_sessions --stats\nHash Map: user_sessions\nCapacity: 1000\nCurrent entries: 347\nLoad factor: 34.7%\nLongest probe: 3\n\nStatistics:\n  Occupied buckets: 347\n  Empty buckets: 653\n  Average probe length: 1.2\n  Max probe length: 3\n</code></pre>"},{"location":"cli_tools/#set-inspect-hash-sets","title":"set - Inspect hash sets","text":"<pre><code>zeroipc set &lt;segment_name&gt; &lt;set_name&gt; [options]\n\nOptions:\n  --contains &lt;v&gt;     Check if value exists\n  --list             List all values\n  --stats            Show set statistics\n\nExample:\n$ zeroipc set /app unique_ids --stats\nHash Set: unique_ids\nCapacity: 5000\nCurrent elements: 1234\nLoad factor: 24.7%\n\nStatistics:\n  Occupied buckets: 1234\n  Empty buckets: 3766\n  Duplicates prevented: 456\n</code></pre>"},{"location":"cli_tools/#pool-inspect-object-pools","title":"pool - Inspect object pools","text":"<pre><code>zeroipc pool &lt;segment_name&gt; &lt;pool_name&gt; [options]\n\nOptions:\n  --stats            Show allocation statistics\n  --free-list        Show free list state\n\nExample:\n$ zeroipc pool /app connection_pool --stats\nObject Pool: connection_pool\nCapacity: 100\nObject size: 256 bytes\nCurrent allocations: 67\nFree objects: 33\n\nStatistics:\n  Total allocated: 67\n  Total freed: 245\n  Peak usage: 89\n  Fragmentation: low\n</code></pre>"},{"location":"cli_tools/#channel-inspect-channels","title":"channel - Inspect channels","text":"<pre><code>zeroipc channel &lt;segment_name&gt; &lt;channel_name&gt; [options]\n\nOptions:\n  --peek &lt;n&gt;         Peek at buffered messages\n  --stats            Show channel statistics\n\nExample:\n$ zeroipc channel /ipc message_chan --stats\nChannel: message_chan\nBuffer capacity: 100\nBuffered mode: true\nCurrent messages: 23\nSenders waiting: 0\nReceivers waiting: 2\n\nStatistics:\n  Messages sent: 45678\n  Messages received: 45655\n  Send blocks: 12\n  Receive blocks: 456\n</code></pre>"},{"location":"cli_tools/#semaphore-inspect-semaphores","title":"semaphore - Inspect semaphores","text":"<pre><code>zeroipc semaphore &lt;segment_name&gt; &lt;semaphore_name&gt;\n\nExample:\n$ zeroipc semaphore /sync resource_sem\nSemaphore: resource_sem\nCurrent count: 3\nMax count: 10 (counting semaphore)\nWaiting processes: 2\n\nStatus: 3 permits available, 2 processes waiting\n</code></pre>"},{"location":"cli_tools/#barrier-inspect-barriers","title":"barrier - Inspect barriers","text":"<pre><code>zeroipc barrier &lt;segment_name&gt; &lt;barrier_name&gt;\n\nExample:\n$ zeroipc barrier /sync phase_barrier\nBarrier: phase_barrier\nParticipants: 8\nArrived: 5\nGeneration: 42\nStatus: Waiting for 3 more participants\n</code></pre>"},{"location":"cli_tools/#latch-inspect-latches","title":"latch - Inspect latches","text":"<pre><code>zeroipc latch &lt;segment_name&gt; &lt;latch_name&gt;\n\nExample:\n$ zeroipc latch /sync startup_latch\nLatch: startup_latch\nInitial count: 10\nCurrent count: 3\nStatus: Waiting for 3 more counts to reach zero\n</code></pre>"},{"location":"cli_tools/#future-inspect-futures","title":"future - Inspect futures","text":"<pre><code>zeroipc future &lt;segment_name&gt; &lt;future_name&gt; [options]\n\nOptions:\n  --value            Show value if ready\n  --wait             Wait for value to become available\n\nExample:\n$ zeroipc future /compute result_future\nFuture: result_future\nState: READY\nValue available: yes\nValue: 42.7182818284\n\n$ zeroipc future /compute pending_future\nFuture: pending_future\nState: PENDING\nValue available: no\nWaiting processes: 3\n</code></pre>"},{"location":"cli_tools/#lazy-inspect-lazy-computations","title":"lazy - Inspect lazy computations","text":"<pre><code>zeroipc lazy &lt;segment_name&gt; &lt;lazy_name&gt;\n\nExample:\n$ zeroipc lazy /compute expensive_calc\nLazy: expensive_calc\nState: COMPUTED\nMemoized: yes\nValue: 3.14159265359\nComputation time: 2.345s (cached)\n</code></pre>"},{"location":"cli_tools/#stream-inspect-reactive-streams","title":"stream - Inspect reactive streams","text":"<pre><code>zeroipc stream &lt;segment_name&gt; &lt;stream_name&gt; [options]\n\nOptions:\n  --tail &lt;n&gt;         Show last n emitted values\n  --subscribers      Show subscriber count\n  --monitor          Monitor emissions in real-time\n\nExample:\n$ zeroipc stream /sensors temp_stream --tail 5\nStream: temp_stream\nBuffer size: 1000\nCurrent elements: 487\nSubscribers: 3\nEmissions/sec: 10.2\n\nLast 5 emissions:\n  [482]: 23.5 (10ms ago)\n  [483]: 23.7 (9ms ago)\n  [484]: 23.6 (8ms ago)\n  [485]: 23.8 (7ms ago)\n  [486]: 24.1 (6ms ago)\n</code></pre>"},{"location":"cli_tools/#read-read-values-from-data-structures","title":"read - Read values from data structures","text":"<p>Legacy read command (use structure-specific commands above for better output):</p> <pre><code>zeroipc read &lt;segment_name&gt; &lt;structure_name&gt; [options]\n</code></pre> <p>Options: - <code>--index &lt;n&gt;</code> - For arrays, specific index - <code>--range &lt;start:end&gt;</code> - Range of indices - <code>--key &lt;k&gt;</code> - For maps, specific key - <code>--type &lt;t&gt;</code> - Interpret as type (int32, float, double, etc.) - <code>--json</code> - Output as JSON</p> <p>Example: <pre><code>$ zeroipc read /sensor_data temperatures --range 0:10 --type float\ntemperatures[0]: 23.5\ntemperatures[1]: 23.7\ntemperatures[2]: 23.6\ntemperatures[3]: 23.8\ntemperatures[4]: 24.1\ntemperatures[5]: 24.0\ntemperatures[6]: 23.9\ntemperatures[7]: 24.2\ntemperatures[8]: 24.3\ntemperatures[9]: 24.1\n</code></pre></p>"},{"location":"cli_tools/#write-write-values-to-data-structures","title":"write - Write values to data structures","text":"<p>Writes values to a data structure (use with caution in production).</p> <pre><code>zeroipc write &lt;segment_name&gt; &lt;structure_name&gt; [options]\n</code></pre> <p>Options: - <code>--index &lt;n&gt;</code> - Array index to write - <code>--key &lt;k&gt;</code> - Map key to write - <code>--value &lt;v&gt;</code> - Value to write - <code>--type &lt;t&gt;</code> - Value type - <code>--stdin</code> - Read values from stdin</p> <p>Example: <pre><code>$ zeroipc write /sensor_data temperatures --index 0 --value 25.0 --type float\nWriting 25.0 to temperatures[0]\nSuccess: Value written\n\n# Bulk write from file\n$ cat values.txt | zeroipc write /sensor_data temperatures --stdin --type float\nWritten 100 values to temperatures\n</code></pre></p>"},{"location":"cli_tools/#stats-show-statistics","title":"stats - Show statistics","text":"<p>Displays statistics about a segment or structure.</p> <pre><code>zeroipc stats &lt;segment_name&gt; [structure_name] [options]\n</code></pre> <p>Options: - <code>--period &lt;s&gt;</code> - Stats for last n seconds - <code>--live</code> - Continuously update stats - <code>--json</code> - Output as JSON</p> <p>Example: <pre><code>$ zeroipc stats /sensor_data temp_stream --live\nStream: temp_stream\nType: Stream&lt;double&gt;\nBuffer size: 1024\nCurrent elements: 512\n\nStatistics (updating every 1s):\n  Messages/sec: 10.2\n  Bytes/sec: 81.6\n  Producers: 1\n  Consumers: 3\n  Buffer usage: 50.0%\n  Dropped: 0\n\n[14:46:00] rate: 10.2 msg/s, buffer: 50.0%, dropped: 0\n[14:46:01] rate: 11.1 msg/s, buffer: 52.3%, dropped: 0\n[14:46:02] rate: 9.8 msg/s, buffer: 49.8%, dropped: 0\n^C\n</code></pre></p>"},{"location":"cli_tools/#watch-watch-for-structure-changes","title":"watch - Watch for structure changes","text":"<p>Monitors a segment for new structures being created or destroyed.</p> <pre><code>zeroipc watch &lt;segment_name&gt; [options]\n</code></pre> <p>Options: - <code>--events</code> - Types of events to watch (create, destroy, modify) - <code>--json</code> - Output events as JSON</p> <p>Example: <pre><code>$ zeroipc watch /analytics --events create,destroy\nWatching /analytics for changes...\n[14:47:15] CREATED: correlation_matrix (Array&lt;double&gt;[10000])\n[14:47:23] CREATED: pca_result (Future&lt;PCAResult&gt;)\n[14:47:45] DESTROYED: temp_buffer\n[14:48:02] CREATED: model_weights (Array&lt;float&gt;[1000000])\n</code></pre></p>"},{"location":"cli_tools/#validate-validate-segment-integrity","title":"validate - Validate segment integrity","text":"<p>Checks a segment for corruption or inconsistencies.</p> <pre><code>zeroipc validate &lt;segment_name&gt; [options]\n</code></pre> <p>Options: - <code>--repair</code> - Attempt to repair issues (dangerous!) - <code>--verbose</code> - Show detailed validation steps</p> <p>Example: <pre><code>$ zeroipc validate /sensor_data --verbose\nValidating segment: /sensor_data\n\n[\u2713] Segment accessible\n[\u2713] Table header valid (magic: 0x5A49504D, version: 1.0)\n[\u2713] Table entries consistent (5 entries)\n[\u2713] No overlapping allocations\n[\u2713] All offsets within bounds\n[\u2713] Structure headers valid\n\nResult: VALID (no issues found)\n</code></pre></p>"},{"location":"cli_tools/#clean-clean-up-orphaned-segments","title":"clean - Clean up orphaned segments","text":"<p>Removes shared memory segments with no attached processes.</p> <pre><code>zeroipc clean [options]\n</code></pre> <p>Options: - <code>--dry-run</code> - Show what would be cleaned without doing it - <code>--force</code> - Clean even if processes attached - <code>--pattern &lt;p&gt;</code> - Only clean segments matching pattern</p> <p>Example: <pre><code>$ zeroipc clean --dry-run\nFound 3 orphaned segments:\n  /test_12345 (1.2 MB) - last accessed 2 days ago\n  /tmp_worker_7 (256 KB) - last accessed 5 hours ago\n  /benchmark_old (50 MB) - last accessed 1 week ago\n\nTotal: 51.5 MB would be freed\nRun without --dry-run to actually clean\n</code></pre></p>"},{"location":"cli_tools/#advanced-usage","title":"Advanced Usage","text":""},{"location":"cli_tools/#scripting-with-json-output","title":"Scripting with JSON Output","text":"<pre><code>#!/bin/bash\n# Monitor queue depth and alert if too high\nwhile true; do\n    depth=$(zeroipc stats /app message_queue --json | jq '.current_elements')\n    if [ $depth -gt 900 ]; then\n        send_alert \"Queue depth critical: $depth\"\n    fi\n    sleep 5\ndone\n</code></pre>"},{"location":"cli_tools/#continuous-monitoring-pipeline","title":"Continuous Monitoring Pipeline","text":"<pre><code># Log stream values to file with timestamps\nzeroipc monitor /sensors temp_stream --csv | \\\n    awk '{print strftime(\"%Y-%m-%d %H:%M:%S\"), $0}' &gt;&gt; sensor_log.csv\n</code></pre>"},{"location":"cli_tools/#debugging-memory-leaks","title":"Debugging Memory Leaks","text":"<pre><code># Track memory usage over time\nwhile true; do\n    zeroipc show /app --json | \\\n        jq '{time: now, used: .next_offset, free: (.size - .next_offset)}' &gt;&gt; memory_usage.jsonl\n    sleep 60\ndone\n</code></pre>"},{"location":"cli_tools/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>ZEROIPC_INSPECT_COLOR</code> - Enable/disable colored output (auto, always, never)</li> <li><code>ZEROIPC_INSPECT_FORMAT</code> - Default output format (text, json, csv)</li> <li><code>ZEROIPC_INSPECT_VERBOSE</code> - Verbose logging (0-3)</li> </ul>"},{"location":"cli_tools/#configuration-file","title":"Configuration File","text":"<p>Create <code>~/.zeroipc/inspect.conf</code>:</p> <pre><code>[general]\ncolor = auto\nformat = text\nverbose = 1\n\n[monitor]\ndefault_interval = 1000\nmax_tail = 100\n\n[aliases]\nsensors = /sensor_data\napp = /application_main\n</code></pre>"},{"location":"cli_tools/#future-tools-planned","title":"Future Tools (Planned)","text":""},{"location":"cli_tools/#zeroipc-bench","title":"zeroipc-bench","text":"<p>Performance benchmarking tool: <pre><code>zeroipc-bench queue --size 1000 --producers 4 --consumers 2\nzeroipc-bench stream --rate 10000 --duration 60\n</code></pre></p>"},{"location":"cli_tools/#zeroipc-replay","title":"zeroipc-replay","text":"<p>Replay captured data: <pre><code>zeroipc-replay capture.bin --segment /test --speed 2.0\n</code></pre></p>"},{"location":"cli_tools/#zeroipc-trace","title":"zeroipc-trace","text":"<p>Trace operations on structures: <pre><code>zeroipc-trace /app --operations push,pop,emit --output trace.log\n</code></pre></p>"},{"location":"cli_tools/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli_tools/#common-issues","title":"Common Issues","text":""},{"location":"cli_tools/#permission-denied","title":"Permission Denied","text":"<pre><code>$ zeroipc show /app\nError: Permission denied accessing /app\n\n# Fix: Check permissions\nls -la /dev/shm/app\n\n# Fix: Run with appropriate user\nsudo zeroipc show /app\n</code></pre>"},{"location":"cli_tools/#segment-not-found","title":"Segment Not Found","text":"<pre><code>$ zeroipc show /missing\nError: Shared memory segment /missing not found\n\n# Check if it exists\nls /dev/shm/ | grep missing\n\n# List all segments\nzeroipc list --all\n</code></pre>"},{"location":"cli_tools/#invalid-table-header","title":"Invalid Table Header","text":"<pre><code>$ zeroipc show /corrupt\nError: Invalid table header (magic number mismatch)\n\n# This segment may not be ZeroIPC-managed or is corrupted\n# Validate to check\nzeroipc validate /corrupt\n</code></pre>"},{"location":"cli_tools/#debug-mode","title":"Debug Mode","text":"<p>Enable debug output: <pre><code>ZEROIPC_INSPECT_VERBOSE=3 zeroipc show /app\n</code></pre></p>"},{"location":"cli_tools/#getting-help","title":"Getting Help","text":"<pre><code># General help\nzeroipc --help\n\n# Command-specific help\nzeroipc monitor --help\n\n# Version information\nzeroipc --version\n</code></pre>"},{"location":"cli_tools/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Monitoring: Set up automated monitoring for production systems</li> <li>Clean Regularly: Schedule cleanup of orphaned segments</li> <li>Validate After Crashes: Always validate segments after unexpected shutdowns</li> <li>Use JSON for Automation: Parse JSON output in scripts for reliability</li> <li>Archive Stats: Keep historical statistics for capacity planning</li> <li>Access Control: Limit write operations in production environments</li> <li>Backup Critical Data: Dump important segments before maintenance</li> </ol>"},{"location":"cli_tools/#integration-examples","title":"Integration Examples","text":""},{"location":"cli_tools/#prometheus-exporter","title":"Prometheus Exporter","text":"<pre><code>#!/usr/bin/env python3\nimport subprocess\nimport json\nfrom prometheus_client import start_http_server, Gauge\n\nqueue_depth = Gauge('zeroipc_queue_depth', 'Queue depth', ['segment', 'queue'])\nstream_rate = Gauge('zeroipc_stream_rate', 'Stream message rate', ['segment', 'stream'])\n\ndef collect_metrics():\n    segments = json.loads(subprocess.check_output(\n        ['zeroipc', 'list', '--json']\n    ))\n\n    for seg in segments:\n        stats = json.loads(subprocess.check_output(\n            ['zeroipc', 'stats', seg['name'], '--json']\n        ))\n\n        for struct in stats['structures']:\n            if struct['type'] == 'queue':\n                queue_depth.labels(seg['name'], struct['name']).set(struct['depth'])\n            elif struct['type'] == 'stream':\n                stream_rate.labels(seg['name'], struct['name']).set(struct['rate'])\n\nif __name__ == '__main__':\n    start_http_server(8000)\n    while True:\n        collect_metrics()\n        time.sleep(10)\n</code></pre>"},{"location":"cli_tools/#grafana-dashboard","title":"Grafana Dashboard","text":"<pre><code>{\n  \"dashboard\": {\n    \"title\": \"ZeroIPC Monitoring\",\n    \"panels\": [\n      {\n        \"title\": \"Queue Depths\",\n        \"targets\": [\n          {\n            \"expr\": \"zeroipc_queue_depth\",\n            \"legendFormat\": \"{{segment}}/{{queue}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Stream Rates\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(zeroipc_stream_rate[5m])\",\n            \"legendFormat\": \"{{segment}}/{{stream}}\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"codata_guide/","title":"Codata Guide: Computational Structures in Shared Memory","text":""},{"location":"codata_guide/#introduction","title":"Introduction","text":"<p>ZeroIPC extends beyond traditional data structures to support codata - computational structures that represent processes, computations, and potentially infinite data flows. While data structures answer \"what values are stored?\", codata structures answer \"how are values computed?\"</p> <p>This guide explains the theory, motivation, and practical applications of codata in ZeroIPC.</p>"},{"location":"codata_guide/#data-vs-codata-a-fundamental-distinction","title":"Data vs Codata: A Fundamental Distinction","text":""},{"location":"codata_guide/#data-finite-eager-pull-based","title":"Data (Finite, Eager, Pull-based)","text":"<p>Traditional data structures store concrete values: - Array: Fixed collection of values - Map: Key-value associations - Queue: FIFO buffer of elements</p> <p>These are eager - values exist in memory and are pulled when needed.</p>"},{"location":"codata_guide/#codata-potentially-infinite-lazy-push-based","title":"Codata (Potentially Infinite, Lazy, Push-based)","text":"<p>Codata structures represent computations and processes: - Future: A value that will exist - Lazy: A computation that might be evaluated - Stream: Potentially infinite sequence of values - Channel: Communication process between threads</p> <p>These are lazy - values are computed on demand or pushed when available.</p>"},{"location":"codata_guide/#the-four-pillars-of-codata-in-zeroipc","title":"The Four Pillars of Codata in ZeroIPC","text":""},{"location":"codata_guide/#1-future-asynchronous-results","title":"1. Future: Asynchronous Results <p>Concept: A Future represents a value that will be available at some point in time. It enables asynchronous computation where producers and consumers are temporally decoupled.</p> <p>Mathematical Model: Future \u2248 Time \u2192 Option <p>Use Cases: - Parallel simulations where processes compute partial results - Long-running computations that shouldn't block consumers - Cross-process async/await patterns</p> <p>Example: <pre><code>// Process A: Starts expensive computation\nMemory mem(\"/simulation\", 100*1024*1024);\nFuture&lt;SimResult&gt; result(mem, \"final_state\");\n\n// Launch computation\nstd::thread([&amp;result]() {\n    SimResult r = run_monte_carlo(1000000);\n    result.set_value(r);\n}).detach();\n\n// Process B: Can check or wait for result\nMemory mem(\"/simulation\");\nFuture&lt;SimResult&gt; result(mem, \"final_state\", true);\n\n// Non-blocking check\nif (result.is_ready()) {\n    process(result.get());\n}\n\n// Blocking wait with timeout\nif (auto val = result.get_for(5s)) {\n    process(*val);\n}\n</code></pre></p> <p>Key Properties: - Single Assignment: Once set, value is immutable - Multiple Readers: Many processes can wait on same future - Error Propagation: Can set error state instead of value - Timeout Support: Consumers can specify maximum wait time</p>","text":""},{"location":"codata_guide/#2-lazy-deferred-computation-with-memoization","title":"2. Lazy: Deferred Computation with Memoization <p>Concept: Lazy evaluation defers computation until the value is actually needed. Once computed, the result is memoized (cached) for all future accesses.</p> <p>Mathematical Model: Lazy \u2248 () \u2192 T with memoization <p>Use Cases: - Expensive computations that might not be needed - Shared computation results across processes - Configuration values computed from complex logic - Caching of derived data</p> <p>Example: <pre><code>// Process A: Sets up lazy computation\nMemory mem(\"/cache\", 50*1024*1024);\nLazy&lt;Matrix&gt; inverse(mem, \"matrix_inverse\");\n\n// Define computation (not executed yet!)\ninverse.set_computation([&amp;original_matrix]() {\n    return compute_inverse(original_matrix);  // Expensive!\n});\n\n// Process B: First access triggers computation\nMemory mem(\"/cache\");\nLazy&lt;Matrix&gt; inverse(mem, \"matrix_inverse\", true);\n\nMatrix m = inverse.get();  // Computes and caches\n// All subsequent calls return cached value instantly\n\n// Process C: Gets cached result\nMatrix m2 = inverse.get();  // Returns immediately\n</code></pre></p> <p>Key Properties: - Lazy Evaluation: Computation deferred until first access - Automatic Memoization: Result cached after first computation - Thread-Safe: Multiple threads can request simultaneously - Invalidation: Can mark as stale to force recomputation</p>","text":""},{"location":"codata_guide/#3-stream-reactive-data-flows","title":"3. Stream: Reactive Data Flows <p>Concept: Streams represent potentially infinite sequences of values over time. They enable functional reactive programming (FRP) with composable operators.</p> <p>Mathematical Model: Stream \u2248 Time \u2192 List <p>Use Cases: - Sensor data processing pipelines - Event-driven architectures - Real-time analytics - Pub-sub messaging systems</p> <p>Example: <pre><code>// Sensor process: Emits temperature readings\nMemory mem(\"/sensors\", 10*1024*1024);\nStream&lt;Reading&gt; temps(mem, \"temperature\", 1000);\n\nwhile (running) {\n    temps.emit(read_sensor());\n    sleep(100ms);\n}\n\n// Analytics process: Complex processing pipeline\nMemory mem(\"/sensors\");\nStream&lt;Reading&gt; temps(mem, \"temperature\");\n\n// Functional transformation pipeline\nauto celsius = temps\n    .map(mem, \"celsius\", [](Reading r) { \n        return r.value; \n    });\n\nauto fahrenheit = celsius\n    .map(mem, \"fahrenheit\", [](double c) { \n        return c * 9/5 + 32; \n    });\n\nauto warnings = fahrenheit\n    .filter(mem, \"warnings\", [](double f) { \n        return f &gt; 100.0; \n    })\n    .window(mem, \"warning_window\", 10)  // Group into windows\n    .map(mem, \"avg_warning\", [](auto window) {\n        return std::accumulate(window.begin(), window.end(), 0.0) / window.size();\n    });\n\n// Subscribe to processed stream\nwarnings.subscribe([](double avg_high_temp) {\n    if (avg_high_temp &gt; 105.0) {\n        trigger_emergency_cooling();\n    }\n});\n</code></pre></p> <p>Stream Operators:</p>    Operator Description Type Signature     map Transform each element <code>Stream&lt;T&gt; \u2192 (T \u2192 U) \u2192 Stream&lt;U&gt;</code>   filter Keep matching elements <code>Stream&lt;T&gt; \u2192 (T \u2192 bool) \u2192 Stream&lt;T&gt;</code>   fold Reduce to single value <code>Stream&lt;T&gt; \u2192 (S \u2192 T \u2192 S) \u2192 S \u2192 Future&lt;S&gt;</code>   take Take first n elements <code>Stream&lt;T&gt; \u2192 int \u2192 Stream&lt;T&gt;</code>   skip Skip first n elements <code>Stream&lt;T&gt; \u2192 int \u2192 Stream&lt;T&gt;</code>   window Group into windows <code>Stream&lt;T&gt; \u2192 int \u2192 Stream&lt;List&lt;T&gt;&gt;</code>   merge Combine two streams <code>Stream&lt;T&gt; \u2192 Stream&lt;T&gt; \u2192 Stream&lt;T&gt;</code>   zip Pair elements from streams <code>Stream&lt;T&gt; \u2192 Stream&lt;U&gt; \u2192 Stream&lt;(T,U)&gt;</code>    <p>Key Properties: - Backpressure: Ring buffer prevents overwhelming consumers - Multi-cast: Multiple consumers can process same stream - Composable: Operators can be chained functionally - Lazy Subscription: Processing only happens with subscribers</p>","text":""},{"location":"codata_guide/#4-channel-csp-style-communication","title":"4. Channel: CSP-Style Communication <p>Concept: Channels provide synchronous communication between processes, inspired by Go channels and CSP (Communicating Sequential Processes).</p> <p>Mathematical Model: Channel \u2248 Process \u2192 Process communication primitive <p>Use Cases: - Task distribution systems - Request-response patterns - Synchronization between processes - Structured concurrency</p> <p>Example: <pre><code>// Worker pool pattern\nMemory mem(\"/workers\", 50*1024*1024);\n\n// Buffered channel for tasks\nChannel&lt;Task&gt; tasks(mem, \"task_queue\", 100);\nChannel&lt;Result&gt; results(mem, \"results\", 100);\n\n// Dispatcher process\nfor (auto&amp; task : work_items) {\n    tasks.send(task);  // Blocks if buffer full\n}\ntasks.close();  // Signal completion\n\n// Worker processes (multiple instances)\nwhile (auto task = tasks.receive()) {\n    Result r = process_task(*task);\n    results.send(r);\n}\n\n// Aggregator process\nstd::vector&lt;Result&gt; all_results;\nwhile (auto result = results.receive()) {\n    all_results.push_back(*result);\n}\n</code></pre></p> <p>Channel Types: - Unbuffered: Synchronous rendezvous (sender blocks until receiver ready) - Buffered: Asynchronous up to buffer capacity - Closing: Can signal no more values will be sent</p> <p>Key Properties: - FIFO Ordering: Messages preserve order - Select Operation: Can wait on multiple channels - Deadlock Prevention: Timeout support on operations - Type Safety: Compile-time type checking in C++</p>","text":""},{"location":"codata_guide/#theoretical-foundation","title":"Theoretical Foundation","text":""},{"location":"codata_guide/#category-theory-perspective","title":"Category Theory Perspective <p>In category theory, data and codata are dual concepts:</p> <ul> <li>Data: Initial algebras (constructed by introduction rules)</li> <li>Built bottom-up from constructors</li> <li>Pattern matching for destruction</li> <li> <p>Example: <code>List = Nil | Cons(head, tail)</code></p> </li> <li> <p>Codata: Final coalgebras (defined by elimination rules)</p> </li> <li>Defined by observations/projections</li> <li>Copattern matching for construction</li> <li>Example: <code>Stream = {head: T, tail: Stream&lt;T&gt;}</code></li> </ul>","text":""},{"location":"codata_guide/#operational-semantics","title":"Operational Semantics <p>Data Evaluation (Call-by-value): <pre><code>evaluate(Array[i]) = memory[base + i * sizeof(T)]\n</code></pre></p> <p>Codata Evaluation (Call-by-need): <pre><code>evaluate(Lazy.get()) = \n    if cached then cached_value\n    else cached_value = compute(); cached_value\n</code></pre></p>","text":""},{"location":"codata_guide/#coinduction-and-infinite-structures","title":"Coinduction and Infinite Structures <p>Streams demonstrate coinduction - defining infinite structures by their observations:</p> <pre><code>// Infinite stream of Fibonacci numbers\nStream&lt;int&gt; fibonacci(Memory&amp; mem) {\n    return Stream&lt;int&gt;::unfold(mem, \"fib\", \n        std::pair{0, 1},\n        [](auto state) {\n            auto [a, b] = state;\n            return std::pair{a, std::pair{b, a + b}};\n        });\n}\n</code></pre>","text":""},{"location":"codata_guide/#design-patterns-with-codata","title":"Design Patterns with Codata","text":""},{"location":"codata_guide/#1-pipeline-pattern","title":"1. Pipeline Pattern <p>Chain stream transformations for data processing: <pre><code>source \u2192 map \u2192 filter \u2192 window \u2192 fold \u2192 sink\n</code></pre></p>","text":""},{"location":"codata_guide/#2-fork-join-pattern","title":"2. Fork-Join Pattern <p>Split computation, process in parallel, join results: <pre><code>auto f1 = Future&lt;T1&gt;(mem, \"branch1\");\nauto f2 = Future&lt;T2&gt;(mem, \"branch2\");\nauto result = f1.combine(f2, [](T1 a, T2 b) { return merge(a, b); });\n</code></pre></p>","text":""},{"location":"codata_guide/#3-supervisor-pattern","title":"3. Supervisor Pattern <p>Monitor and restart failed computations: <pre><code>Future&lt;T&gt; supervised(Memory&amp; mem, std::function&lt;T()&gt; computation) {\n    Future&lt;T&gt; result(mem, \"supervised_result\");\n    while (!result.is_ready()) {\n        try {\n            result.set_value(computation());\n        } catch (...) {\n            result.set_error(\"Computation failed, retrying...\");\n            sleep(1s);\n            result.reset();  // Clear error state\n        }\n    }\n    return result;\n}\n</code></pre></p>","text":""},{"location":"codata_guide/#4-reactive-state-machine","title":"4. Reactive State Machine <p>Use streams to model state transitions: <pre><code>Stream&lt;Event&gt; events(mem, \"events\");\nStream&lt;State&gt; states = events.scan(mem, \"states\", \n    InitialState{}, \n    [](State s, Event e) { return transition(s, e); });\n</code></pre></p>","text":""},{"location":"codata_guide/#performance-considerations","title":"Performance Considerations","text":""},{"location":"codata_guide/#memory-layout","title":"Memory Layout <ul> <li>Future: Fixed header + value storage</li> <li>Lazy: Header + cached value + computation state</li> <li>Stream: Header + ring buffer for elements</li> <li>Channel: Header + circular buffer + semaphores</li> </ul>","text":""},{"location":"codata_guide/#concurrency-overhead","title":"Concurrency Overhead <ul> <li>Lock-free: All structures use atomic operations</li> <li>Cache-friendly: Data locality optimized</li> <li>Minimal contention: CAS loops with exponential backoff</li> </ul>","text":""},{"location":"codata_guide/#best-practices","title":"Best Practices <ol> <li>Buffer Sizing: Choose stream/channel buffers based on production rate</li> <li>Timeout Usage: Always use timeouts to prevent indefinite blocking</li> <li>Error Handling: Propagate errors through Future/Lazy error states</li> <li>Resource Management: Close channels and streams when done</li> <li>Composition: Build complex behaviors from simple stream operators</li> </ol>","text":""},{"location":"codata_guide/#real-world-applications","title":"Real-World Applications","text":""},{"location":"codata_guide/#high-frequency-trading","title":"High-Frequency Trading <pre><code>Stream&lt;MarketData&gt; quotes(mem, \"quotes\");\nauto signals = quotes\n    .window(mem, \"1min\", duration(1min))\n    .map(mem, \"vwap\", calculate_vwap)\n    .filter(mem, \"triggers\", is_trade_signal);\n</code></pre>","text":""},{"location":"codata_guide/#distributed-simulation","title":"Distributed Simulation <pre><code>Future&lt;State&gt; states[N];\nfor (int i = 0; i &lt; N; i++) {\n    states[i] = Future&lt;State&gt;(mem, \"state_\" + std::to_string(i));\n}\nauto final_state = Future&lt;State&gt;::all(states).map(combine_states);\n</code></pre>","text":""},{"location":"codata_guide/#iot-data-pipeline","title":"IoT Data Pipeline <pre><code>Stream&lt;SensorData&gt; raw(mem, \"raw_sensor\");\nauto processed = raw\n    .filter(mem, \"valid\", validate)\n    .map(mem, \"normalized\", normalize)\n    .window(mem, \"5min\", 5min)\n    .map(mem, \"aggregated\", aggregate)\n    .foreach(store_to_database);\n</code></pre>","text":""},{"location":"codata_guide/#comparison-with-traditional-ipc","title":"Comparison with Traditional IPC","text":"Aspect Traditional IPC ZeroIPC Codata Model Message passing Computational substrate Coupling Tight (sender/receiver) Loose (producer/consumer) Timing Synchronous Asynchronous/Reactive Composition Manual Functional operators Patterns Request-response Streams, futures, lazy Overhead Serialization Zero-copy"},{"location":"codata_guide/#conclusion","title":"Conclusion","text":"<p>Codata in ZeroIPC transforms shared memory from a passive storage medium into an active computational substrate. By bringing functional programming concepts like lazy evaluation, reactive streams, and futures to IPC, ZeroIPC enables sophisticated cross-process coordination patterns with minimal overhead.</p> <p>The key insight is that computation itself becomes a first-class citizen in shared memory, not just data. This opens new possibilities for distributed systems, parallel processing, and reactive architectures.</p>"},{"location":"codata_guide/#further-reading","title":"Further Reading","text":"<ul> <li>The Essence of Dataflow Programming</li> <li>Codata in Action</li> <li>Functional Reactive Programming</li> <li>CSP and Go Channels</li> </ul>"},{"location":"conan_submission/","title":"Conan Center Index Submission Guide","text":""},{"location":"conan_submission/#steps-to-submit-posix_shm-to-conan-center","title":"Steps to Submit posix_shm to Conan Center","text":"<ol> <li> <p>Fork the Conan Center Index repository <pre><code>git clone https://github.com/conan-io/conan-center-index.git\ncd conan-center-index\n</code></pre></p> </li> <li> <p>Create a new branch <pre><code>git checkout -b posix_shm-1.0.0\n</code></pre></p> </li> <li> <p>Create the recipe directory structure <pre><code>mkdir -p recipes/posix_shm/all\nmkdir -p recipes/posix_shm/config.yml\n</code></pre></p> </li> <li> <p>Copy our conanfile.py to the recipe directory</p> </li> <li>Copy <code>/home/spinoza/github/repos/posix_shm/conanfile.py</code> to <code>recipes/posix_shm/all/conanfile.py</code></li> <li> <p>Update the recipe to download from GitHub release instead of using local files</p> </li> <li> <p>Create config.yml <pre><code>versions:\n  \"1.0.0\":\n    folder: all\n</code></pre></p> </li> <li> <p>Test the recipe locally <pre><code>conan create recipes/posix_shm/all@ --version=1.0.0\n</code></pre></p> </li> <li> <p>Create test package <pre><code>mkdir -p recipes/posix_shm/all/test_package\n</code></pre></p> </li> </ol> <p>Create <code>test_package/conanfile.py</code>:    <pre><code>from conan import ConanFile\nfrom conan.tools.build import can_run\nfrom conan.tools.cmake import cmake_layout, CMake\nimport os\n\nclass TestPackageConan(ConanFile):\n    settings = \"os\", \"arch\", \"compiler\", \"build_type\"\n    generators = \"CMakeDeps\", \"CMakeToolchain\", \"VirtualRunEnv\"\n    test_type = \"explicit\"\n\n    def requirements(self):\n        self.requires(self.tested_reference_str)\n\n    def layout(self):\n        cmake_layout(self)\n\n    def build(self):\n        cmake = CMake(self)\n        cmake.configure()\n        cmake.build()\n\n    def test(self):\n        if can_run(self):\n            bin_path = os.path.join(self.cpp.build.bindir, \"test_package\")\n            self.run(bin_path, env=\"conanrun\")\n</code></pre></p> <ol> <li>Submit Pull Request</li> <li>Commit changes</li> <li>Push to your fork</li> <li>Open PR to conan-io/conan-center-index</li> <li>Title: \"[posix_shm] Add posix_shm/1.0.0\"</li> <li>Follow the PR template</li> </ol>"},{"location":"conan_submission/#recipe-updates-needed","title":"Recipe Updates Needed","text":"<p>The current conanfile.py needs these modifications for Conan Center:</p> <ol> <li> <p>Change source to download from GitHub:    <pre><code>def source(self):\n    get(self, **self.conan_data[\"sources\"][self.version], strip_root=True)\n</code></pre></p> </li> <li> <p>Add conandata.yml:    <pre><code>sources:\n  \"1.0.0\":\n    url: \"https://github.com/queelius/posix_shm/archive/v1.0.0.tar.gz\"\n    sha256: \"&lt;calculate-sha256&gt;\"\n</code></pre></p> </li> <li> <p>Remove local file references</p> </li> </ol>"},{"location":"design_philosophy/","title":"Design Philosophy","text":""},{"location":"design_philosophy/#core-philosophy-simplicity-through-constraints","title":"Core Philosophy: Simplicity Through Constraints","text":"<p>\"Perfection is achieved not when there is nothing more to add, but when there is nothing left to take away.\" - Antoine de Saint-Exup\u00e9ry</p> <p>ZeroIPC makes deliberate constraints that enable extraordinary simplicity and performance. We're not trying to be a general-purpose shared memory allocator. We're building fast, cross-language IPC.</p>"},{"location":"design_philosophy/#the-vision","title":"The Vision","text":"<p>We set out to solve these problems: - Cross-language IPC between C++, Python, and other languages - Zero-copy data sharing through shared memory - Lock-free operations where algorithmically possible - Simple discovery via named structures - Minimal metadata for true language independence</p> <p>What we explicitly did NOT try to build: - \u274c A general-purpose memory allocator - \u274c A replacement for malloc/new - \u274c A distributed computing framework - \u274c A database or persistence layer</p>"},{"location":"design_philosophy/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"design_philosophy/#1-stack-allocation-simple-by-design","title":"1. Stack Allocation: Simple by Design","text":"<p>The Decision: We use stack/bump allocation - memory only grows forward, no individual deallocation.</p> <p>What we do (simple): - Allocate by incrementing offset - O(1) allocation time - No fragmentation</p> <p>What we DON'T do (complex): - Free lists - Best-fit algorithms - Defragmentation - Complex allocator metadata</p> <p>Why This Is The Right Choice:</p> <p>\u2705 Matches Simulation Patterns - Simulations initialize structures once at startup - Structures live for entire program lifetime - Dynamic needs handled by <code>shm_object_pool</code> (which DOES have a free list)</p> <p>\u2705 Fastest Possible Allocation - O(1) with just addition - No scanning, no fragmentation - Predictable memory layout</p> <p>\u2705 Eliminates Entire Classes of Bugs - No use-after-free - No double-free - No fragmentation issues - No allocator metadata corruption</p> <p>The Trade-off: - Can't deallocate individual structures - Can \"leak\" if you repeatedly create/destroy</p> <p>Our Answer: - Don't repeatedly create/destroy! - Initialize once, use forever - This constraint makes the system simpler AND faster</p>"},{"location":"design_philosophy/#2-runtime-configured-tables-balance-of-flexibility","title":"2. Runtime-Configured Tables: Balance of Flexibility","text":"<p>The Decision: Table size determined at creation time, not compile time.</p> <p>Why This Is The Right Choice:</p> <p>\u2705 Zero Dynamic Allocation - No hidden malloc calls - No surprising memory spikes - Works in constrained environments</p> <p>\u2705 Predictable Performance - Lookup is always O(n) where n \u2264 MAX_ENTRIES - Memory usage known at compile time - No reallocation pauses</p> <p>\u2705 Runtime Flexibility - Choose table size when creating shared memory - Different processes can open same memory - Language-agnostic configuration</p> <p>The Trade-off: - Must know limits at compile time - Can't grow beyond MAX_ENTRIES</p> <p>Our Answer: - Simulations have known structure counts - If you need 1000 structures, compile with 1024 - Explicit is better than implicit</p>"},{"location":"design_philosophy/#3-minimal-metadata-true-language-independence","title":"3. Minimal Metadata: True Language Independence","text":"<p>The Decision: Store only name, offset, and size - NO type information.</p> <p>Why This Matters:</p> <p>\u2705 Language Equality - Python and C++ are equal partners - No language is \"primary\" - Each reads the binary format directly</p> <p>\u2705 Preserves Performance Guarantees - <code>shm_array[i]</code> is EXACTLY a pointer dereference - No hidden indirection from std::vector - No capacity vs size confusion</p> <p>\u2705 Avoids Impedance Mismatch - STL expects individual deallocation - We don't support that (by design) - Would lead to confusion and bugs</p> <p>\u2705 Duck Typing in Python <pre><code># User specifies type at access time\ndata = Array(mem, \"sensor_data\", dtype=np.float32)\n</code></pre></p> <p>\u2705 Templates in C++ <pre><code>// Type specified at compile time\nArray&lt;float&gt; data(mem, \"sensor_data\", 1000);\n</code></pre></p> <p>The Trade-off: - Can't use arbitrary STL containers - Must use our provided structures</p> <p>Our Answer: - Our structures cover 95% of simulation needs - They're optimized for shared memory - Constraints enable optimization</p>"},{"location":"design_philosophy/#the-yagni-principle","title":"The YAGNI Principle","text":"<p>\"You Aren't Gonna Need It\" - We actively resist:</p>"},{"location":"design_philosophy/#features-we-consciously-rejected","title":"Features We Consciously Rejected","text":"<ol> <li>Arbitrary Container Support</li> <li>Why: STL containers aren't designed for shared memory</li> <li> <p>Instead: Purpose-built structures that excel at IPC</p> </li> <li> <p>Defragmentation</p> </li> <li>Why: Adds complexity, unpredictable pauses</li> <li> <p>Instead: Stack allocation with upfront sizing</p> </li> <li> <p>Serialization</p> </li> <li>Why: We use trivially-copyable types</li> <li> <p>Instead: Direct memory access, zero overhead</p> </li> <li> <p>Network Transparency</p> </li> <li>Why: Different problem domain entirely</li> <li> <p>Instead: Focus on single-node performance</p> </li> <li> <p>Persistence</p> </li> <li>Why: Shared memory is for IPC, not storage</li> <li>Instead: Let users handle persistence separately</li> </ol>"},{"location":"design_philosophy/#real-world-validation","title":"Real-World Validation","text":"<p>This design philosophy is validated by production systems:</p>"},{"location":"design_philosophy/#game-engines","title":"Game Engines","text":"<p>Unity's Burst Compiler/Job System: - Fixed-size, preallocated buffers - No dynamic allocation in hot paths - Stack allocators for frame data</p>"},{"location":"design_philosophy/#high-frequency-trading","title":"High-Frequency Trading","text":"<p>LMAX Disruptor: - Fixed-size ring buffers - Preallocated everything - Zero allocation during trading</p>"},{"location":"design_philosophy/#embedded-systems","title":"Embedded Systems","text":"<p>NASA's JPL Coding Standard: - No dynamic allocation after init - Fixed-size tables - Predictability over flexibility</p>"},{"location":"design_philosophy/#when-not-to-use-this-library","title":"When NOT to Use This Library","text":"<p>Be honest about limitations:</p> <p>Don't Use If You Need: - \u274c General-purpose shared memory allocation - \u274c Arbitrary container types in shared memory - \u274c Dynamic structure creation/destruction - \u274c Garbage collection or automatic memory management - \u274c Network-distributed shared memory</p> <p>Do Use If You Have: - \u2705 Known data structures at compile time - \u2705 Multiple processes on single node - \u2705 Performance-critical requirements - \u2705 Need for zero-overhead reads - \u2705 Simulation or real-time systems</p>"},{"location":"design_philosophy/#the-benefit-of-constraints","title":"The Benefit of Constraints","text":"<p>Our constraints aren't limitations - they're features:</p> Constraint Enables Stack allocation only Zero fragmentation, O(1) allocation Fixed-size tables Compile-time memory bounds No individual deallocation No use-after-free bugs Trivially-copyable only Zero-copy, no serialization Named structures Simple discovery, debugging"},{"location":"design_philosophy/#conclusion-simplicity-is-a-feature","title":"Conclusion: Simplicity Is A Feature","text":"<p>We could add: - Complex allocators with free lists - STL allocator interfaces - Defragmentation algorithms - Serialization layers - Network transparency</p> <p>But we choose not to. </p> <p>Every feature we DON'T add: - Keeps the codebase smaller - Makes performance more predictable - Reduces bug surface area - Lowers cognitive load - Maintains our zero-overhead guarantee</p> <p>Our success metric isn't feature count - it's nanoseconds.</p> <p>The library does exactly what it promises: - Fast: Proven identical to native arrays - Simple: ~2000 lines of focused code - Reliable: No allocator bugs to worry about - Predictable: No hidden behaviors</p> <p>And that's exactly what simulations need.</p>"},{"location":"lock_free_patterns/","title":"Lock-Free Programming Patterns and Insights","text":""},{"location":"lock_free_patterns/#executive-summary","title":"Executive Summary","text":"<p>Through extensive testing and debugging of Queue, Stack, and Array implementations across C, C++, and Python, we've identified critical patterns for correct lock-free programming in shared memory contexts.</p>"},{"location":"lock_free_patterns/#core-insights","title":"Core Insights","text":""},{"location":"lock_free_patterns/#1-the-fundamental-race-condition","title":"1. The Fundamental Race Condition","text":"<p>Problem: In our initial implementation, we updated the index (tail/head) BEFORE writing/reading data: <pre><code>// WRONG - Race condition!\nCAS(&amp;tail, current, next);  // Other threads now see new tail\ndata[current] = value;       // But data isn't written yet!\n</code></pre></p> <p>Solution: We need atomic reservation followed by data operation: <pre><code>// CORRECT - Reserve slot atomically\nCAS(&amp;tail, current, next);  // Reserve slot\ndata[current] = value;       // Write to reserved slot\nfence(release);              // Ensure visibility\n</code></pre></p>"},{"location":"lock_free_patterns/#2-memory-ordering-requirements","title":"2. Memory Ordering Requirements","text":"<p>For lock-free MPMC (Multiple Producer Multiple Consumer) queues/stacks:</p>"},{"location":"lock_free_patterns/#pushenqueue-pattern","title":"Push/Enqueue Pattern:","text":"<ol> <li>Reserve slot: Use relaxed or acquire-release CAS</li> <li>Write data: Standard memory write</li> <li>Release fence: Ensure data is visible before next operation</li> </ol>"},{"location":"lock_free_patterns/#popdequeue-pattern","title":"Pop/Dequeue Pattern:","text":"<ol> <li>Reserve slot: Use acquire-release CAS</li> <li>Acquire fence: Ensure we see complete data</li> <li>Read data: Standard memory read</li> </ol>"},{"location":"lock_free_patterns/#3-the-aba-problem","title":"3. The ABA Problem","text":"<p>Issue: A value changes from A\u2192B\u2192A between observations, causing CAS to succeed incorrectly.</p> <p>Solutions: - Bounded arrays: Our implementation uses indices into fixed arrays, avoiding pointer reuse - Tagged pointers: Could add version numbers (not needed for our design) - Hazard pointers: For dynamic allocation (not applicable here)</p>"},{"location":"lock_free_patterns/#4-circular-buffer-subtlety","title":"4. Circular Buffer Subtlety","text":"<p>Circular buffers require one empty slot to distinguish full from empty: - Empty: <code>head == tail</code> - Full: <code>(tail + 1) % capacity == head</code></p> <p>This means a capacity-N buffer holds N-1 items maximum.</p>"},{"location":"lock_free_patterns/#5-language-specific-considerations","title":"5. Language-Specific Considerations","text":""},{"location":"lock_free_patterns/#cc","title":"C/C++","text":"<ul> <li>True lock-free with atomic operations</li> <li>Memory fences critical for correctness</li> <li>Compiler optimizations can reorder without proper barriers</li> </ul>"},{"location":"lock_free_patterns/#python","title":"Python","text":"<ul> <li>GIL prevents true parallelism in threads</li> <li>Use <code>threading.Lock()</code> for correctness</li> <li>Multiprocessing achieves true parallelism but with IPC overhead</li> </ul>"},{"location":"lock_free_patterns/#validated-patterns","title":"Validated Patterns","text":""},{"location":"lock_free_patterns/#pattern-1-lock-free-ring-buffer-queue","title":"Pattern 1: Lock-Free Ring Buffer Queue","text":"<pre><code>typedef struct {\n    _Atomic uint32_t head;\n    _Atomic uint32_t tail;\n    uint32_t capacity;\n    // data follows\n} queue_header_t;\n\n// Push (MPMC)\nint push(queue_t* q, value_t value) {\n    uint32_t current_tail, next_tail;\n    do {\n        current_tail = atomic_load(&amp;q-&gt;tail);\n        next_tail = (current_tail + 1) % q-&gt;capacity;\n        if (next_tail == atomic_load(&amp;q-&gt;head))\n            return FULL;\n    } while (!CAS(&amp;q-&gt;tail, current_tail, next_tail));\n\n    q-&gt;data[current_tail] = value;\n    atomic_thread_fence(memory_order_release);\n    return OK;\n}\n</code></pre>"},{"location":"lock_free_patterns/#pattern-2-lock-free-stack","title":"Pattern 2: Lock-Free Stack","text":"<pre><code>typedef struct {\n    _Atomic int32_t top;  // -1 when empty\n    uint32_t capacity;\n    // data follows\n} stack_header_t;\n\n// Push\nint push(stack_t* s, value_t value) {\n    int32_t current_top, new_top;\n    do {\n        current_top = atomic_load(&amp;s-&gt;top);\n        if (current_top &gt;= capacity - 1)\n            return FULL;\n        new_top = current_top + 1;\n    } while (!CAS(&amp;s-&gt;top, current_top, new_top));\n\n    s-&gt;data[new_top] = value;\n    atomic_thread_fence(memory_order_release);\n    return OK;\n}\n</code></pre>"},{"location":"lock_free_patterns/#pattern-3-shared-memory-layout","title":"Pattern 3: Shared Memory Layout","text":"<pre><code>[Table Header][Table Entries][Structure 1][Structure 2]...[Structure N]\n     Fixed         Fixed         Dynamic      Dynamic         Dynamic\n</code></pre>"},{"location":"lock_free_patterns/#testing-insights","title":"Testing Insights","text":""},{"location":"lock_free_patterns/#stress-test-requirements","title":"Stress Test Requirements","text":"<ol> <li>Concurrent operations: 16+ threads minimum</li> <li>High contention: Small queue (10 slots) with many threads</li> <li>Checksum validation: Sum all produced/consumed values</li> <li>Long duration: 10,000+ operations per thread</li> <li>Memory barriers: Test with TSan or Helgrind</li> </ol>"},{"location":"lock_free_patterns/#critical-test-cases","title":"Critical Test Cases","text":"<ul> <li>Basic correctness (empty, single item, full)</li> <li>FIFO/LIFO ordering</li> <li>Producer-consumer balance</li> <li>High contention on small structures</li> <li>Rapid create/destroy cycles</li> <li>Cross-process access</li> <li>Memory boundary conditions</li> </ul>"},{"location":"lock_free_patterns/#recommendations-for-future-data-structures","title":"Recommendations for Future Data Structures","text":""},{"location":"lock_free_patterns/#1-hashmaphashset","title":"1. HashMap/HashSet","text":"<ul> <li>Use open addressing with linear probing</li> <li>CAS for claiming slots</li> <li>Tombstones for deletion</li> <li>Consider Robin Hood hashing for better distribution</li> </ul>"},{"location":"lock_free_patterns/#2-priority-queue-heap","title":"2. Priority Queue (Heap)","text":"<ul> <li>Challenge: Maintaining heap property atomically</li> <li>Consider skip list as alternative</li> <li>Or use sharded heaps with per-shard locks</li> </ul>"},{"location":"lock_free_patterns/#3-b-tree","title":"3. B-Tree","text":"<ul> <li>Node-level locking more practical than lock-free</li> <li>Consider B-link trees for concurrent access</li> <li>Read-copy-update (RCU) for read-heavy workloads</li> </ul>"},{"location":"lock_free_patterns/#4-graph-structures","title":"4. Graph Structures","text":"<ul> <li>Edge list representation most amenable to lock-free</li> <li>Adjacency lists challenging without garbage collection</li> <li>Consider epoch-based reclamation</li> </ul>"},{"location":"lock_free_patterns/#performance-characteristics","title":"Performance Characteristics","text":"<p>From our benchmarks:</p> Implementation Single-Thread Multi-Thread (8) Notes C ~50M ops/sec ~20M ops/sec True lock-free C++ ~100M ops/sec ~12M ops/sec Template optimizations Python ~1M ops/sec ~0.5M ops/sec GIL-limited"},{"location":"lock_free_patterns/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ol> <li>Writing data after index update: Creates visibility race</li> <li>Missing memory fences: Compiler/CPU reordering</li> <li>Incorrect empty/full checks: Off-by-one errors</li> <li>ABA vulnerability: When reusing memory</li> <li>Assuming atomicity: Multi-word updates aren't atomic</li> <li>Overflow handling: Index wraparound must be correct</li> </ol>"},{"location":"lock_free_patterns/#conclusion","title":"Conclusion","text":"<p>Lock-free programming requires careful attention to: - Atomic operations and their memory ordering - Data visibility across threads - Race condition prevention - Thorough testing under high concurrency</p> <p>The patterns validated here provide a solid foundation for implementing additional lock-free data structures in shared memory contexts.</p>"},{"location":"mainpage/","title":"POSIX Shared Memory Data Structures","text":""},{"location":"mainpage/#zero-overhead-inter-process-communication-for-high-performance-computing","title":"Zero-Overhead Inter-Process Communication for High-Performance Computing","text":""},{"location":"mainpage/#overview","title":"Overview","text":"<p>This library provides production-ready, lock-free data structures built on POSIX shared memory for high-performance inter-process communication (IPC). Designed for simulations, real-time systems, and high-throughput applications where nanosecond-level performance matters.</p>"},{"location":"mainpage/#key-features","title":"Key Features","text":"<ul> <li>\ud83d\ude80 Zero read overhead - Proven identical performance to native arrays</li> <li>\ud83d\udd13 Lock-free operations - Where algorithmically possible  </li> <li>\ud83d\udce6 Auto-discovery - Named data structures findable across processes</li> <li>\ud83c\udfaf Cache-efficient - Optimized memory layouts for modern CPUs</li> <li>\ud83d\udd27 Modern C++23 - Concepts, ranges, string_view, [[nodiscard]]</li> <li>\ud83d\udccf Configurable overhead - Template-based table sizes from 904B to 26KB</li> <li>\ud83e\uddea Battle-tested - Comprehensive test suite with Catch2</li> </ul>"},{"location":"mainpage/#why-shared-memory","title":"Why Shared Memory?","text":"<p>Traditional IPC methods (sockets, pipes, message queues) require: - Kernel transitions (~1000ns overhead) - Data copying (2x memory bandwidth) - Serialization (CPU cycles + allocations)</p> <p>Shared memory provides: - Direct memory access (~0.5ns for L1 hit) - Zero-copy data sharing - No serialization for POD types - Cache coherence handled by hardware</p>"},{"location":"mainpage/#performance-guarantees","title":"Performance Guarantees","text":"Operation Time Complexity Actual Performance Array Read O(1) 0.5-2ns (cache hit) Array Write O(1) 2-5ns (atomic CAS) Queue Enqueue O(1) 5-10ns (lock-free) Queue Dequeue O(1) 5-10ns (lock-free) Pool Acquire O(1) 10-20ns (lock-free) Atomic Update O(1) 2-5ns (hardware) Discovery O(n) ~100ns (one-time)"},{"location":"mainpage/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                Shared Memory Segment            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n\u2502  \u2502 RefCount \u2502  Atomic reference counting        \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                                   \u2502\n\u2502  \u2502 Table    \u2502  Metadata for discovery           \u2502\n\u2502  \u2502  \u251c\u2500Entry1\u2502  \"sensor_data\" \u2192 offset, size     \u2502\n\u2502  \u2502  \u251c\u2500Entry2\u2502  \"event_queue\" \u2192 offset, size     \u2502\n\u2502  \u2502  \u2514\u2500...   \u2502                                   \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                                   \u2502\n\u2502  \u2502 Array&lt;T&gt; \u2502  Contiguous data                  \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                                   \u2502\n\u2502  \u2502 Queue&lt;T&gt; \u2502  Circular buffer + atomics        \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                                   \u2502\n\u2502  \u2502 Pool&lt;T&gt;  \u2502  Free list + object storage       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"mainpage/#core-components","title":"Core Components","text":""},{"location":"mainpage/#1-foundation-layer","title":"1. Foundation Layer","text":"<ul> <li>posix_shm - POSIX shared memory lifecycle management</li> <li>shm_table - Metadata and discovery system</li> <li>shm_span - Base class for memory regions</li> </ul>"},{"location":"mainpage/#2-data-structures","title":"2. Data Structures","text":"<ul> <li>shm_array - Fixed-size contiguous array <li>shm_queue - Lock-free FIFO queue <li>shm_atomic - Named atomic variables <li>shm_object_pool - O(1) object allocation <li>shm_ring_buffer - Bulk operations for streaming"},{"location":"mainpage/#3-template-configurations","title":"3. Template Configurations","text":"<pre><code>// Minimal overhead (904 bytes) - embedded systems\nusing my_shm = posix_shm_impl&lt;shm_table_impl&lt;16, 16&gt;&gt;;\n\n// Default (4KB) - balanced\nusing my_shm = posix_shm;  \n\n// Large (26KB) - complex simulations  \nusing my_shm = posix_shm_impl&lt;shm_table_impl&lt;64, 256&gt;&gt;;\n</code></pre>"},{"location":"mainpage/#quick-start","title":"Quick Start","text":"<pre><code>#include \"posix_shm.h\"\n#include \"shm_array.h\"\n#include \"shm_queue.h\"\n\n// Process 1: Create and populate\nposix_shm shm(\"simulation\", 10*1024*1024);  // 10MB\nshm_array&lt;double&gt; sensors(shm, \"sensors\", 1000);\nsensors[0] = 3.14159;\n\nshm_queue&lt;Event&gt; events(shm, \"events\", 100);\nevents.enqueue({timestamp, data});\n\n// Process 2: Discover and use\nposix_shm shm(\"simulation\");  // Open existing\nshm_array&lt;double&gt; sensors(shm, \"sensors\");  // Find by name\nauto value = sensors[0];  // Direct memory read!\n\nshm_queue&lt;Event&gt; events(shm, \"events\");\nif (auto e = events.dequeue()) {\n    process(*e);\n}\n</code></pre>"},{"location":"mainpage/#use-cases","title":"Use Cases","text":""},{"location":"mainpage/#high-frequency-trading","title":"High-Frequency Trading","text":"<ul> <li>Market data distribution</li> <li>Order book sharing</li> <li>Strategy coordination</li> </ul>"},{"location":"mainpage/#scientific-simulation","title":"Scientific Simulation","text":"<ul> <li>Particle systems (10,000+ entities)</li> <li>Sensor data aggregation (MHz rates)</li> <li>Grid-based computations (CFD, weather)</li> </ul>"},{"location":"mainpage/#robotics-autonomous-systems","title":"Robotics &amp; Autonomous Systems","text":"<ul> <li>Sensor fusion pipelines</li> <li>Control loop communication</li> <li>Perception data sharing</li> </ul>"},{"location":"mainpage/#game-servers","title":"Game Servers","text":"<ul> <li>Entity state replication</li> <li>Physics synchronization</li> <li>Event broadcasting</li> </ul>"},{"location":"mainpage/#proven-performance","title":"Proven Performance","text":"<p>Our benchmarks demonstrate zero overhead for reads:</p> <pre><code>Sequential Read Performance:\nHeap array:              2.32 ns/op\nShared array:            2.32 ns/op  \u2190 Identical!\nShared raw pointer:      2.31 ns/op  \u2190 Direct access\n\nRandom Access Performance:\nHeap array:              2.33 ns/op  \nShared array:            2.33 ns/op  \u2190 Same cache behavior\n</code></pre>"},{"location":"mainpage/#safety-correctness","title":"Safety &amp; Correctness","text":"<ul> <li>Type safety via C++23 concepts</li> <li>Bounds checking in debug builds</li> <li>RAII memory management</li> <li>Atomic operations for thread safety</li> <li>Process crash resilience </li> </ul>"},{"location":"mainpage/#getting-started","title":"Getting Started","text":"<ol> <li>Tutorial - Step-by-step guide</li> <li>Performance Guide - Optimization tips</li> <li>Architecture - Design deep dive</li> <li>API Reference - Complete documentation</li> </ol>"},{"location":"mainpage/#requirements","title":"Requirements","text":"<ul> <li>C++23 compiler (GCC 13+, Clang 16+)</li> <li>POSIX-compliant OS (Linux, macOS, BSD)</li> <li>CMake 3.20+</li> </ul>"},{"location":"mainpage/#license","title":"License","text":"<p>MIT License - Use freely in commercial projects</p>"},{"location":"mainpage/#contributing","title":"Contributing","text":"<p>Contributions welcome! Areas of interest: - Additional data structures (B-tree, hash map) - Performance optimizations (huge pages, NUMA) - Language bindings (Python, Rust) - Platform ports (Windows shared memory)</p>"},{"location":"optimizations/","title":"Performance Optimizations Guide","text":""},{"location":"optimizations/#current-performance-characteristics","title":"Current Performance Characteristics","text":"<p>Our benchmarks show zero overhead for shared memory reads: - ~2.3ns per array access (identical to heap arrays) - Lock-free operations in 10-15ns range - No cache line false sharing with proper alignment</p>"},{"location":"optimizations/#optimization-opportunities","title":"Optimization Opportunities","text":""},{"location":"optimizations/#1-memory-prefetching-for-sequential-access","title":"1. Memory Prefetching for Sequential Access","text":"<p>For simulation workloads with predictable access patterns:</p> <pre><code>template&lt;typename T&gt;\nclass shm_array_prefetch : public shm_array&lt;T&gt; {\n    void prefetch_next(size_t current_idx, size_t distance = 8) {\n        if (current_idx + distance &lt; this-&gt;size()) {\n            __builtin_prefetch(&amp;(*this)[current_idx + distance], 0, 1);\n        }\n    }\n};\n</code></pre> <p>Benefit: 20-40% improvement for streaming access patterns</p>"},{"location":"optimizations/#2-simd-friendly-alignment","title":"2. SIMD-Friendly Alignment","text":"<p>Ensure arrays are aligned for AVX-512:</p> <pre><code>template&lt;typename T&gt;\nclass shm_array_simd : public shm_array&lt;T&gt; {\n    static constexpr size_t SIMD_ALIGN = 64;  // AVX-512 alignment\n\n    void* allocate_aligned(size_t size) {\n        size_t offset = shm.get_current_offset();\n        size_t aligned = (offset + SIMD_ALIGN - 1) &amp; ~(SIMD_ALIGN - 1);\n        return shm.allocate_at(aligned, size);\n    }\n};\n</code></pre> <p>Benefit: Enables vectorized operations, 4-8x speedup for bulk operations</p>"},{"location":"optimizations/#3-cache-conscious-data-structures","title":"3. Cache-Conscious Data Structures","text":""},{"location":"optimizations/#packed-arrays-for-structs-of-arrays-soa","title":"Packed Arrays for Structs of Arrays (SoA)","text":"<p>Transform Array of Structs (AoS) to Struct of Arrays (SoA):</p> <pre><code>// Instead of:\nstruct Particle {\n    float x, y, z;\n    float vx, vy, vz;\n};\nshm_array&lt;Particle&gt; particles;\n\n// Use:\nstruct ParticlesSoA {\n    shm_array&lt;float&gt; x, y, z;\n    shm_array&lt;float&gt; vx, vy, vz;\n};\n</code></pre> <p>Benefit: Better cache utilization, enables SIMD on individual components</p>"},{"location":"optimizations/#4-huge-pages-support","title":"4. Huge Pages Support","text":"<p>Enable 2MB/1GB pages for large simulations:</p> <pre><code>void* mmap_huge(size_t size) {\n    return mmap(NULL, size, \n                PROT_READ | PROT_WRITE,\n                MAP_SHARED | MAP_HUGETLB | MAP_HUGE_2MB,\n                fd, 0);\n}\n</code></pre> <p>Benefit: Reduces TLB misses by 512x, 5-15% overall improvement</p>"},{"location":"optimizations/#5-numa-aware-allocation","title":"5. NUMA-Aware Allocation","text":"<p>For multi-socket systems:</p> <pre><code>class numa_shm : public posix_shm {\n    void bind_to_node(int node) {\n        numa_tonode_memory(base_addr, size, node);\n    }\n\n    void interleave_nodes() {\n        numa_set_interleave_mask(numa_all_nodes_ptr);\n    }\n};\n</code></pre> <p>Benefit: 2-3x improvement for NUMA-sensitive workloads</p>"},{"location":"optimizations/#6-lock-free-improvements","title":"6. Lock-Free Improvements","text":""},{"location":"optimizations/#relaxed-memory-ordering-where-safe","title":"Relaxed Memory Ordering Where Safe","text":"<pre><code>// For statistics/counters that don't need strict ordering:\ncounter.fetch_add(1, std::memory_order_relaxed);  // Faster than seq_cst\n</code></pre>"},{"location":"optimizations/#padding-to-prevent-false-sharing","title":"Padding to Prevent False Sharing","text":"<pre><code>struct alignas(64) CacheLineCounter {\n    std::atomic&lt;uint64_t&gt; value;\n    char padding[56];  // Ensure exclusive cache line\n};\n</code></pre> <p>Benefit: 10-50x improvement under contention</p>"},{"location":"optimizations/#7-batch-operations","title":"7. Batch Operations","text":"<p>Amortize overhead with bulk operations:</p> <pre><code>template&lt;typename T&gt;\nclass shm_array_batch : public shm_array&lt;T&gt; {\n    void write_batch(size_t start, std::span&lt;const T&gt; values) {\n        // Single bounds check\n        if (start + values.size() &gt; this-&gt;size()) \n            throw std::out_of_range(\"Batch write out of bounds\");\n\n        // Optimized memcpy for trivially copyable types\n        std::memcpy(&amp;(*this)[start], values.data(), \n                    values.size() * sizeof(T));\n    }\n};\n</code></pre> <p>Benefit: 5-10x faster than individual writes</p>"},{"location":"optimizations/#8-compile-time-optimizations","title":"8. Compile-Time Optimizations","text":""},{"location":"optimizations/#link-time-optimization-lto","title":"Link-Time Optimization (LTO)","text":"<pre><code>set(CMAKE_INTERPROCEDURAL_OPTIMIZATION TRUE)\ntarget_compile_options(posix_shm INTERFACE -flto)\n</code></pre>"},{"location":"optimizations/#profile-guided-optimization-pgo","title":"Profile-Guided Optimization (PGO)","text":"<pre><code># Generate profile\ng++ -fprofile-generate ...\n./simulation_benchmark\n# Use profile\ng++ -fprofile-use ...\n</code></pre> <p>Benefit: 10-20% improvement from better inlining and branch prediction</p>"},{"location":"optimizations/#9-memory-pool-with-size-classes","title":"9. Memory Pool with Size Classes","text":"<p>For dynamic allocation patterns:</p> <pre><code>template&lt;size_t... Sizes&gt;\nclass shm_size_class_pool {\n    std::tuple&lt;shm_object_pool&lt;Sizes&gt;...&gt; pools;\n\n    template&lt;size_t Size&gt;\n    auto&amp; get_pool() {\n        return std::get&lt;shm_object_pool&lt;Size&gt;&gt;(pools);\n    }\n};\n</code></pre> <p>Benefit: O(1) allocation without fragmentation</p>"},{"location":"optimizations/#10-zero-copy-string-views","title":"10. Zero-Copy String Views","text":"<p>For string data in shared memory:</p> <pre><code>class shm_string_view {\n    size_t offset;\n    size_t length;\n\n    std::string_view get(const posix_shm&amp; shm) const {\n        return std::string_view(\n            static_cast&lt;const char*&gt;(shm.get_base_addr()) + offset,\n            length\n        );\n    }\n};\n</code></pre> <p>Benefit: No string allocation overhead</p>"},{"location":"optimizations/#simulation-specific-optimizations","title":"Simulation-Specific Optimizations","text":""},{"location":"optimizations/#particle-simulations","title":"Particle Simulations","text":"<pre><code>// Use SoA layout\n// Enable SIMD operations\n// Prefetch next particles during force calculation\n// Use spatial hashing for neighbor finding\n</code></pre>"},{"location":"optimizations/#time-series-data","title":"Time-Series Data","text":"<pre><code>// Ring buffer with power-of-2 size (faster modulo)\n// Cache-aligned write position\n// Bulk read/write operations\n// Consider compression for historical data\n</code></pre>"},{"location":"optimizations/#graph-simulations","title":"Graph Simulations","text":"<pre><code>// CSR format for sparse adjacency\n// Cache-blocking for matrix operations\n// Parallel edge iteration with atomic updates\n// Consider vertex reordering for locality\n</code></pre>"},{"location":"optimizations/#benchmarking-recommendations","title":"Benchmarking Recommendations","text":"<ol> <li> <p>Use <code>perf</code> for profiling:    <pre><code>perf record -e cache-misses,cache-references ./simulation\nperf report\n</code></pre></p> </li> <li> <p>Monitor TLB misses:    <pre><code>perf stat -e dTLB-load-misses ./simulation\n</code></pre></p> </li> <li> <p>Check false sharing:    <pre><code>perf c2c record ./simulation\nperf c2c report\n</code></pre></p> </li> <li> <p>Measure memory bandwidth:    <pre><code>mbw 1000  # Memory bandwidth benchmark\n</code></pre></p> </li> </ol>"},{"location":"optimizations/#implementation-priority","title":"Implementation Priority","text":"<p>Based on typical simulation workloads:</p> <ol> <li>High Priority: Cache-line alignment, SIMD alignment, batch operations</li> <li>Medium Priority: Huge pages, prefetching, SoA layout</li> <li>Low Priority: NUMA optimization (unless multi-socket), PGO</li> </ol>"},{"location":"optimizations/#trade-offs-to-consider","title":"Trade-offs to Consider","text":"<ul> <li>Complexity vs Performance: Simple code is often fast enough</li> <li>Memory vs Speed: Padding wastes memory but improves speed</li> <li>Latency vs Throughput: Batching improves throughput but adds latency</li> <li>Portability vs Optimization: Platform-specific optimizations reduce portability</li> </ul> <p>Remember: Measure first, optimize second. Profile your specific workload to identify actual bottlenecks before applying these optimizations.</p>"},{"location":"patterns/","title":"Cross-Process Communication Patterns with ZeroIPC","text":""},{"location":"patterns/#introduction","title":"Introduction","text":"<p>This document presents common patterns and architectural designs for building robust multi-process systems using ZeroIPC. Each pattern is illustrated with concrete examples and best practices.</p>"},{"location":"patterns/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Patterns</li> <li>Async Patterns</li> <li>Streaming Patterns</li> <li>Concurrency Patterns</li> <li>Architectural Patterns</li> <li>Reliability Patterns</li> </ol>"},{"location":"patterns/#basic-patterns","title":"Basic Patterns","text":""},{"location":"patterns/#producer-consumer","title":"Producer-Consumer","text":"<p>The fundamental pattern where one process produces data and another consumes it.</p> <p>Using Queue: <pre><code>// Producer Process\nmemory mem(\"/app\", 10*1024*1024);\nqueue&lt;Task&gt; task_queue(mem, \"tasks\", 1000);\n\nwhile (running) {\n    Task t = generate_task();\n    if (!task_queue.push(t)) {\n        // Handle backpressure\n        std::this_thread::sleep_for(10ms);\n    }\n}\n\n// Consumer Process\nmemory mem(\"/app\");\nqueue&lt;Task&gt; task_queue(mem, \"tasks\");\n\nwhile (running) {\n    if (auto task = task_queue.pop()) {\n        process_task(*task);\n    } else {\n        // Queue empty, wait\n        std::this_thread::sleep_for(1ms);\n    }\n}\n</code></pre></p> <p>Best Practices: - Handle backpressure gracefully - Use exponential backoff for retries - Monitor queue depth for capacity planning</p>"},{"location":"patterns/#request-response","title":"Request-Response","text":"<p>Synchronous communication pattern using Futures.</p> <pre><code>// Service Process\nmemory mem(\"/service\", 50*1024*1024);\nqueue&lt;Request&gt; requests(mem, \"requests\", 100);\n\nwhile (running) {\n    if (auto req = requests.pop()) {\n        // Create future for response\n        future&lt;Response&gt; response(mem, \"response_\" + std::to_string(req-&gt;id));\n\n        // Process request\n        Response res = handle_request(*req);\n        response.set_value(res);\n    }\n}\n\n// Client Process\nmemory mem(\"/service\");\nqueue&lt;Request&gt; requests(mem, \"requests\");\n\nRequest req{.id = 42, .data = \"query\"};\nrequests.push(req);\n\n// Wait for response\nfuture&lt;Response&gt; response(mem, \"response_42\", true);\nif (auto res = response.get_for(5s)) {\n    process_response(*res);\n} else {\n    handle_timeout();\n}\n</code></pre>"},{"location":"patterns/#pub-sub-publish-subscribe","title":"Pub-Sub (Publish-Subscribe)","text":"<p>Multiple consumers receiving the same data stream.</p> <pre><code>// Publisher\nmemory mem(\"/events\", 20*1024*1024);\nstream&lt;Event&gt; events(mem, \"event_stream\", 10000);\n\nvoid publish_event(const Event&amp; e) {\n    events.emit(e);\n}\n\n// Subscriber A - Logging\nmemory mem(\"/events\");\nstream&lt;Event&gt; events(mem, \"event_stream\");\n\nevents.subscribe([](const Event&amp; e) {\n    log_to_file(e);\n});\n\n// Subscriber B - Analytics\nevents.subscribe([](const Event&amp; e) {\n    update_metrics(e);\n});\n\n// Subscriber C - Alerting\nauto critical = events.filter(mem, \"critical\", \n    [](const Event&amp; e) { return e.severity &gt;= CRITICAL; });\n\ncritical.subscribe([](const Event&amp; e) {\n    send_alert(e);\n});\n</code></pre>"},{"location":"patterns/#async-patterns","title":"Async Patterns","text":""},{"location":"patterns/#future-chaining","title":"Future Chaining","text":"<p>Compose asynchronous operations without blocking.</p> <pre><code>// Process A: Initiate computation chain\nmemory mem(\"/pipeline\", 100*1024*1024);\n\nfuture&lt;RawData&gt; raw_future(mem, \"raw_data\");\nfuture&lt;ProcessedData&gt; processed_future(mem, \"processed_data\");\nfuture&lt;Result&gt; final_future(mem, \"final_result\");\n\n// Start pipeline\nRawData raw = fetch_data();\nraw_future.set_value(raw);\n\n// Process B: First transformation\nfuture&lt;RawData&gt; raw_future(mem, \"raw_data\", true);\nfuture&lt;ProcessedData&gt; processed_future(mem, \"processed_data\");\n\nRawData raw = raw_future.get();\nProcessedData processed = transform(raw);\nprocessed_future.set_value(processed);\n\n// Process C: Final computation\nfuture&lt;ProcessedData&gt; processed_future(mem, \"processed_data\", true);\nfuture&lt;Result&gt; final_future(mem, \"final_result\");\n\nProcessedData processed = processed_future.get();\nResult result = compute_final(processed);\nfinal_future.set_value(result);\n</code></pre>"},{"location":"patterns/#fork-join-parallelism","title":"Fork-Join Parallelism","text":"<p>Split work across processes and join results.</p> <pre><code>// Coordinator Process\nmemory mem(\"/parallel\", 200*1024*1024);\n\n// Create work items\nconst int N = 10;\nfor (int i = 0; i &lt; N; i++) {\n    array&lt;double&gt; input(mem, \"input_\" + std::to_string(i), 1000);\n    fill_with_data(input);\n\n    future&lt;double&gt; result(mem, \"result_\" + std::to_string(i));\n}\n\n// Wait for all results\nstd::vector&lt;double&gt; results;\nfor (int i = 0; i &lt; N; i++) {\n    future&lt;double&gt; result(mem, \"result_\" + std::to_string(i), true);\n    results.push_back(result.get());\n}\n\ndouble final = aggregate(results);\n\n// Worker Process (run N instances)\nmemory mem(\"/parallel\");\nint worker_id = get_worker_id();\n\narray&lt;double&gt; input(mem, \"input_\" + std::to_string(worker_id));\nfuture&lt;double&gt; result(mem, \"result_\" + std::to_string(worker_id), true);\n\ndouble res = heavy_computation(input);\nresult.set_value(res);\n</code></pre>"},{"location":"patterns/#lazy-initialization","title":"Lazy Initialization","text":"<p>Defer expensive initialization until needed.</p> <pre><code>// Configuration Service\nmemory mem(\"/config\", 10*1024*1024);\nlazy&lt;Config&gt; config(mem, \"app_config\");\n\nconfig.set_computation([]() {\n    // Expensive: read files, parse, validate\n    Config c;\n    c.parse_from_file(\"/etc/app.conf\");\n    c.validate();\n    c.resolve_dependencies();\n    return c;\n});\n\n// Application Processes\nmemory mem(\"/config\");\nlazy&lt;Config&gt; config(mem, \"app_config\", true);\n\n// First access triggers computation\nConfig c = config.get();  // Expensive first time\n\n// Subsequent accesses are instant\nConfig c2 = config.get();  // Returns cached\n</code></pre>"},{"location":"patterns/#streaming-patterns","title":"Streaming Patterns","text":""},{"location":"patterns/#pipeline-processing","title":"Pipeline Processing","text":"<p>Chain stream transformations for complex processing.</p> <pre><code>// Data Processing Pipeline\nmemory mem(\"/pipeline\", 50*1024*1024);\n\n// Raw sensor data\nstream&lt;SensorReading&gt; raw(mem, \"raw_sensors\", 10000);\n\n// Stage 1: Validation and cleaning\nauto validated = raw\n    .filter(mem, \"validated\", [](const SensorReading&amp; r) {\n        return r.timestamp &gt; 0 &amp;&amp; r.value &gt;= 0;\n    })\n    .map(mem, \"cleaned\", [](const SensorReading&amp; r) {\n        return SensorReading{\n            .timestamp = r.timestamp,\n            .value = apply_calibration(r.value)\n        };\n    });\n\n// Stage 2: Windowing and aggregation\nauto windows = validated\n    .window(mem, \"5sec_windows\", 50)  // 50 readings per window\n    .map(mem, \"aggregated\", [](const auto&amp; window) {\n        double sum = 0, min = DBL_MAX, max = DBL_MIN;\n        for (const auto&amp; r : window) {\n            sum += r.value;\n            min = std::min(min, r.value);\n            max = std::max(max, r.value);\n        }\n        return Aggregate{\n            .avg = sum / window.size(),\n            .min = min,\n            .max = max,\n            .count = window.size()\n        };\n    });\n\n// Stage 3: Alerting\nwindows.subscribe([](const Aggregate&amp; agg) {\n    if (agg.avg &gt; THRESHOLD) {\n        trigger_alert(agg);\n    }\n    store_metrics(agg);\n});\n</code></pre>"},{"location":"patterns/#merge-and-join-streams","title":"Merge and Join Streams","text":"<p>Combine multiple data streams.</p> <pre><code>// Merge Pattern - Combine streams of same type\nstream&lt;Event&gt; ui_events(mem, \"ui_events\", 1000);\nstream&lt;Event&gt; system_events(mem, \"system_events\", 1000);\nstream&lt;Event&gt; network_events(mem, \"network_events\", 1000);\n\nauto all_events = ui_events\n    .merge(mem, \"merged_1\", system_events)\n    .merge(mem, \"all_events\", network_events);\n\n// Zip Pattern - Combine streams element-wise\nstream&lt;Temperature&gt; temps(mem, \"temperatures\", 1000);\nstream&lt;Pressure&gt; pressures(mem, \"pressures\", 1000);\n\nauto combined = temps.zip(mem, \"temp_pressure\", pressures);\ncombined.subscribe([](const auto&amp; pair) {\n    auto [temp, pressure] = pair;\n    calculate_altitude(temp, pressure);\n});\n</code></pre>"},{"location":"patterns/#backpressure-management","title":"Backpressure Management","text":"<p>Handle fast producers and slow consumers.</p> <pre><code>class BackpressureStream {\n    stream&lt;Data&gt; primary;\n    queue&lt;Data&gt; overflow;\n    std::atomic&lt;bool&gt; use_overflow{false};\n\npublic:\n    void emit(const Data&amp; d) {\n        if (!primary.emit(d)) {\n            // Primary stream full, use overflow queue\n            if (!overflow.push(d)) {\n                // Both full - apply backpressure strategy\n                use_overflow.store(true);\n                drop_oldest();  // or block, or sample\n            }\n        }\n    }\n\n    void process() {\n        // Drain overflow when primary has space\n        if (use_overflow.load() &amp;&amp; primary.available_space() &gt; 0) {\n            while (auto d = overflow.pop()) {\n                if (!primary.emit(*d)) break;\n            }\n            use_overflow.store(overflow.size() &gt; 0);\n        }\n    }\n};\n</code></pre>"},{"location":"patterns/#concurrency-patterns","title":"Concurrency Patterns","text":""},{"location":"patterns/#worker-pool","title":"Worker Pool","text":"<p>Distribute work across a pool of worker processes.</p> <pre><code>// Manager Process\nmemory mem(\"/workers\", 100*1024*1024);\nchannel&lt;Task&gt; task_channel(mem, \"tasks\", 1000);\nchannel&lt;Result&gt; result_channel(mem, \"results\", 1000);\n\n// Submit tasks\nfor (const auto&amp; task : tasks) {\n    task_channel.send(task);\n}\ntask_channel.close();\n\n// Collect results\nstd::vector&lt;Result&gt; results;\nwhile (auto result = result_channel.receive()) {\n    results.push_back(*result);\n}\n\n// Worker Process Template\nvoid worker_main(int worker_id) {\n    memory mem(\"/workers\");\n    channel&lt;Task&gt; task_channel(mem, \"tasks\");\n    channel&lt;Result&gt; result_channel(mem, \"results\");\n\n    while (auto task = task_channel.receive()) {\n        Result r = process(*task);\n        r.worker_id = worker_id;\n        result_channel.send(r);\n    }\n}\n\n// Launch workers\nfor (int i = 0; i &lt; NUM_WORKERS; i++) {\n    std::thread(worker_main, i).detach();\n}\n</code></pre>"},{"location":"patterns/#actor-model","title":"Actor Model","text":"<p>Implement actors with channels as mailboxes.</p> <pre><code>class Actor {\n    memory&amp; mem;\n    std::string name;\n    channel&lt;Message&gt; mailbox;\n    std::thread worker;\n    std::atomic&lt;bool&gt; running{true};\n\npublic:\n    Actor(memory&amp; m, std::string_view n) \n        : mem(m), name(n), \n          mailbox(m, std::string(n) + \"_mailbox\", 100) {\n        worker = std::thread([this]() { run(); });\n    }\n\n    void send(const Message&amp; msg) {\n        mailbox.send(msg);\n    }\n\nprivate:\n    void run() {\n        while (running.load()) {\n            if (auto msg = mailbox.receive_for(100ms)) {\n                handle_message(*msg);\n            }\n        }\n    }\n\n    void handle_message(const Message&amp; msg) {\n        switch (msg.type) {\n            case COMPUTE:\n                do_computation(msg.data);\n                break;\n            case FORWARD:\n                forward_to_actor(msg.target, msg.data);\n                break;\n            case STOP:\n                running.store(false);\n                break;\n        }\n    }\n};\n\n// Usage\nmemory mem(\"/actors\", 50*1024*1024);\nActor compute_actor(mem, \"compute\");\nActor storage_actor(mem, \"storage\");\nActor network_actor(mem, \"network\");\n\ncompute_actor.send({COMPUTE, data});\n</code></pre>"},{"location":"patterns/#csp-select","title":"CSP Select","text":"<p>Wait on multiple channels simultaneously.</p> <pre><code>template&lt;typename T1, typename T2&gt;\nvoid select_loop(channel&lt;T1&gt;&amp; ch1, channel&lt;T2&gt;&amp; ch2) {\n    while (true) {\n        // Try channels in order with timeout\n        if (auto v1 = ch1.receive_for(0ms)) {\n            handle_type1(*v1);\n            continue;\n        }\n\n        if (auto v2 = ch2.receive_for(0ms)) {\n            handle_type2(*v2);\n            continue;\n        }\n\n        // No data available, wait a bit\n        std::this_thread::sleep_for(1ms);\n    }\n}\n\n// Alternative: Priority select\nclass PrioritySelect {\n    struct ChannelPriority {\n        int priority;\n        std::function&lt;bool()&gt; try_receive;\n    };\n\n    std::vector&lt;ChannelPriority&gt; channels;\n\npublic:\n    void add_channel(int priority, auto&amp; channel, auto handler) {\n        channels.push_back({priority, [&amp;]() {\n            if (auto v = channel.try_receive()) {\n                handler(*v);\n                return true;\n            }\n            return false;\n        }});\n\n        // Sort by priority\n        std::sort(channels.begin(), channels.end(),\n            [](const auto&amp; a, const auto&amp; b) { \n                return a.priority &gt; b.priority; \n            });\n    }\n\n    void select() {\n        for (auto&amp; ch : channels) {\n            if (ch.try_receive()) return;\n        }\n    }\n};\n</code></pre>"},{"location":"patterns/#architectural-patterns","title":"Architectural Patterns","text":""},{"location":"patterns/#microservices-communication","title":"Microservices Communication","text":"<p>Build microservices that communicate through shared memory.</p> <pre><code>// Service Registry\nclass ServiceRegistry {\n    memory mem;\n    map&lt;uint32_t, ServiceInfo&gt; services;\n\npublic:\n    ServiceRegistry() : mem(\"/services\", 10*1024*1024),\n                       services(mem, \"registry\", 100) {}\n\n    void register_service(uint32_t id, const ServiceInfo&amp; info) {\n        services.insert(id, info);\n    }\n\n    std::optional&lt;ServiceInfo&gt; lookup(uint32_t id) {\n        return services.get(id);\n    }\n};\n\n// Service Base Class\nclass Service {\nprotected:\n    memory mem;\n    queue&lt;Request&gt; requests;\n    map&lt;uint32_t, future&lt;Response&gt;*&gt; responses;\n\npublic:\n    Service(std::string_view name, size_t mem_size)\n        : mem(std::string(name), mem_size),\n          requests(mem, \"requests\", 100),\n          responses(mem, \"responses\", 100) {}\n\n    virtual Response handle_request(const Request&amp; req) = 0;\n\n    void run() {\n        while (true) {\n            if (auto req = requests.pop()) {\n                Response res = handle_request(*req);\n\n                future&lt;Response&gt; response(mem, \n                    \"response_\" + std::to_string(req-&gt;id));\n                response.set_value(res);\n            }\n        }\n    }\n};\n\n// Example Service\nclass DataService : public Service {\npublic:\n    DataService() : Service(\"/data_service\", 50*1024*1024) {}\n\n    Response handle_request(const Request&amp; req) override {\n        switch (req.type) {\n            case QUERY:\n                return execute_query(req.data);\n            case UPDATE:\n                return update_data(req.data);\n            default:\n                return error_response(\"Unknown request type\");\n        }\n    }\n};\n</code></pre>"},{"location":"patterns/#event-sourcing","title":"Event Sourcing","text":"<p>Store events and rebuild state from event stream.</p> <pre><code>class EventStore {\n    memory mem;\n    stream&lt;Event&gt; events;\n    std::atomic&lt;uint64_t&gt; event_id{0};\n\npublic:\n    EventStore() : mem(\"/events\", 1000*1024*1024),  // 1GB\n                   events(mem, \"event_stream\", 1000000) {}\n\n    void append(Event e) {\n        e.id = event_id.fetch_add(1);\n        e.timestamp = std::chrono::system_clock::now();\n        events.emit(e);\n    }\n\n    // Replay events to rebuild state\n    template&lt;typename State&gt;\n    State replay(State initial, \n                 std::function&lt;State(State, Event)&gt; reducer) {\n        State state = initial;\n\n        events.subscribe([&amp;](const Event&amp; e) {\n            state = reducer(state, e);\n        });\n\n        return state;\n    }\n};\n\n// Usage\nEventStore store;\n\n// Append events\nstore.append({CREATE_USER, user_data});\nstore.append({UPDATE_PROFILE, profile_data});\nstore.append({DELETE_USER, user_id});\n\n// Rebuild current state\nauto current_state = store.replay(InitialState{}, \n    [](auto state, const Event&amp; e) {\n        return apply_event(state, e);\n    });\n</code></pre>"},{"location":"patterns/#cqrs-command-query-responsibility-segregation","title":"CQRS (Command Query Responsibility Segregation)","text":"<p>Separate read and write models.</p> <pre><code>class CQRSSystem {\n    memory write_mem;\n    memory read_mem;\n\n    // Write side\n    queue&lt;Command&gt; commands;\n    stream&lt;Event&gt; events;\n\n    // Read side\n    array&lt;QueryModel&gt; read_models;\n    lazy&lt;Statistics&gt; stats;\n\npublic:\n    CQRSSystem() \n        : write_mem(\"/write\", 100*1024*1024),\n          read_mem(\"/read\", 200*1024*1024),\n          commands(write_mem, \"commands\", 1000),\n          events(write_mem, \"events\", 10000),\n          read_models(read_mem, \"models\", 10000),\n          stats(read_mem, \"stats\") {}\n\n    // Command handler (write side)\n    void handle_commands() {\n        while (auto cmd = commands.pop()) {\n            Event e = process_command(*cmd);\n            events.emit(e);\n\n            // Update read models asynchronously\n            update_read_model(e);\n        }\n    }\n\n    // Query handler (read side)\n    QueryResult query(const Query&amp; q) {\n        // Read from optimized read models\n        switch (q.type) {\n            case GET_BY_ID:\n                return read_models[q.id];\n            case GET_STATS:\n                return stats.get();\n            default:\n                return {};\n        }\n    }\n\nprivate:\n    void update_read_model(const Event&amp; e) {\n        // Project event to read model\n        read_models[e.entity_id] = project(e);\n\n        // Invalidate cached statistics\n        stats.invalidate();\n    }\n};\n</code></pre>"},{"location":"patterns/#reliability-patterns","title":"Reliability Patterns","text":""},{"location":"patterns/#circuit-breaker","title":"Circuit Breaker","text":"<p>Prevent cascading failures in distributed systems.</p> <pre><code>class CircuitBreaker {\n    enum State { CLOSED, OPEN, HALF_OPEN };\n\n    memory&amp; mem;\n    std::atomic&lt;State&gt; state{CLOSED};\n    std::atomic&lt;int&gt; failure_count{0};\n    std::atomic&lt;uint64_t&gt; last_failure_time{0};\n\n    const int failure_threshold = 5;\n    const int timeout_ms = 5000;\n    const int half_open_success_threshold = 3;\n\npublic:\n    CircuitBreaker(memory&amp; m) : mem(m) {}\n\n    template&lt;typename Func&gt;\n    auto call(Func func) -&gt; std::optional&lt;decltype(func())&gt; {\n        if (state.load() == OPEN) {\n            auto now = current_time_ms();\n            if (now - last_failure_time.load() &gt; timeout_ms) {\n                state.store(HALF_OPEN);\n            } else {\n                return std::nullopt;  // Fast fail\n            }\n        }\n\n        try {\n            auto result = func();\n\n            if (state.load() == HALF_OPEN) {\n                // Success in half-open state, close circuit\n                state.store(CLOSED);\n                failure_count.store(0);\n            }\n\n            return result;\n\n        } catch (...) {\n            failure_count.fetch_add(1);\n            last_failure_time.store(current_time_ms());\n\n            if (failure_count.load() &gt;= failure_threshold) {\n                state.store(OPEN);\n            }\n\n            return std::nullopt;\n        }\n    }\n};\n\n// Usage\nCircuitBreaker breaker(mem);\n\nif (auto result = breaker.call([&amp;]() { \n    return risky_operation(); \n})) {\n    process(*result);\n} else {\n    use_fallback();\n}\n</code></pre>"},{"location":"patterns/#saga-pattern","title":"Saga Pattern","text":"<p>Manage distributed transactions with compensations.</p> <pre><code>class Saga {\n    struct Step {\n        std::function&lt;bool()&gt; action;\n        std::function&lt;void()&gt; compensation;\n        bool completed = false;\n    };\n\n    memory&amp; mem;\n    std::vector&lt;Step&gt; steps;\n    stream&lt;SagaEvent&gt; events;\n\npublic:\n    Saga(memory&amp; m, std::string_view name) \n        : mem(m), events(m, std::string(name) + \"_events\", 1000) {}\n\n    void add_step(auto action, auto compensation) {\n        steps.push_back({action, compensation, false});\n    }\n\n    bool execute() {\n        for (size_t i = 0; i &lt; steps.size(); i++) {\n            events.emit({STEP_STARTED, i});\n\n            if (!steps[i].action()) {\n                events.emit({STEP_FAILED, i});\n\n                // Compensate completed steps in reverse\n                for (int j = i - 1; j &gt;= 0; j--) {\n                    if (steps[j].completed) {\n                        events.emit({COMPENSATION_STARTED, j});\n                        steps[j].compensation();\n                        events.emit({COMPENSATION_COMPLETED, j});\n                    }\n                }\n\n                return false;\n            }\n\n            steps[i].completed = true;\n            events.emit({STEP_COMPLETED, i});\n        }\n\n        events.emit({SAGA_COMPLETED, 0});\n        return true;\n    }\n};\n\n// Usage: Distributed order processing\nSaga order_saga(mem, \"order_123\");\n\norder_saga.add_step(\n    [&amp;]() { return reserve_inventory(order); },\n    [&amp;]() { release_inventory(order); }\n);\n\norder_saga.add_step(\n    [&amp;]() { return charge_payment(order); },\n    [&amp;]() { refund_payment(order); }\n);\n\norder_saga.add_step(\n    [&amp;]() { return schedule_shipping(order); },\n    [&amp;]() { cancel_shipping(order); }\n);\n\nif (!order_saga.execute()) {\n    notify_order_failed();\n}\n</code></pre>"},{"location":"patterns/#bulkhead-isolation","title":"Bulkhead Isolation","text":"<p>Isolate resources to prevent total system failure.</p> <pre><code>class Bulkhead {\n    memory&amp; mem;\n    std::vector&lt;pool&lt;Resource&gt;&gt; pools;\n\npublic:\n    Bulkhead(memory&amp; m, size_t num_compartments, size_t resources_per)\n        : mem(m) {\n\n        for (size_t i = 0; i &lt; num_compartments; i++) {\n            pools.emplace_back(mem, \n                \"pool_\" + std::to_string(i), \n                resources_per);\n        }\n    }\n\n    template&lt;typename Func&gt;\n    auto execute_in_compartment(size_t compartment, Func func) {\n        auto* resource = pools[compartment].allocate();\n        if (!resource) {\n            throw std::runtime_error(\"Compartment \" + \n                std::to_string(compartment) + \" exhausted\");\n        }\n\n        // RAII wrapper for automatic return\n        struct Guard {\n            pool&lt;Resource&gt;&amp; p;\n            Resource* r;\n            ~Guard() { p.deallocate(r); }\n        } guard{pools[compartment], resource};\n\n        return func(resource);\n    }\n};\n\n// Usage: Isolate different request types\nBulkhead bulkhead(mem, 3, 10);  // 3 compartments, 10 resources each\n\n// Critical requests use compartment 0\nbulkhead.execute_in_compartment(0, [](auto* res) {\n    handle_critical_request(res);\n});\n\n// Normal requests use compartment 1\nbulkhead.execute_in_compartment(1, [](auto* res) {\n    handle_normal_request(res);\n});\n\n// Background tasks use compartment 2\nbulkhead.execute_in_compartment(2, [](auto* res) {\n    handle_background_task(res);\n});\n</code></pre>"},{"location":"patterns/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"patterns/#design-principles","title":"Design Principles","text":"<ol> <li>Loose Coupling: Use named structures for discovery rather than hard-coded offsets</li> <li>Backpressure Handling: Always handle full queues/channels gracefully</li> <li>Timeout Everything: Never block indefinitely, always use timeouts</li> <li>Error Propagation: Use futures and channels to propagate errors</li> <li>Resource Cleanup: Ensure proper cleanup in all code paths</li> </ol>"},{"location":"patterns/#performance-guidelines","title":"Performance Guidelines","text":"<ol> <li>Batch Operations: Process multiple items at once when possible</li> <li>Avoid Polling: Use blocking operations with timeouts instead of busy-waiting</li> <li>Cache Locality: Group related data in memory</li> <li>Lock-Free First: Prefer lock-free structures for high concurrency</li> <li>Profile and Monitor: Measure actual performance, don't guess</li> </ol>"},{"location":"patterns/#reliability-guidelines","title":"Reliability Guidelines","text":"<ol> <li>Graceful Degradation: Design for partial failures</li> <li>Idempotency: Make operations repeatable without side effects</li> <li>Compensations: Plan rollback strategies for distributed operations</li> <li>Health Checks: Implement health monitoring for all services</li> <li>Observability: Log, trace, and monitor all critical paths</li> </ol>"},{"location":"patterns/#testing-strategies","title":"Testing Strategies","text":"<ol> <li>Chaos Testing: Randomly kill processes to test recovery</li> <li>Load Testing: Test with realistic data volumes and rates</li> <li>Race Condition Testing: Use thread sanitizers and stress tests</li> <li>Integration Testing: Test complete workflows across processes</li> <li>Performance Regression: Track performance metrics over time</li> </ol>"},{"location":"patterns/#conclusion","title":"Conclusion","text":"<p>These patterns demonstrate how ZeroIPC's combination of traditional data structures and codata primitives enable sophisticated multi-process architectures. The key is choosing the right pattern for your specific use case and combining them effectively to build robust, scalable systems.</p>"},{"location":"performance/","title":"Performance Guide","text":""},{"location":"performance/#memory-access-performance-analysis","title":"Memory Access Performance Analysis","text":""},{"location":"performance/#the-zero-overhead-claim","title":"The Zero-Overhead Claim","text":"<p>Claim: Shared memory reads are as fast as normal array reads.</p> <p>Proof: Benchmarked on Intel i7-12700K @ 5.0GHz, 32GB DDR5-5600</p> <pre><code>=== Read Performance Benchmark ===\nArray size: 10000 integers\nIterations: 1000000\n\nSequential Read Performance:\n-----------------------------\nHeap array (sequential):              2.324 ns/operation\nStack array (sequential):             2.654 ns/operation\nShared array operator[] (sequential): 2.318 ns/operation  \u2190 Identical!\nShared array raw pointer (sequential): 2.316 ns/operation \u2190 Direct access\n\nRandom Access Performance:\n-----------------------------\nHeap array (random):                  2.327 ns/operation\nShared array operator[] (random):     2.326 ns/operation  \u2190 Same penalty\nShared array raw pointer (random):    2.378 ns/operation\n</code></pre>"},{"location":"performance/#why-its-so-fast","title":"Why It's So Fast","text":""},{"location":"performance/#1-memory-hierarchy-identical-path","title":"1. Memory Hierarchy - Identical Path","text":"<pre><code>CPU Register (0 cycles)\n     \u2193\nL1 Cache (4 cycles, ~0.8ns)\n     \u2193\nL2 Cache (12 cycles, ~2.4ns)\n     \u2193\nL3 Cache (42 cycles, ~8.4ns)\n     \u2193\nMain Memory (200+ cycles, ~40ns)\n</code></pre> <p>Both heap and shared memory follow the exact same path.</p>"},{"location":"performance/#2-assembly-analysis","title":"2. Assembly Analysis","text":"<pre><code>; Normal array access\nmov     rax, QWORD PTR [rbp-24]  ; Load base pointer\nmov     edx, DWORD PTR [rbp-28]  ; Load index\nmov     eax, DWORD PTR [rax+rdx*4] ; Read array[index]\n\n; Shared memory access (after setup)\nmov     rax, QWORD PTR [rbp-32]  ; Load base pointer  \nmov     edx, DWORD PTR [rbp-36]  ; Load index\nmov     eax, DWORD PTR [rax+rdx*4] ; Read array[index] - IDENTICAL!\n</code></pre>"},{"location":"performance/#3-cache-line-behavior","title":"3. Cache Line Behavior","text":"<ul> <li>64-byte cache lines loaded identically</li> <li>Hardware prefetching works the same</li> <li>Spatial locality preserved</li> <li>Temporal locality preserved</li> </ul>"},{"location":"performance/#lock-free-performance","title":"Lock-Free Performance","text":""},{"location":"performance/#atomic-operations-timing","title":"Atomic Operations Timing","text":"Operation x86-64 Cycles Time (5GHz) Notes Load (relaxed) 1 0.2ns Same as normal load Store (relaxed) 1 0.2ns Same as normal store CAS (uncontended) 10-20 2-4ns Lock cmpxchg CAS (contended) 100-300 20-60ns Cache line ping-pong Fetch-Add 10-20 2-4ns Lock xadd"},{"location":"performance/#queue-performance","title":"Queue Performance","text":"<pre><code>// Measured enqueue/dequeue pairs\nSingle Producer/Consumer:  8-12ns per operation\nMultiple Producers (4):    25-40ns per operation (contention)\nBatch Operations (n=100):  2-3ns amortized per item\n</code></pre>"},{"location":"performance/#object-pool-performance","title":"Object Pool Performance","text":"<pre><code>// Allocation performance vs alternatives\nObject Pool acquire():     10-15ns   \u2190 Lock-free stack\nmalloc():                  40-80ns   \u2190 System allocator\nnew T():                   45-85ns   \u2190 C++ allocator\nmmap():                    500-1000ns \u2190 System call\n</code></pre>"},{"location":"performance/#optimization-techniques","title":"Optimization Techniques","text":""},{"location":"performance/#1-cache-line-alignment","title":"1. Cache Line Alignment","text":"<pre><code>struct alignas(64) CacheAligned {\n    std::atomic&lt;uint64_t&gt; counter;\n    char padding[56];  // Prevent false sharing\n};\n</code></pre> <p>Impact: 10-50x improvement under contention</p>"},{"location":"performance/#2-huge-pages-2mb1gb","title":"2. Huge Pages (2MB/1GB)","text":"<pre><code># Enable huge pages\necho 1024 &gt; /proc/sys/vm/nr_hugepages\n\n# Mount hugetlbfs\nmount -t hugetlbfs none /mnt/hugepages\n\n# Use MAP_HUGETLB flag\nmmap(NULL, size, PROT_READ|PROT_WRITE, \n     MAP_SHARED|MAP_HUGETLB, fd, 0);\n</code></pre> <p>Impact:  - Reduces TLB misses by 512x (4KB\u21922MB) - 5-15% performance improvement for large datasets</p>"},{"location":"performance/#3-numa-awareness","title":"3. NUMA Awareness","text":"<pre><code>// Pin to local NUMA node\nnuma_set_localalloc();\nnuma_tonode_memory(addr, size, numa_node_of_cpu(cpu));\n\n// Measure distance\nint distance = numa_distance(node1, node2);\n// Local: 10, Remote: 20+\n</code></pre> <p>Impact:  - Local access: ~50ns - Remote access: ~100-150ns - 2-3x penalty for remote NUMA access</p>"},{"location":"performance/#4-prefetching","title":"4. Prefetching","text":"<pre><code>// Manual prefetching for random access\nfor (int i = 0; i &lt; n; i++) {\n    __builtin_prefetch(&amp;array[indices[i+8]], 0, 1);\n    process(array[indices[i]]);\n}\n</code></pre> <p>Impact: 20-40% improvement for random patterns</p>"},{"location":"performance/#real-world-benchmarks","title":"Real-World Benchmarks","text":""},{"location":"performance/#particle-simulation-100k-particles","title":"Particle Simulation (100K particles)","text":"<pre><code>Traditional (message passing):\n- Serialize particles:     850 \u00b5s\n- Send via socket:         420 \u00b5s  \n- Deserialize:            780 \u00b5s\n- Total:                 2050 \u00b5s\n\nShared Memory:\n- Write to shm_array:      12 \u00b5s  \u2190 170x faster!\n- Read from shm_array:      8 \u00b5s\n- Total:                   20 \u00b5s\n</code></pre>"},{"location":"performance/#sensor-data-pipeline-1mhz-sampling","title":"Sensor Data Pipeline (1MHz sampling)","text":"<pre><code>Traditional (pipes):\n- Max throughput:      50K samples/sec\n- Latency:            20-50 \u00b5s\n- CPU usage:          45%\n\nShared Memory (ring buffer):\n- Max throughput:      10M samples/sec  \u2190 200x higher!\n- Latency:            50-100 ns         \u2190 400x lower!\n- CPU usage:          8%\n</code></pre>"},{"location":"performance/#memory-overhead","title":"Memory Overhead","text":""},{"location":"performance/#table-size-configurations","title":"Table Size Configurations","text":"Configuration Table Overhead Use Case shm_table16 (16,16) 904 bytes Embedded, minimal shm_table (32,64) 4,168 bytes Default, balanced shm_table256 (64,256) 26,632 bytes Complex simulations shm_table1024 (256,1024) 422,920 bytes Maximum flexibility"},{"location":"performance/#per-structure-overhead","title":"Per-Structure Overhead","text":"<pre><code>shm_array&lt;T&gt;:      0 bytes (just data)\nshm_queue&lt;T&gt;:      16 bytes (head + tail atomics)  \nshm_atomic&lt;T&gt;:     0 bytes (just atomic)\nshm_object_pool&lt;T&gt;: 12 bytes + N*4 bytes (free list)\nshm_ring_buffer&lt;T&gt;: 16 bytes (read + write positions)\n</code></pre>"},{"location":"performance/#scalability-analysis","title":"Scalability Analysis","text":""},{"location":"performance/#process-scaling","title":"Process Scaling","text":"<pre><code>Readers     Throughput (ops/sec)\n1           450M\n2           890M  (1.98x)\n4           1750M (3.89x)\n8           3400M (7.56x)\n16          6200M (13.8x)\n</code></pre> <p>Near-linear scaling for read-heavy workloads!</p>"},{"location":"performance/#contention-characteristics","title":"Contention Characteristics","text":"<pre><code>Writers    Queue Throughput    Array Writes\n1          120M ops/sec        450M ops/sec\n2          95M ops/sec         380M ops/sec  \n4          70M ops/sec         290M ops/sec\n8          45M ops/sec         180M ops/sec\n</code></pre>"},{"location":"performance/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"performance/#linux","title":"Linux","text":"<ul> <li>Best performance with <code>MAP_POPULATE</code></li> <li>Use <code>madvise(MADV_HUGEPAGE)</code> for THP</li> <li>Consider <code>memfd_create()</code> for anonymous shared memory</li> </ul>"},{"location":"performance/#macos","title":"macOS","text":"<ul> <li>Limited to 4GB shared memory by default</li> <li>Increase with <code>kern.sysv.shmmax</code> sysctl</li> <li>No huge page support</li> </ul>"},{"location":"performance/#freebsd","title":"FreeBSD","text":"<ul> <li>Excellent performance with <code>minherit(INHERIT_SHARE)</code></li> <li>Support for super pages via <code>mmap(MAP_ALIGNED_SUPER)</code></li> </ul>"},{"location":"performance/#profiling-tuning","title":"Profiling &amp; Tuning","text":""},{"location":"performance/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<ol> <li> <p>Cache Misses <pre><code>perf stat -e cache-misses,cache-references ./app\n</code></pre></p> </li> <li> <p>TLB Misses <pre><code>perf stat -e dTLB-load-misses ./app\n</code></pre></p> </li> <li> <p>False Sharing <pre><code>perf c2c record ./app\nperf c2c report\n</code></pre></p> </li> <li> <p>Lock Contention <pre><code>perf record -e lock:* ./app\nperf report\n</code></pre></p> </li> </ol>"},{"location":"performance/#best-practices-summary","title":"Best Practices Summary","text":"<p>\u2705 DO: - Align structures to cache lines (64 bytes) - Use huge pages for datasets &gt; 10MB - Batch operations when possible - Profile with <code>perf</code> on Linux - Consider NUMA topology</p> <p>\u274c DON'T: - Share cache lines between writers - Use atomic operations unnecessarily - Assume uniform memory access on NUMA - Forget to handle page faults gracefully - Mix frequently/infrequently accessed data</p>"},{"location":"tutorial/","title":"Tutorial: Building High-Performance IPC Systems","text":""},{"location":"tutorial/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Concepts</li> <li>Simple Producer-Consumer</li> <li>Particle Simulation System</li> <li>Sensor Data Pipeline</li> <li>Multi-Process Coordination</li> <li>Error Handling &amp; Recovery</li> </ol>"},{"location":"tutorial/#basic-concepts","title":"Basic Concepts","text":""},{"location":"tutorial/#the-power-of-the-table-system","title":"The Power of the Table System","text":"<p>The metadata table is what makes our shared memory system powerful. It allows multiple data structures to coexist in the same shared memory segment, each discoverable by name. This is crucial for complex systems.</p> <pre><code>// One shared memory segment, many data structures!\nposix_shm shm(\"my_system\", 100 * 1024 * 1024);  // 100MB total\n\n// All these live in the SAME shared memory:\nshm_array&lt;float&gt; sensor_data(shm, \"sensors\", 10000);\nshm_queue&lt;Event&gt; event_queue(shm, \"events\", 500);\nshm_atomic_uint64 frame_counter(shm, \"frame\", 0);\nshm_object_pool&lt;Entity&gt; entities(shm, \"entities\", 1000);\nshm_ring_buffer&lt;LogEntry&gt; logs(shm, \"logs\", 10000);\n\n// Another process can discover ALL of them:\nposix_shm shm2(\"my_system\");  // Open existing\nshm_array&lt;float&gt; sensors2(shm2, \"sensors\");  // Find by name\nshm_queue&lt;Event&gt; events2(shm2, \"events\");    // Find by name\n// ... etc\n</code></pre>"},{"location":"tutorial/#creating-shared-memory","title":"Creating Shared Memory","text":"<pre><code>#include \"posix_shm.h\"\n\n// Process 1: Create shared memory\nposix_shm shm(\"my_simulation\", 10 * 1024 * 1024);  // 10MB\n\n// Process 2: Open existing shared memory\nposix_shm shm(\"my_simulation\");  // Size discovered automatically\n</code></pre>"},{"location":"tutorial/#choosing-table-configuration","title":"Choosing Table Configuration","text":"<pre><code>// Minimal overhead (904B) for embedded systems\nposix_shm_small shm(\"embedded\", 1024 * 1024);\n\n// Default (4KB) for most applications  \nposix_shm shm(\"default\", 10 * 1024 * 1024);\n\n// Large (26KB) for complex simulations\nposix_shm_large shm(\"complex\", 100 * 1024 * 1024);\n\n// Custom configuration\nusing my_table = shm_table_impl&lt;48, 128&gt;;  // 48 char names, 128 entries\nusing my_shm = posix_shm_impl&lt;my_table&gt;;\nmy_shm shm(\"custom\", 50 * 1024 * 1024);\n</code></pre>"},{"location":"tutorial/#complete-multi-structure-example","title":"Complete Multi-Structure Example","text":""},{"location":"tutorial/#game-server-state-multiple-structures-in-one-segment","title":"Game Server State (Multiple Structures in One Segment)","text":"<p>This example shows how a game server uses multiple data structures in a single shared memory segment for different purposes:</p> <pre><code>// game_server.cpp\n#include \"posix_shm.h\"\n#include \"shm_array.h\"\n#include \"shm_queue.h\"\n#include \"shm_object_pool.h\"\n#include \"shm_atomic.h\"\n#include \"shm_ring_buffer.h\"\n\nclass GameServer {\nprivate:\n    // Single shared memory segment for everything\n    posix_shm_large shm;  // Using large table for many structures\n\n    // Game entities (dynamic allocation)\n    shm_object_pool&lt;Player, shm_table256&gt; player_pool;\n    shm_object_pool&lt;Monster, shm_table256&gt; monster_pool;\n    shm_object_pool&lt;Projectile, shm_table256&gt; projectile_pool;\n\n    // Spatial grid for collision detection\n    shm_array&lt;uint32_t, shm_table256&gt; collision_grid;\n\n    // Event queues for different systems\n    shm_queue&lt;DamageEvent, shm_table256&gt; damage_events;\n    shm_queue&lt;SpawnRequest, shm_table256&gt; spawn_requests;\n    shm_queue&lt;NetworkMessage, shm_table256&gt; network_msgs;\n\n    // Performance metrics\n    shm_ring_buffer&lt;FrameStats, shm_table256&gt; frame_history;\n\n    // Global state\n    shm_atomic_uint64&lt;shm_table256&gt; tick_counter;\n    shm_atomic_uint32&lt;shm_table256&gt; player_count;\n    shm_atomic_bool&lt;shm_table256&gt; match_active;\n\n    // Leaderboard\n    shm_array&lt;ScoreEntry, shm_table256&gt; leaderboard;\n\npublic:\n    GameServer() \n        : shm(\"game_server\", 500 * 1024 * 1024),  // 500MB total\n          // Initialize all structures in the SAME shared memory\n          player_pool(shm, \"players\", 100),\n          monster_pool(shm, \"monsters\", 1000),  \n          projectile_pool(shm, \"projectiles\", 500),\n          collision_grid(shm, \"collision\", 256 * 256),\n          damage_events(shm, \"damage_queue\", 1000),\n          spawn_requests(shm, \"spawn_queue\", 100),\n          network_msgs(shm, \"network_queue\", 5000),\n          frame_history(shm, \"frame_stats\", 3600),  // 1 minute at 60fps\n          tick_counter(shm, \"tick\", 0),\n          player_count(shm, \"players_online\", 0),\n          match_active(shm, \"match_active\", false),\n          leaderboard(shm, \"leaderboard\", 10) {\n\n        std::cout &lt;&lt; \"Game server initialized with structures:\\n\";\n        print_memory_layout();\n    }\n\n    void print_memory_layout() {\n        // The table tracks everything!\n        auto* table = shm.get_table();\n\n        std::cout &lt;&lt; \"Shared Memory Layout:\\n\";\n        std::cout &lt;&lt; \"Total size: 500MB\\n\";\n        std::cout &lt;&lt; \"Table overhead: \" &lt;&lt; sizeof(shm_table256) &lt;&lt; \" bytes\\n\";\n        std::cout &lt;&lt; \"Structures allocated: \" &lt;&lt; table-&gt;get_entry_count() &lt;&lt; \"\\n\\n\";\n\n        // We could iterate through all entries if the table exposed that\n        std::cout &lt;&lt; \"Named structures:\\n\";\n        std::cout &lt;&lt; \"  - players (object pool): \" &lt;&lt; player_pool.capacity() &lt;&lt; \" slots\\n\";\n        std::cout &lt;&lt; \"  - monsters (object pool): \" &lt;&lt; monster_pool.capacity() &lt;&lt; \" slots\\n\"; \n        std::cout &lt;&lt; \"  - projectiles (object pool): \" &lt;&lt; projectile_pool.capacity() &lt;&lt; \" slots\\n\";\n        std::cout &lt;&lt; \"  - collision (array): \" &lt;&lt; collision_grid.size() &lt;&lt; \" cells\\n\";\n        std::cout &lt;&lt; \"  - damage_queue: \" &lt;&lt; damage_events.capacity() &lt;&lt; \" capacity\\n\";\n        std::cout &lt;&lt; \"  - spawn_queue: \" &lt;&lt; spawn_requests.capacity() &lt;&lt; \" capacity\\n\";\n        std::cout &lt;&lt; \"  - network_queue: \" &lt;&lt; network_msgs.capacity() &lt;&lt; \" capacity\\n\";\n        std::cout &lt;&lt; \"  - frame_stats (ring): \" &lt;&lt; frame_history.capacity() &lt;&lt; \" samples\\n\";\n        std::cout &lt;&lt; \"  - tick (atomic): current=\" &lt;&lt; tick_counter.load() &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  - players_online (atomic): \" &lt;&lt; player_count.load() &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  - match_active (atomic): \" &lt;&lt; match_active.load() &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"  - leaderboard (array): \" &lt;&lt; leaderboard.size() &lt;&lt; \" entries\\n\";\n    }\n};\n\n// ai_system.cpp - Separate process that reads game state\nclass AISystem {\nprivate:\n    posix_shm_large shm;\n\n    // Discover existing structures by name\n    shm_object_pool&lt;Player, shm_table256&gt; players;\n    shm_object_pool&lt;Monster, shm_table256&gt; monsters;\n    shm_array&lt;uint32_t, shm_table256&gt; collision_grid;\n    shm_queue&lt;SpawnRequest, shm_table256&gt; spawn_queue;\n    shm_atomic_uint64&lt;shm_table256&gt; tick;\n\npublic:\n    AISystem()\n        : shm(\"game_server\"),  // Attach to existing\n          // Find all structures by name - they already exist!\n          players(shm, \"players\"),\n          monsters(shm, \"monsters\"),\n          collision_grid(shm, \"collision\"),\n          spawn_queue(shm, \"spawn_queue\"),\n          tick(shm, \"tick\") {\n\n        std::cout &lt;&lt; \"AI System connected to game server\\n\";\n        std::cout &lt;&lt; \"Found \" &lt;&lt; monsters.num_allocated() &lt;&lt; \" active monsters\\n\";\n    }\n\n    void update_ai() {\n        uint64_t current_tick = tick.load();\n\n        // Read monster positions, make decisions\n        for (uint32_t handle = 0; handle &lt; monsters.capacity(); ++handle) {\n            if (auto* monster = monsters.get(handle)) {\n                // Read collision grid around monster\n                int grid_x = monster-&gt;x / CELL_SIZE;\n                int grid_y = monster-&gt;y / CELL_SIZE;\n                uint32_t cell = collision_grid[grid_y * 256 + grid_x];\n\n                // Make AI decision...\n                if (should_spawn_adds(monster)) {\n                    SpawnRequest req{\n                        .type = SPAWN_MINION,\n                        .x = monster-&gt;x,\n                        .y = monster-&gt;y\n                    };\n                    spawn_queue.enqueue(req);  // Communicate back!\n                }\n            }\n        }\n    }\n};\n\n// analytics.cpp - Another process for metrics\nclass Analytics {\nprivate:\n    posix_shm_large shm;\n    shm_ring_buffer&lt;FrameStats, shm_table256&gt; frame_history;\n    shm_atomic_uint32&lt;shm_table256&gt; player_count;\n    shm_array&lt;ScoreEntry, shm_table256&gt; leaderboard;\n\npublic:\n    Analytics()\n        : shm(\"game_server\"),\n          frame_history(shm, \"frame_stats\"),\n          player_count(shm, \"players_online\"),\n          leaderboard(shm, \"leaderboard\") {\n    }\n\n    void generate_report() {\n        // Read last 60 seconds of frame data\n        FrameStats stats[3600];\n        size_t count = frame_history.get_last_n(3600, stats);\n\n        // Calculate metrics\n        double avg_fps = calculate_average_fps(stats, count);\n        uint32_t current_players = player_count.load();\n\n        std::cout &lt;&lt; \"=== Performance Report ===\\n\";\n        std::cout &lt;&lt; \"Average FPS: \" &lt;&lt; avg_fps &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"Players online: \" &lt;&lt; current_players &lt;&lt; \"\\n\";\n        std::cout &lt;&lt; \"\\nTop Players:\\n\";\n\n        for (size_t i = 0; i &lt; leaderboard.size(); ++i) {\n            auto&amp; entry = leaderboard[i];\n            if (entry.score &gt; 0) {\n                std::cout &lt;&lt; i+1 &lt;&lt; \". \" &lt;&lt; entry.name \n                          &lt;&lt; \" - \" &lt;&lt; entry.score &lt;&lt; \"\\n\";\n            }\n        }\n    }\n};\n</code></pre>"},{"location":"tutorial/#memory-layout-visualization","title":"Memory Layout Visualization","text":"<p>Here's what the shared memory segment looks like with all these structures:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Shared Memory: \"game_server\" (500MB)          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Offset    | Size    | Structure               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0x0000    | 4B      | Reference Counter       \u2502\n\u2502 0x0004    | 26KB    | shm_table256         \u2502\n\u2502           |         |   \u251c\u2500 \"players\"          \u2502\n\u2502           |         |   \u251c\u2500 \"monsters\"         \u2502\n\u2502           |         |   \u251c\u2500 \"projectiles\"      \u2502\n\u2502           |         |   \u251c\u2500 \"collision\"        \u2502\n\u2502           |         |   \u251c\u2500 \"damage_queue\"     \u2502\n\u2502           |         |   \u251c\u2500 \"spawn_queue\"      \u2502\n\u2502           |         |   \u251c\u2500 \"network_queue\"    \u2502\n\u2502           |         |   \u251c\u2500 \"frame_stats\"      \u2502\n\u2502           |         |   \u251c\u2500 \"tick\"             \u2502\n\u2502           |         |   \u251c\u2500 \"players_online\"   \u2502\n\u2502           |         |   \u251c\u2500 \"match_active\"     \u2502\n\u2502           |         |   \u2514\u2500 \"leaderboard\"      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0x6808    | ~4KB    | Player Pool             \u2502\n\u2502 0x7808    | ~40KB   | Monster Pool            \u2502\n\u2502 0x11408   | ~10KB   | Projectile Pool         \u2502\n\u2502 0x13C08   | 256KB   | Collision Grid          \u2502\n\u2502 0x53C08   | ~16KB   | Damage Queue            \u2502\n\u2502 0x57C08   | ~2KB    | Spawn Queue             \u2502\n\u2502 0x58408   | ~80KB   | Network Queue           \u2502\n\u2502 0x6C408   | ~56KB   | Frame History           \u2502\n\u2502 0x7A008   | 8B      | Tick Counter            \u2502\n\u2502 0x7A010   | 4B      | Player Count            \u2502\n\u2502 0x7A014   | 1B      | Match Active            \u2502\n\u2502 0x7A018   | 400B    | Leaderboard             \u2502\n\u2502 ...       | ...     | (Unused space)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorial/#key-points-about-multiple-structures","title":"Key Points About Multiple Structures","text":"<ol> <li>Single Allocation: One <code>posix_shm</code> object manages the entire segment</li> <li>Automatic Layout: The table system tracks offsets automatically</li> <li>No Fragmentation: Structures are allocated sequentially</li> <li>Discovery by Name: Any process can find any structure</li> <li>Type Safety: Template parameters ensure consistency</li> <li>Independent Lifecycles: Each structure can be used independently</li> </ol>"},{"location":"tutorial/#simple-producer-consumer","title":"Simple Producer-Consumer","text":""},{"location":"tutorial/#producer-process","title":"Producer Process","text":"<pre><code>#include \"posix_shm.h\"\n#include \"shm_queue.h\"\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n#include &lt;chrono&gt;\n\nstruct Message {\n    uint64_t timestamp;\n    int producer_id;\n    double value;\n};\n\nint main() {\n    // Create shared memory and queue\n    posix_shm shm(\"producer_consumer\", 1024 * 1024);\n    shm_queue&lt;Message&gt; queue(shm, \"messages\", 100);\n\n    // Produce messages\n    for (int i = 0; i &lt; 1000; ++i) {\n        Message msg{\n            .timestamp = std::chrono::system_clock::now().time_since_epoch().count(),\n            .producer_id = 1,\n            .value = i * 3.14\n        };\n\n        // Retry if queue is full\n        while (!queue.enqueue(msg)) {\n            std::this_thread::sleep_for(std::chrono::microseconds(10));\n        }\n\n        std::cout &lt;&lt; \"Produced message \" &lt;&lt; i &lt;&lt; \"\\n\";\n        std::this_thread::sleep_for(std::chrono::milliseconds(10));\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"tutorial/#consumer-process","title":"Consumer Process","text":"<pre><code>int main() {\n    // Open existing shared memory\n    posix_shm shm(\"producer_consumer\");\n\n    // Discover queue by name\n    shm_queue&lt;Message&gt; queue(shm, \"messages\");\n\n    // Consume messages\n    int count = 0;\n    while (count &lt; 1000) {\n        if (auto msg = queue.dequeue()) {\n            std::cout &lt;&lt; \"Consumed message from producer \" \n                      &lt;&lt; msg-&gt;producer_id \n                      &lt;&lt; \" with value \" &lt;&lt; msg-&gt;value &lt;&lt; \"\\n\";\n            count++;\n        } else {\n            // Queue empty, wait a bit\n            std::this_thread::sleep_for(std::chrono::microseconds(10));\n        }\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"tutorial/#particle-simulation-system","title":"Particle Simulation System","text":""},{"location":"tutorial/#simulation-core","title":"Simulation Core","text":"<pre><code>#include \"posix_shm.h\"\n#include \"shm_object_pool.h\"\n#include \"shm_array.h\"\n#include \"shm_atomic.h\"\n\nclass ParticleSimulation {\nprivate:\n    posix_shm shm;\n    shm_object_pool&lt;Particle&gt; particle_pool;\n    shm_array&lt;uint32_t&gt; active_particles;\n    shm_atomic_uint64 frame_counter;\n    shm_atomic_uint32 active_count;\n\npublic:\n    struct Particle {\n        float position[3];\n        float velocity[3];\n        float mass;\n        float lifetime;\n        uint32_t type;\n        uint32_t flags;\n    };\n\n    ParticleSimulation(const std::string&amp; name, size_t max_particles)\n        : shm(name, calculate_memory_size(max_particles)),\n          particle_pool(shm, \"particles\", max_particles),\n          active_particles(shm, \"active_list\", max_particles),\n          frame_counter(shm, \"frame\", 0),\n          active_count(shm, \"active\", 0) {\n    }\n\n    static size_t calculate_memory_size(size_t max_particles) {\n        return sizeof(shm_table) +                          // Metadata table\n               sizeof(Particle) * max_particles +           // Particle pool\n               sizeof(uint32_t) * max_particles +           // Active list\n               sizeof(std::atomic&lt;uint64_t&gt;) +              // Frame counter\n               sizeof(std::atomic&lt;uint32_t&gt;) +              // Active count\n               1024 * 1024;                                  // Extra buffer\n    }\n\n    uint32_t spawn_particle(const Particle&amp; p) {\n        auto handle = particle_pool.acquire();\n        if (handle == particle_pool.invalid_handle) {\n            return particle_pool.invalid_handle;  // Pool full\n        }\n\n        // Initialize particle\n        particle_pool[handle] = p;\n\n        // Add to active list\n        uint32_t index = active_count.fetch_add(1);\n        active_particles[index] = handle;\n\n        return handle;\n    }\n\n    void update_physics(float dt) {\n        uint32_t count = active_count.load();\n\n        // Parallel update possible - each particle independent\n        #pragma omp parallel for\n        for (uint32_t i = 0; i &lt; count; ++i) {\n            uint32_t handle = active_particles[i];\n            auto&amp; p = particle_pool[handle];\n\n            // Simple physics\n            p.position[0] += p.velocity[0] * dt;\n            p.position[1] += p.velocity[1] * dt;\n            p.position[2] += p.velocity[2] * dt;\n\n            // Gravity\n            p.velocity[1] -= 9.8f * dt;\n\n            // Lifetime\n            p.lifetime -= dt;\n        }\n\n        // Remove dead particles\n        compact_active_list();\n\n        frame_counter++;\n    }\n\n    void compact_active_list() {\n        uint32_t read = 0, write = 0;\n        uint32_t count = active_count.load();\n\n        while (read &lt; count) {\n            uint32_t handle = active_particles[read];\n            auto&amp; p = particle_pool[handle];\n\n            if (p.lifetime &gt; 0) {\n                active_particles[write++] = handle;\n            } else {\n                particle_pool.release(handle);\n            }\n            read++;\n        }\n\n        active_count.store(write);\n    }\n};\n</code></pre>"},{"location":"tutorial/#renderer-process","title":"Renderer Process","text":"<pre><code>int main() {\n    // Open existing simulation\n    posix_shm shm(\"particle_sim\");\n\n    // Discover data structures\n    shm_object_pool&lt;Particle&gt; particles(shm, \"particles\");\n    shm_array&lt;uint32_t&gt; active_list(shm, \"active_list\");\n    shm_atomic_uint32 active_count(shm, \"active\");\n    shm_atomic_uint64 frame(shm, \"frame\");\n\n    uint64_t last_frame = 0;\n\n    while (true) {\n        uint64_t current_frame = frame.load();\n\n        // Only render if new frame\n        if (current_frame != last_frame) {\n            uint32_t count = active_count.load();\n\n            // Read particle positions\n            for (uint32_t i = 0; i &lt; count; ++i) {\n                uint32_t handle = active_list[i];\n                const auto&amp; p = particles[handle];\n\n                render_particle(p.position, p.type);\n            }\n\n            present_frame();\n            last_frame = current_frame;\n        }\n\n        std::this_thread::sleep_for(std::chrono::milliseconds(16));  // 60 FPS\n    }\n}\n</code></pre>"},{"location":"tutorial/#sensor-data-pipeline","title":"Sensor Data Pipeline","text":""},{"location":"tutorial/#high-frequency-data-collection","title":"High-Frequency Data Collection","text":"<pre><code>#include \"shm_ring_buffer.h\"\n\nclass SensorCollector {\nprivate:\n    posix_shm shm;\n    shm_ring_buffer&lt;SensorReading&gt; buffer;\n    shm_atomic_uint64 total_samples;\n    shm_atomic_uint64 dropped_samples;\n\npublic:\n    struct SensorReading {\n        uint64_t timestamp_ns;\n        float values[8];  // 8 channels\n        uint16_t status;\n        uint16_t sensor_id;\n    };\n\n    SensorCollector()\n        : shm(\"sensor_pipeline\", 100 * 1024 * 1024),  // 100MB\n          buffer(shm, \"readings\", 1000000),  // 1M samples\n          total_samples(shm, \"total\", 0),\n          dropped_samples(shm, \"dropped\", 0) {\n    }\n\n    void collect_samples() {\n        SensorReading batch[1000];\n\n        while (true) {\n            // Read from hardware (simulated)\n            size_t collected = read_sensors(batch, 1000);\n\n            // Bulk push for efficiency\n            size_t pushed = buffer.push_bulk({batch, collected});\n\n            total_samples.fetch_add(pushed);\n\n            if (pushed &lt; collected) {\n                // Buffer full, using overwrite mode\n                for (size_t i = pushed; i &lt; collected; ++i) {\n                    buffer.push_overwrite(batch[i]);\n                    dropped_samples++;\n                }\n            }\n\n            // 1MHz sampling rate\n            std::this_thread::sleep_for(std::chrono::microseconds(1000));\n        }\n    }\n};\n</code></pre>"},{"location":"tutorial/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code>class DataProcessor {\nprivate:\n    posix_shm shm;\n    shm_ring_buffer&lt;SensorReading&gt; input_buffer;\n    shm_ring_buffer&lt;ProcessedData&gt; output_buffer;\n\npublic:\n    void process_pipeline() {\n        SensorReading raw_batch[100];\n        ProcessedData processed_batch[100];\n\n        while (true) {\n            // Bulk read from input\n            size_t count = input_buffer.pop_bulk(raw_batch);\n\n            if (count &gt; 0) {\n                // Process data (FFT, filtering, etc.)\n                for (size_t i = 0; i &lt; count; ++i) {\n                    processed_batch[i] = process_reading(raw_batch[i]);\n                }\n\n                // Push to next stage\n                output_buffer.push_bulk({processed_batch, count});\n            } else {\n                // No data, brief wait\n                std::this_thread::sleep_for(std::chrono::microseconds(100));\n            }\n        }\n    }\n\n    ProcessedData process_reading(const SensorReading&amp; raw) {\n        ProcessedData result;\n        // Apply filters, transforms, etc.\n        return result;\n    }\n};\n</code></pre>"},{"location":"tutorial/#multi-process-coordination","title":"Multi-Process Coordination","text":""},{"location":"tutorial/#master-worker-pattern","title":"Master-Worker Pattern","text":"<pre><code>class TaskScheduler {\nprivate:\n    posix_shm shm;\n    shm_queue&lt;Task&gt; task_queue;\n    shm_queue&lt;Result&gt; result_queue;\n    shm_atomic_uint32 workers_ready;\n    shm_atomic_bool shutdown;\n\npublic:\n    struct Task {\n        uint64_t task_id;\n        uint32_t type;\n        char parameters[256];\n    };\n\n    struct Result {\n        uint64_t task_id;\n        uint32_t worker_id;\n        bool success;\n        char output[1024];\n    };\n\n    // Master process\n    void run_master() {\n        // Wait for workers\n        while (workers_ready.load() &lt; 4) {\n            std::this_thread::sleep_for(std::chrono::milliseconds(100));\n        }\n\n        // Distribute tasks\n        for (uint64_t i = 0; i &lt; 1000; ++i) {\n            Task t{.task_id = i, .type = i % 4};\n\n            while (!task_queue.enqueue(t)) {\n                // Process results while waiting\n                process_results();\n            }\n        }\n\n        // Collect all results\n        for (int i = 0; i &lt; 1000; ++i) {\n            process_results();\n        }\n\n        shutdown.store(true);\n    }\n\n    // Worker process\n    void run_worker(uint32_t worker_id) {\n        workers_ready++;\n\n        while (!shutdown.load()) {\n            if (auto task = task_queue.dequeue()) {\n                Result r{\n                    .task_id = task-&gt;task_id,\n                    .worker_id = worker_id,\n                    .success = true\n                };\n\n                // Process task\n                execute_task(*task, r);\n\n                // Submit result\n                while (!result_queue.enqueue(r)) {\n                    if (shutdown.load()) break;\n                    std::this_thread::yield();\n                }\n            } else {\n                std::this_thread::sleep_for(std::chrono::microseconds(10));\n            }\n        }\n    }\n};\n</code></pre>"},{"location":"tutorial/#error-handling-recovery","title":"Error Handling &amp; Recovery","text":""},{"location":"tutorial/#robust-initialization","title":"Robust Initialization","text":"<pre><code>class RobustSystem {\npublic:\n    static std::unique_ptr&lt;RobustSystem&gt; create(const std::string&amp; name) {\n        try {\n            // Try to create new\n            return std::make_unique&lt;RobustSystem&gt;(name, true);\n        } catch (const std::exception&amp; e) {\n            // Try to attach to existing\n            try {\n                return std::make_unique&lt;RobustSystem&gt;(name, false);\n            } catch (...) {\n                // Clean up and retry\n                cleanup_shared_memory(name);\n                return std::make_unique&lt;RobustSystem&gt;(name, true);\n            }\n        }\n    }\n\nprivate:\n    static void cleanup_shared_memory(const std::string&amp; name) {\n        // Force cleanup of stale shared memory\n        shm_unlink(name.c_str());\n    }\n};\n</code></pre>"},{"location":"tutorial/#crash-recovery","title":"Crash Recovery","text":"<pre><code>class CrashResilientQueue {\nprivate:\n    shm_queue&lt;Message&gt; queue;\n    shm_atomic_uint64 last_processed_id;\n\npublic:\n    void process_with_recovery() {\n        uint64_t last_id = last_processed_id.load();\n\n        while (true) {\n            if (auto msg = queue.dequeue()) {\n                if (msg-&gt;id &lt;= last_id) {\n                    // Already processed, skip\n                    continue;\n                }\n\n                try {\n                    process_message(*msg);\n                    last_processed_id.store(msg-&gt;id);\n                } catch (const std::exception&amp; e) {\n                    // Log error but continue\n                    log_error(e.what());\n                    // Message lost but system continues\n                }\n            }\n        }\n    }\n};\n</code></pre>"},{"location":"tutorial/#monitoring-diagnostics","title":"Monitoring &amp; Diagnostics","text":"<pre><code>class SystemMonitor {\nprivate:\n    struct Stats {\n        std::atomic&lt;uint64_t&gt; messages_processed{0};\n        std::atomic&lt;uint64_t&gt; errors_count{0};\n        std::atomic&lt;uint64_t&gt; queue_full_count{0};\n        std::atomic&lt;double&gt; avg_latency_us{0};\n        std::atomic&lt;uint64_t&gt; last_heartbeat{0};\n    };\n\n    posix_shm shm;\n    shm_array&lt;Stats&gt; process_stats;\n\npublic:\n    void monitor_system() {\n        while (true) {\n            auto now = std::chrono::system_clock::now().time_since_epoch().count();\n\n            for (size_t i = 0; i &lt; process_stats.size(); ++i) {\n                auto&amp; stats = process_stats[i];\n                uint64_t last_heartbeat = stats.last_heartbeat.load();\n\n                if (now - last_heartbeat &gt; 5000000000) {  // 5 seconds\n                    std::cerr &lt;&lt; \"Process \" &lt;&lt; i &lt;&lt; \" appears hung!\\n\";\n                    alert_operator(i);\n                }\n\n                std::cout &lt;&lt; \"Process \" &lt;&lt; i &lt;&lt; \": \"\n                          &lt;&lt; stats.messages_processed.load() &lt;&lt; \" messages, \"\n                          &lt;&lt; stats.errors_count.load() &lt;&lt; \" errors, \"\n                          &lt;&lt; stats.avg_latency_us.load() &lt;&lt; \" \u00b5s latency\\n\";\n            }\n\n            std::this_thread::sleep_for(std::chrono::seconds(1));\n        }\n    }\n};\n</code></pre>"},{"location":"tutorial/#best-practices","title":"Best Practices","text":""},{"location":"tutorial/#1-always-name-your-structures","title":"1. Always Name Your Structures","text":"<pre><code>// Good - discoverable\nshm_array&lt;float&gt; data(shm, \"sensor_readings\", 1000);\n\n// Bad - anonymous, not discoverable\nfloat* data = static_cast&lt;float*&gt;(shm.get_base_addr());\n</code></pre>"},{"location":"tutorial/#2-handle-full-conditions-gracefully","title":"2. Handle Full Conditions Gracefully","text":"<pre><code>// Good - retry with backoff\nwhile (!queue.enqueue(msg)) {\n    if (++retries &gt; max_retries) {\n        handle_overflow();\n        break;\n    }\n    std::this_thread::sleep_for(backoff);\n    backoff *= 2;\n}\n\n// Bad - silent data loss\nqueue.enqueue(msg);  // Ignoring return value\n</code></pre>"},{"location":"tutorial/#3-use-bulk-operations-for-efficiency","title":"3. Use Bulk Operations for Efficiency","text":"<pre><code>// Good - amortize overhead\nReading batch[100];\nsize_t count = buffer.pop_bulk(batch);\nprocess_batch(batch, count);\n\n// Bad - one at a time\nfor (int i = 0; i &lt; 100; ++i) {\n    if (auto val = buffer.pop()) {\n        process_single(*val);\n    }\n}\n</code></pre>"},{"location":"tutorial/#4-monitor-system-health","title":"4. Monitor System Health","text":"<pre><code>// Good - observable system\nshm_atomic_uint64 heartbeat(shm, \"heartbeat\");\nwhile (running) {\n    do_work();\n    heartbeat.store(now());\n}\n\n// Bad - black box\nwhile (running) {\n    do_work();  // No visibility\n}\n</code></pre>"},{"location":"tutorial/#5-plan-for-growth","title":"5. Plan for Growth","text":"<pre><code>// Good - configurable\nconstexpr size_t MAX_PARTICLES = \n    std::getenv(\"MAX_PARTICLES\") ? \n    std::atoi(std::getenv(\"MAX_PARTICLES\")) : 10000;\n\n// Bad - hardcoded limits\nconstexpr size_t MAX_PARTICLES = 1000;  // Will need recompile\n</code></pre>"},{"location":"vcpkg_submission/","title":"vcpkg Submission Guide","text":""},{"location":"vcpkg_submission/#steps-to-submit-posix_shm-to-vcpkg","title":"Steps to Submit posix_shm to vcpkg","text":"<ol> <li> <p>Fork the vcpkg repository <pre><code>git clone https://github.com/Microsoft/vcpkg.git\ncd vcpkg\n</code></pre></p> </li> <li> <p>Create a new branch <pre><code>git checkout -b add-posix-shm\n</code></pre></p> </li> <li> <p>Create port directory <pre><code>mkdir ports/posix-shm\n</code></pre></p> </li> <li> <p>Update portfile.cmake</p> </li> </ol> <p>Update the SHA512 hash in <code>vcpkg/portfile.cmake</code>:    <pre><code># Download the release tarball and calculate SHA512\nwget https://github.com/queelius/posix_shm/archive/v1.0.0.tar.gz\nsha512sum v1.0.0.tar.gz\n</code></pre></p> <p>Then update the portfile:    <pre><code>vcpkg_from_github(\n    OUT_SOURCE_PATH SOURCE_PATH\n    REPO queelius/posix_shm\n    REF v1.0.0\n    SHA512 &lt;paste-calculated-sha512&gt;\n    HEAD_REF master\n)\n</code></pre></p> <ol> <li>Copy port files</li> <li>Copy updated <code>portfile.cmake</code> to <code>ports/posix-shm/</code></li> <li> <p>Copy <code>vcpkg.json</code> to <code>ports/posix-shm/</code></p> </li> <li> <p>Test the port <pre><code>./vcpkg install posix-shm\n</code></pre></p> </li> <li> <p>Update versions database <pre><code># Add to versions/p-/posix-shm.json\n{\n  \"versions\": [\n    {\n      \"version\": \"1.0.0\",\n      \"git-tree\": \"&lt;git-tree-hash&gt;\",\n      \"port-version\": 0\n    }\n  ]\n}\n</code></pre></p> </li> </ol> <p>Calculate git-tree hash:    <pre><code>git add ports/posix-shm\ngit commit -m \"temp\"\ngit rev-parse HEAD:ports/posix-shm\n</code></pre></p> <ol> <li>Update baseline</li> </ol> <p>Add to <code>versions/baseline.json</code>:    <pre><code>\"posix-shm\": {\n  \"baseline\": \"1.0.0\",\n  \"port-version\": 0\n}\n</code></pre></p> <ol> <li>Submit Pull Request</li> <li>Commit all changes</li> <li>Push to your fork</li> <li>Open PR to Microsoft/vcpkg</li> <li>Title: \"[posix-shm] Add new port\"</li> <li>Follow the PR checklist</li> </ol>"},{"location":"vcpkg_submission/#important-notes","title":"Important Notes","text":"<ul> <li>vcpkg requires all dependencies to be available in vcpkg</li> <li>The port must build on all supported platforms (or explicitly mark unsupported ones)</li> <li>Header-only libraries should set <code>vcpkg_cmake_config_fixup()</code> appropriately</li> <li>Tests are not built by default in vcpkg ports</li> </ul>"},{"location":"advanced/","title":"Advanced Topics","text":"<p>Advanced concepts and techniques for expert ZeroIPC users.</p>"},{"location":"advanced/#topics","title":"Topics","text":""},{"location":"advanced/#codata-programming","title":"Codata Programming","text":"<p>Understanding codata and computational structures:</p> <ul> <li>What is codata?</li> <li>Futures and promises</li> <li>Lazy evaluation</li> <li>Infinite streams</li> <li>Reactive programming</li> <li>CSP-style channels</li> </ul> <p>Codata represents computations over time rather than values in space, enabling powerful abstractions like async/await across processes.</p>"},{"location":"advanced/#lock-free-patterns","title":"Lock-Free Patterns","text":"<p>Deep dive into lock-free programming:</p> <ul> <li>Compare-And-Swap (CAS) operations</li> <li>ABA problem and solutions</li> <li>Memory ordering models</li> <li>Progress guarantees</li> <li>Lock-free data structures</li> <li>Performance characteristics</li> </ul> <p>Learn how ZeroIPC implements lock-free queues, stacks, and maps for high-performance concurrent access.</p>"},{"location":"advanced/#memory-ordering","title":"Memory Ordering","text":"<p>Understanding C++ memory ordering:</p> <ul> <li>Sequential consistency</li> <li>Acquire-release semantics</li> <li>Relaxed ordering</li> <li>Memory fences</li> <li>Happens-before relationships</li> <li>Cross-process synchronization</li> </ul> <p>Master the subtleties of memory ordering for correct lock-free implementations.</p>"},{"location":"advanced/#custom-structures","title":"Custom Structures","text":"<p>Creating your own ZeroIPC structures:</p> <ul> <li>Designing binary formats</li> <li>Implementing in C++</li> <li>Implementing in Python</li> <li>Adding to the specification</li> <li>Testing cross-language compatibility</li> <li>Contributing back</li> </ul>"},{"location":"advanced/#preview-codata-programming","title":"Preview: Codata Programming","text":"<p>Traditional data structures store values: <pre><code>Array&lt;int&gt; values(mem, \"data\", 100);  // 100 integers stored in memory\n</code></pre></p> <p>Codata structures represent computations: <pre><code>Future&lt;Result&gt; result(mem, \"calc\");  // A result that will exist in the future\nStream&lt;Event&gt; events(mem, \"stream\"); // An infinite sequence of events\nLazy&lt;Value&gt; value(mem, \"lazy\");      // A value computed on-demand\n</code></pre></p>"},{"location":"advanced/#why-codata-matters","title":"Why Codata Matters","text":"<p>Codata enables:</p> <ol> <li> <p>Async/Await Across Processes <pre><code>// Process A\nFuture&lt;double&gt; result(mem, \"pi_calculation\");\nresult.set_value(calculate_pi(1000000));\n\n// Process B\nFuture&lt;double&gt; result(mem, \"pi_calculation\", true);\ndouble pi = result.get();  // Waits if not ready\n</code></pre></p> </li> <li> <p>Reactive Event Processing <pre><code>Stream&lt;Event&gt; raw(mem, \"raw\");\nauto filtered = raw.filter(mem, \"important\", \n    [](Event&amp; e) { return e.priority &gt; 5; });\nauto transformed = filtered.map(mem, \"alerts\",\n    [](Event&amp; e) { return Alert{e}; });\n</code></pre></p> </li> <li> <p>Lazy Evaluation and Caching <pre><code>Lazy&lt;ExpensiveResult&gt; cached(mem, \"result\");\n// First call: computes and caches\nauto r1 = cached.value();\n// Subsequent calls: returns cached value\nauto r2 = cached.value();  // Same instance\n</code></pre></p> </li> </ol>"},{"location":"advanced/#preview-lock-free-patterns","title":"Preview: Lock-Free Patterns","text":"<p>ZeroIPC uses lock-free algorithms for all concurrent structures. Here's how lock-free enqueue works:</p> <pre><code>template&lt;typename T&gt;\nbool Queue&lt;T&gt;::enqueue(const T&amp; value) {\n    uint64_t current_tail, next_tail;\n\n    do {\n        // 1. Load current tail (can be stale)\n        current_tail = tail_.load(std::memory_order_relaxed);\n\n        // 2. Calculate next position\n        next_tail = (current_tail + 1) % capacity_;\n\n        // 3. Check if full\n        if (next_tail == head_.load(std::memory_order_acquire)) {\n            return false;  // Queue full\n        }\n\n        // 4. Try to claim this slot\n    } while (!tail_.compare_exchange_weak(\n        current_tail, \n        next_tail,\n        std::memory_order_relaxed,\n        std::memory_order_relaxed\n    ));\n\n    // 5. Write data (we own this slot now)\n    data_[current_tail] = value;\n\n    // 6. Ensure write is visible to dequeuers\n    std::atomic_thread_fence(std::memory_order_release);\n\n    return true;\n}\n</code></pre> <p>Key points: - No locks, no blocking - CAS loop retries on contention - Memory fences ensure ordering - Safe for multiple producers</p>"},{"location":"advanced/#preview-memory-ordering","title":"Preview: Memory Ordering","text":"<p>Different memory orders have different guarantees:</p> <pre><code>// Relaxed: No ordering guarantees, fastest\nvalue.store(42, std::memory_order_relaxed);\n\n// Release: Previous writes visible to acquire loads\ndata_ready.store(true, std::memory_order_release);\n\n// Acquire: Subsequent reads see previous releases\nif (data_ready.load(std::memory_order_acquire)) {\n    use_data();\n}\n\n// Sequential consistency: Total ordering, slowest\ncounter.fetch_add(1, std::memory_order_seq_cst);\n</code></pre> <p>For shared memory, you need to carefully choose ordering to ensure: 1. Correctness - No races or undefined behavior 2. Performance - Not stricter than necessary 3. Cross-process - Works across process boundaries</p>"},{"location":"advanced/#when-to-read-advanced-topics","title":"When to Read Advanced Topics","text":"<p>These topics are for:</p> <ul> <li>Codata: When building reactive systems or need async/await</li> <li>Lock-Free: When implementing custom structures or debugging races</li> <li>Memory Ordering: When optimizing performance or ensuring correctness</li> <li>Custom Structures: When ZeroIPC doesn't provide what you need</li> </ul>"},{"location":"advanced/#prerequisites","title":"Prerequisites","text":"<p>Before diving into advanced topics:</p> <ul> <li>\u2713 Completed the Tutorial</li> <li>\u2713 Understand Architecture</li> <li>\u2713 Comfortable with C++ templates (for C++ users)</li> <li>\u2713 Understand multi-threading basics</li> <li>\u2713 Familiar with the API Reference</li> </ul>"},{"location":"advanced/#next-steps","title":"Next Steps","text":"<p>Choose your path:</p> <ul> <li>Codata Programming - Computational structures</li> <li>Lock-Free Patterns - Concurrent algorithms</li> <li>Memory Ordering - Synchronization details</li> <li>Custom Structures - Extend ZeroIPC</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for ZeroIPC across all supported languages.</p>"},{"location":"api/#language-apis","title":"Language APIs","text":""},{"location":"api/#c-api","title":"C++ API","text":"<p>Modern C++23 header-only template library.</p> <p>Key Features: - Template-based for zero overhead - RAII resource management - Compile-time type safety - Lock-free atomic operations</p> <p>Documentation: - Memory &amp; Table - Core memory management - Data Structures - Arrays, queues, stacks, etc. - Synchronization - Semaphores, barriers, latches - Codata Structures - Futures, streams, channels</p>"},{"location":"api/#python-api","title":"Python API","text":"<p>Pure Python implementation with NumPy integration.</p> <p>Key Features: - No compilation required - NumPy for performance - Duck typing for flexibility - mmap for direct memory access</p> <p>Documentation: - Memory &amp; Table - Core memory management - Data Structures - Arrays, queues, stacks, etc. - Synchronization - Semaphores, barriers, latches - Codata Structures - Futures, streams, channels</p>"},{"location":"api/#c-api_1","title":"C API","text":"<p>Pure C99 static library for maximum portability.</p> <p>Key Features: - Zero dependencies beyond POSIX - Explicit memory management - Minimal overhead - Maximum portability</p>"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#common-operations","title":"Common Operations","text":""},{"location":"api/#creating-shared-memory","title":"Creating Shared Memory","text":"C++PythonC <pre><code>#include &lt;zeroipc/memory.h&gt;\nzeroipc::Memory mem(\"/name\", 1024*1024);  // 1MB\n</code></pre> <pre><code>from zeroipc import Memory\nmem = Memory(\"/name\", 1024*1024)  # 1MB\n</code></pre> <pre><code>#include &lt;zeroipc/memory.h&gt;\nzipc_memory_t* mem = zipc_memory_create(\"/name\", 1024*1024);\n</code></pre>"},{"location":"api/#creating-an-array","title":"Creating an Array","text":"C++PythonC <pre><code>#include &lt;zeroipc/array.h&gt;\nzeroipc::Array&lt;float&gt; arr(mem, \"data\", 1000);\n</code></pre> <pre><code>from zeroipc import Array\nimport numpy as np\narr = Array(mem, \"data\", dtype=np.float32, capacity=1000)\n</code></pre> <pre><code>#include &lt;zeroipc/array.h&gt;\nzipc_array_t* arr = zipc_array_create(mem, \"data\", sizeof(float), 1000);\n</code></pre>"},{"location":"api/#creating-a-queue","title":"Creating a Queue","text":"C++Python <pre><code>#include &lt;zeroipc/queue.h&gt;\nzeroipc::Queue&lt;int&gt; q(mem, \"tasks\", 100);\n</code></pre> <pre><code>from zeroipc import Queue\nq = Queue(mem, \"tasks\", dtype=np.int32, capacity=100)\n</code></pre>"},{"location":"api/#type-mapping","title":"Type Mapping","text":"<p>Cross-language type compatibility:</p> C++ Python C Size <code>int8_t</code> <code>np.int8</code> <code>int8_t</code> 1 byte <code>int16_t</code> <code>np.int16</code> <code>int16_t</code> 2 bytes <code>int32_t</code> <code>np.int32</code> <code>int32_t</code> 4 bytes <code>int64_t</code> <code>np.int64</code> <code>int64_t</code> 8 bytes <code>float</code> <code>np.float32</code> <code>float</code> 4 bytes <code>double</code> <code>np.float64</code> <code>double</code> 8 bytes"},{"location":"api/#naming-conventions","title":"Naming Conventions","text":""},{"location":"api/#c","title":"C++","text":"<ul> <li>Classes: PascalCase (<code>Memory</code>, <code>Array&lt;T&gt;</code>)</li> <li>Methods: camelCase (<code>enqueue()</code>, <code>dequeue()</code>)</li> <li>Namespaces: <code>zeroipc::</code></li> </ul>"},{"location":"api/#python","title":"Python","text":"<ul> <li>Classes: PascalCase (<code>Memory</code>, <code>Array</code>)</li> <li>Methods: snake_case (<code>enqueue()</code>, <code>dequeue()</code>)</li> <li>Module: <code>zeroipc</code></li> </ul>"},{"location":"api/#c_1","title":"C","text":"<ul> <li>Functions: <code>zipc_*</code> prefix (<code>zipc_memory_create()</code>)</li> <li>Types: <code>zipc_*_t</code> suffix (<code>zipc_memory_t</code>)</li> <li>Struct tags: snake_case</li> </ul>"},{"location":"api/#error-handling","title":"Error Handling","text":""},{"location":"api/#c_2","title":"C++","text":"<pre><code>// Returns std::optional&lt;T&gt; for operations that may fail\nauto value = queue.dequeue();\nif (value) {\n    process(*value);\n} else {\n    // Queue was empty\n}\n</code></pre>"},{"location":"api/#python_1","title":"Python","text":"<pre><code># Returns None for operations that may fail\nvalue = queue.dequeue()\nif value is not None:\n    process(value)\nelse:\n    # Queue was empty\n</code></pre>"},{"location":"api/#c_3","title":"C","text":"<pre><code>// Returns status codes\nint result = zipc_queue_dequeue(queue, &amp;value);\nif (result == ZIPC_SUCCESS) {\n    process(value);\n} else {\n    // Queue was empty\n}\n</code></pre>"},{"location":"api/#next-steps","title":"Next Steps","text":"<ul> <li>Browse the C++ API</li> <li>Browse the Python API</li> <li>Browse the C API</li> <li>See Examples for usage patterns</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>Deep dive into ZeroIPC's internal design and implementation.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>ZeroIPC is built on three fundamental pillars:</p> <ol> <li>Binary Format Specification - Language-agnostic memory layout</li> <li>Lock-Free Algorithms - High-performance concurrent operations</li> <li>Minimal Metadata - Maximum flexibility through duck typing</li> </ol>"},{"location":"architecture/#architecture-documents","title":"Architecture Documents","text":""},{"location":"architecture/#binary-format","title":"Binary Format","text":"<p>Complete specification of the on-disk/in-memory format:</p> <ul> <li>Table header structure</li> <li>Table entry layout</li> <li>Data structure formats</li> <li>Alignment requirements</li> <li>Version compatibility</li> </ul> <p>All language implementations must follow this specification exactly.</p>"},{"location":"architecture/#design-principles","title":"Design Principles","text":"<p>Core philosophy and trade-offs:</p> <ul> <li>Language Equality - No primary language</li> <li>Minimal Overhead - Only essential metadata</li> <li>User Responsibility - Trust users for type consistency</li> <li>Zero Dependencies - Each implementation stands alone</li> <li>Binary Compatibility - Strict format adherence</li> </ul>"},{"location":"architecture/#lock-free-implementation","title":"Lock-Free Implementation","text":"<p>How concurrent structures work without locks:</p> <ul> <li>Compare-And-Swap (CAS) operations</li> <li>Memory ordering and fences</li> <li>ABA problem prevention</li> <li>Progress guarantees</li> <li>Performance characteristics</li> </ul>"},{"location":"architecture/#memory-layout","title":"Memory Layout","text":"<p>How data is organized in shared memory:</p> <ul> <li>Segment structure</li> <li>Table organization</li> <li>Data structure placement</li> <li>Alignment and padding</li> <li>Growth and allocation</li> </ul>"},{"location":"architecture/#testing-strategy","title":"Testing Strategy","text":"<p>Comprehensive testing approach:</p> <ul> <li>Unit tests per structure</li> <li>Integration tests (cross-language)</li> <li>Stress tests (high concurrency)</li> <li>Property-based testing</li> <li>Test categorization (FAST/MEDIUM/SLOW/STRESS)</li> </ul>"},{"location":"architecture/#key-concepts","title":"Key Concepts","text":""},{"location":"architecture/#shared-memory-model","title":"Shared Memory Model","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        POSIX Shared Memory          \u2502\n\u2502         (/dev/shm/name)             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          Table Header               \u2502\n\u2502  (magic, version, count, offset)    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         Table Entries               \u2502\n\u2502 (name, offset, size) \u00d7 max_entries  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       Data Structures               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502      Array Header            \u2502  \u2502\n\u2502  \u2502      Array Data              \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502      Queue Header            \u2502  \u2502\n\u2502  \u2502      Queue Buffer            \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502  ...                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#lock-free-queue-algorithm","title":"Lock-Free Queue Algorithm","text":"<p>High-level overview of lock-free enqueue:</p> <pre><code>1. Load current tail index (relaxed)\n2. Calculate next tail index\n3. Check if queue full\n4. CAS tail to next index\n   - If successful: write data, fence\n   - If failed: retry from step 1\n</code></pre> <p>This avoids locks while maintaining consistency across threads/processes.</p>"},{"location":"architecture/#type-system","title":"Type System","text":"<p>ZeroIPC uses structural typing (duck typing):</p> <ul> <li>No type metadata stored in shared memory</li> <li>Users specify types at access time</li> <li>Binary layout must match between languages</li> <li>Size checking only validation performed</li> </ul> <p>Example: <pre><code>// C++ creates with int32_t\nArray&lt;int32_t&gt; arr(mem, \"data\", 100);\n</code></pre></p> <pre><code># Python must match with np.int32\narr = Array(mem, \"data\", dtype=np.int32)\n</code></pre> <p>If types mismatch in size, behavior is undefined!</p>"},{"location":"architecture/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/#operation-complexity","title":"Operation Complexity","text":"Operation Time Complexity Lock-Free Array index O(1) N/A Array atomic op O(1) Yes Queue enqueue O(1) amortized Yes Queue dequeue O(1) amortized Yes Stack push O(1) Yes Stack pop O(1) Yes Map insert O(1) average Yes Map lookup O(1) average Yes"},{"location":"architecture/#memory-overhead","title":"Memory Overhead","text":"Structure Metadata Size Notes Memory 16 bytes + table Table size = 16 + 40*max_entries Array 8 bytes Just capacity Queue 24 bytes Head, tail, capacity Stack 16 bytes Top, capacity Map Variable Bucket count dependent"},{"location":"architecture/#design-decisions","title":"Design Decisions","text":""},{"location":"architecture/#why-no-type-metadata","title":"Why No Type Metadata?","text":"<p>Advantages: - Maximum flexibility across languages - Smaller memory footprint - Simpler implementation - Faster allocation</p> <p>Trade-offs: - Users must track types externally - No runtime type checking - Potential for type mismatches</p>"},{"location":"architecture/#why-no-deallocation","title":"Why No Deallocation?","text":"<p>Advantages: - Simpler allocation (bump pointer) - No fragmentation - No free list management - Faster creation</p> <p>Trade-offs: - Cannot reclaim individual structures - Must delete entire segment to free memory - Poor fit for dynamic workloads</p>"},{"location":"architecture/#why-lock-free","title":"Why Lock-Free?","text":"<p>Advantages: - No deadlocks - No priority inversion - Scalable across cores - Progress guarantees</p> <p>Trade-offs: - More complex implementation - May retry on high contention - Larger instruction footprint - Requires atomic operations</p>"},{"location":"architecture/#implementation-notes","title":"Implementation Notes","text":""},{"location":"architecture/#c-implementation","title":"C++ Implementation","text":"<ul> <li>Header-only for zero link-time overhead</li> <li>Templates for compile-time type safety</li> <li>Concepts for type constraints</li> <li>RAII for automatic cleanup</li> </ul>"},{"location":"architecture/#python-implementation","title":"Python Implementation","text":"<ul> <li>Pure Python for portability</li> <li>NumPy for performance</li> <li>mmap for direct access</li> <li>ctypes for atomics</li> </ul>"},{"location":"architecture/#c-implementation_1","title":"C Implementation","text":"<ul> <li>Static library for linking</li> <li>Explicit management (no RAII)</li> <li>Minimal dependencies (just POSIX)</li> <li>Maximum portability</li> </ul>"},{"location":"architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Binary Format - Detailed format specification</li> <li>Lock-Free Implementation - Algorithm deep dive</li> <li>Testing Strategy - How we ensure correctness</li> <li>Design Principles - Philosophy and trade-offs</li> </ul>"},{"location":"best-practices/","title":"Best Practices","text":"<p>Learn how to use ZeroIPC effectively and avoid common pitfalls.</p>"},{"location":"best-practices/#quick-guidelines","title":"Quick Guidelines","text":""},{"location":"best-practices/#dos","title":"Do's","text":"<ul> <li>Use fixed-width integer types (<code>int32_t</code> not <code>int</code>)</li> <li>Document shared data structures and types</li> <li>Choose appropriate table sizes for your workload</li> <li>Clean up test shared memory segments</li> <li>Use meaningful names for segments and structures</li> <li>Handle errors appropriately (check return values)</li> <li>Test cross-language compatibility early</li> </ul>"},{"location":"best-practices/#donts","title":"Don'ts","text":"<ul> <li>Don't use platform-dependent types (<code>long</code>, <code>size_t</code>) for shared data</li> <li>Don't assume structure exists without checking</li> <li>Don't fill tables to 100% capacity</li> <li>Don't create circular dependencies</li> <li>Don't ignore type size mismatches</li> <li>Don't leak shared memory segments</li> <li>Don't rely on undefined behavior</li> </ul>"},{"location":"best-practices/#best-practices-by-topic","title":"Best Practices by Topic","text":""},{"location":"best-practices/#performance-tips","title":"Performance Tips","text":"<p>Optimize for maximum throughput and minimum latency:</p> <ul> <li>Choose appropriate data structures</li> <li>Minimize contention</li> <li>Use proper memory ordering</li> <li>Batch operations when possible</li> <li>Pre-allocate structures</li> <li>Monitor utilization</li> </ul>"},{"location":"best-practices/#common-pitfalls","title":"Common Pitfalls","text":"<p>Avoid these frequent mistakes:</p> <ul> <li>Type size mismatches</li> <li>Table overflow</li> <li>Memory leaks</li> <li>Race conditions</li> <li>Deadlocks (with user locks)</li> <li>Name collisions</li> <li>Incorrect cleanup</li> </ul>"},{"location":"best-practices/#type-safety","title":"Type Safety","text":"<p>Maintain consistency across languages:</p> <ul> <li>Use shared type definitions</li> <li>Document type mappings</li> <li>Create type verification utilities</li> <li>Test cross-language compatibility</li> <li>Use fixed-width types</li> <li>Align structure members</li> </ul>"},{"location":"best-practices/#error-handling","title":"Error Handling","text":"<p>Handle errors gracefully:</p> <ul> <li>Check return values</li> <li>Handle <code>std::optional</code> / <code>None</code> returns</li> <li>Validate inputs</li> <li>Provide clear error messages</li> <li>Fail fast on corruption</li> <li>Log errors appropriately</li> </ul>"},{"location":"best-practices/#testing-applications","title":"Testing Applications","text":"<p>Test shared memory applications:</p> <ul> <li>Unit test each component</li> <li>Integration test cross-language</li> <li>Stress test under high concurrency</li> <li>Test error paths</li> <li>Verify cleanup</li> <li>Use unique names in tests</li> </ul>"},{"location":"best-practices/#code-style-guide","title":"Code Style Guide","text":""},{"location":"best-practices/#c-style","title":"C++ Style","text":"<pre><code>// Good: Clear, safe, idiomatic\n#include &lt;zeroipc/memory.h&gt;\n#include &lt;zeroipc/array.h&gt;\n\nvoid process_data() {\n    // Use RAII for automatic cleanup\n    zeroipc::Memory mem(\"/sensors\", 1024*1024);\n\n    // Use fixed-width types\n    zeroipc::Array&lt;int32_t&gt; data(mem, \"readings\", 1000);\n\n    // Check optional returns\n    if (auto value = data.at(0)) {\n        std::cout &lt;&lt; \"First value: \" &lt;&lt; *value &lt;&lt; \"\\n\";\n    }\n\n    // Clear names\n    constexpr size_t SENSOR_COUNT = 100;\n}\n\n// Bad: Unclear, unsafe, non-idiomatic\nvoid bad_example() {\n    auto m = new zeroipc::Memory(\"/x\", 999999);  // Manual alloc\n    zeroipc::Array&lt;long&gt; d(m, \"d\", 9999);  // Platform-dependent type\n    cout &lt;&lt; d[0];  // No error checking\n    // Memory leak - never deleted\n}\n</code></pre>"},{"location":"best-practices/#python-style","title":"Python Style","text":"<pre><code># Good: Clear, type-safe, Pythonic\nfrom zeroipc import Memory, Array\nimport numpy as np\nfrom typing import Optional\n\ndef process_data() -&gt; None:\n    \"\"\"Process sensor data from shared memory.\"\"\"\n    # Use context managers (when available)\n    mem = Memory(\"/sensors\")\n\n    # Explicit dtype\n    data = Array(mem, \"readings\", dtype=np.int32)\n\n    # Check None\n    value = data[0]\n    if value is not None:\n        print(f\"First value: {value}\")\n\n    # Clear constants\n    SENSOR_COUNT = 100\n\n# Bad: Unclear, fragile\ndef bad_example():\n    m = Memory(\"/x\")\n    d = Array(m, \"d\")  # Missing dtype!\n    print(d[0])  # May crash\n</code></pre>"},{"location":"best-practices/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"best-practices/#pattern-1-publisher-subscriber","title":"Pattern 1: Publisher-Subscriber","text":"<p>One publisher, multiple subscribers:</p> <pre><code>// Publisher\nzeroipc::Memory mem(\"/events\", 10*1024*1024);\nzeroipc::Stream&lt;Event&gt; events(mem, \"stream\", 1000);\n\nwhile (running) {\n    Event e = generate_event();\n    events.emit(e);\n}\n\n// Subscribers (many processes)\nzeroipc::Memory mem(\"/events\");\nzeroipc::Stream&lt;Event&gt; events(mem, \"stream\");\n\nevents.subscribe([](Event&amp; e) {\n    process_event(e);\n});\n</code></pre>"},{"location":"best-practices/#pattern-2-request-response","title":"Pattern 2: Request-Response","text":"<p>Use futures for RPC-like patterns:</p> <pre><code>// Server\nzeroipc::Memory mem(\"/rpc\", 10*1024*1024);\nzeroipc::Future&lt;Response&gt; result(mem, \"response\");\n\nRequest req = get_request();\nResponse res = process(req);\nresult.set_value(res);\n\n// Client\nzeroipc::Memory mem(\"/rpc\");\nzeroipc::Future&lt;Response&gt; result(mem, \"response\", true);  // Read-only\n\nif (auto res = result.get_for(std::chrono::seconds(5))) {\n    use_response(*res);\n} else {\n    handle_timeout();\n}\n</code></pre>"},{"location":"best-practices/#pattern-3-pipeline","title":"Pattern 3: Pipeline","text":"<p>Chain processing stages:</p> <pre><code>// Stage 1: Raw data\nzeroipc::Stream&lt;RawData&gt; raw(mem, \"raw\", 1000);\n\n// Stage 2: Validated data\nauto validated = raw.filter(mem, \"validated\",\n    [](RawData&amp; d) { return d.is_valid(); });\n\n// Stage 3: Transformed data\nauto transformed = validated.map(mem, \"transformed\",\n    [](RawData&amp; d) { return transform(d); });\n\n// Stage 4: Final processing\ntransformed.subscribe([](TransformedData&amp; d) {\n    final_process(d);\n});\n</code></pre>"},{"location":"best-practices/#documentation-templates","title":"Documentation Templates","text":""},{"location":"best-practices/#shared-type-definition","title":"Shared Type Definition","text":"<p>Create shared headers:</p> <p>shared_types.hpp (C++): <pre><code>#pragma once\n#include &lt;cstdint&gt;\n\nnamespace shared {\n\nstruct SensorReading {\n    float temperature;\n    float humidity;\n    uint64_t timestamp;\n};\n\nconstexpr size_t MAX_SENSORS = 100;\nconstexpr char SEGMENT_NAME[] = \"/sensors\";\n\n}  // namespace shared\n</code></pre></p> <p>shared_types.py (Python): <pre><code>\"\"\"Shared type definitions for sensor application.\"\"\"\nimport numpy as np\n\n# Match C++ struct SensorReading\nSENSOR_READING_DTYPE = np.dtype([\n    ('temperature', np.float32),\n    ('humidity', np.float32),\n    ('timestamp', np.uint64),\n])\n\nMAX_SENSORS = 100\nSEGMENT_NAME = \"/sensors\"\n</code></pre></p>"},{"location":"best-practices/#checklist","title":"Checklist","text":"<p>Use this checklist for new projects:</p> <ul> <li> Defined shared types in both languages</li> <li> Chosen appropriate table size</li> <li> Documented segment names</li> <li> Added error handling</li> <li> Created cleanup procedures</li> <li> Written unit tests</li> <li> Written integration tests</li> <li> Tested cross-language compatibility</li> <li> Added monitoring/debugging support</li> <li> Documented deployment requirements</li> </ul>"},{"location":"best-practices/#next-steps","title":"Next Steps","text":"<p>Dive deeper into specific topics:</p> <ul> <li>Performance Tips</li> <li>Common Pitfalls</li> <li>Type Safety</li> <li>Error Handling</li> <li>Testing Applications</li> </ul>"},{"location":"cli/","title":"CLI Tool Guide","text":"<p>The ZeroIPC CLI tool provides a powerful interface for inspecting, debugging, and monitoring shared memory segments. With support for all 16 data structures and a virtual filesystem interface, it's an essential tool for development and operations.</p>"},{"location":"cli/#overview","title":"Overview","text":"<p>The <code>zeroipc</code> tool offers two modes of operation:</p> <ol> <li>Command-line mode: Execute single commands and exit</li> <li>REPL mode: Interactive exploration with virtual filesystem navigation</li> </ol>"},{"location":"cli/#key-features","title":"Key Features","text":"<ul> <li>Structure Inspection: View contents of all 16 data structure types</li> <li>Virtual Filesystem: Navigate shared memory like a directory structure  </li> <li>Real-time Monitoring: Watch structures update in real-time</li> <li>Cross-Language Debugging: Inspect structures created by any language</li> <li>JSON Output: Machine-readable output for scripting</li> </ul>"},{"location":"cli/#quick-examples","title":"Quick Examples","text":""},{"location":"cli/#list-all-shared-memory","title":"List All Shared Memory","text":"<pre><code>zeroipc list\n</code></pre> <p>Output: <pre><code>/sensor_data    10.0 MB    5 structures\n/analytics      50.0 MB   12 structures  \n/messages        1.0 MB    2 structures\n</code></pre></p>"},{"location":"cli/#show-segment-details","title":"Show Segment Details","text":"<pre><code>zeroipc show /sensor_data\n</code></pre>"},{"location":"cli/#inspect-an-array","title":"Inspect an Array","text":"<pre><code>zeroipc array /sensor_data temperatures\n</code></pre>"},{"location":"cli/#interactive-repl-mode","title":"Interactive REPL Mode","text":"<pre><code>zeroipc -r\n</code></pre>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI tool is built with the C++ library:</p> <pre><code>cd cpp\ncmake -B build .\ncmake --build build\n\n# The tool is at build/tools/zeroipc\n./build/tools/zeroipc --help\n\n# Optional: install system-wide\nsudo cp build/tools/zeroipc /usr/local/bin/\n</code></pre>"},{"location":"cli/#documentation-sections","title":"Documentation Sections","text":"<ul> <li>Basic Commands - Core CLI commands and usage</li> <li>Virtual Filesystem - Navigate shared memory interactively</li> <li>Structure Inspection - Detailed structure viewing</li> <li>Monitoring and Debugging - Real-time monitoring and troubleshooting</li> </ul>"},{"location":"cli/#supported-structures","title":"Supported Structures","text":"<p>The CLI tool supports all ZeroIPC data structures:</p>"},{"location":"cli/#traditional-structures","title":"Traditional Structures","text":"<ul> <li>Array - View elements with indices</li> <li>Queue - Show queue state and contents</li> <li>Stack - Display stack contents</li> <li>Ring - Ring buffer inspection</li> <li>Map - Key-value pair listing</li> <li>Set - Set member listing</li> <li>Pool - Pool allocation state</li> <li>Table - Metadata table inspection</li> </ul>"},{"location":"cli/#synchronization-primitives","title":"Synchronization Primitives","text":"<ul> <li>Semaphore - Current count and waiters</li> <li>Barrier - Participants and state</li> <li>Latch - Countdown state</li> </ul>"},{"location":"cli/#codata-structures","title":"Codata Structures","text":"<ul> <li>Future - Value and state</li> <li>Lazy - Computed state</li> <li>Stream - Stream contents</li> <li>Channel - Channel state and buffer</li> </ul>"},{"location":"cli/#common-workflows","title":"Common Workflows","text":""},{"location":"cli/#development-workflow","title":"Development Workflow","text":"<pre><code># 1. Run your application\n./myapp &amp;\n\n# 2. Inspect what it created\nzeroipc list\nzeroipc show /myapp_data\n\n# 3. Check specific structures\nzeroipc array /myapp_data sensor_readings\nzeroipc queue /myapp_data task_queue\n\n# 4. Monitor in real-time\nzeroipc monitor /myapp_data sensor_readings\n</code></pre>"},{"location":"cli/#debugging-workflow","title":"Debugging Workflow","text":"<pre><code># 1. Interactive exploration\nzeroipc -r\n\n# 2. Navigate to segment\nzeroipc&gt; cd /myapp_data\n\n# 3. List structures\n/myapp_data&gt; ls\n\n# 4. Inspect suspicious structure\n/myapp_data&gt; cat problematic_array\n\n# 5. Check raw memory if needed\n/myapp_data&gt; dump --offset 1024 --size 256\n</code></pre>"},{"location":"cli/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Commands - Learn all available commands</li> <li>Virtual Filesystem - Master interactive navigation</li> <li>Structure Inspection - Deep dive into structure viewing</li> <li>Monitoring - Real-time monitoring techniques</li> </ul>"},{"location":"cli/basic-commands/","title":"CLI Basic Commands Documentation","text":"<p>This page covers all basic commands available in the <code>zeroipc</code> tool.</p>"},{"location":"cli/basic-commands/#global-options","title":"Global Options","text":"<p>Available for all commands:</p> <pre><code>zeroipc [global options] &lt;command&gt; [arguments]\n</code></pre> <p>Global Options: - <code>-h, --help</code> - Show help message - <code>-v, --version</code> - Show version information - <code>--json</code> - Output in JSON format (where applicable) - <code>-r, --repl</code> - Enter REPL mode</p>"},{"location":"cli/basic-commands/#command-reference","title":"Command Reference","text":""},{"location":"cli/basic-commands/#list","title":"list","text":"<p>List all ZeroIPC shared memory segments on the system.</p> <p>Syntax: <pre><code>zeroipc list [options]\n</code></pre></p> <p>Options: - <code>--all</code> - Include non-ZeroIPC segments - <code>--details</code> - Show detailed information - <code>--json</code> - JSON output</p> <p>Examples:</p> <p>Basic listing: <pre><code>$ zeroipc list\n/sensor_data    10.0 MB    5 structures\n/analytics      50.0 MB   12 structures  \n/messages        1.0 MB    2 structures\n</code></pre></p> <p>Detailed listing: <pre><code>$ zeroipc list --details\nNAME            SIZE        CREATED              PROCESSES  STRUCTURES\n/sensor_data    10485760    2024-01-15 14:23:01  3          5\n/analytics      52428800    2024-01-15 14:20:15  2          12\n/messages       1048576     2024-01-15 14:25:44  4          2\n\nTotal: 3 segments, 63.0 MB\n</code></pre></p> <p>JSON output: <pre><code>$ zeroipc list --json\n{\n  \"segments\": [\n    {\n      \"name\": \"/sensor_data\",\n      \"size\": 10485760,\n      \"structures\": 5,\n      \"created\": \"2024-01-15T14:23:01Z\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"cli/basic-commands/#show","title":"show","text":"<p>Display detailed information about a specific shared memory segment.</p> <p>Syntax: <pre><code>zeroipc show &lt;segment_name&gt; [options]\n</code></pre></p> <p>Options: - <code>--structures</code> - List all structures in the segment - <code>--metadata</code> - Show raw table metadata - <code>--json</code> - JSON output</p> <p>Examples:</p> <p>Basic info: <pre><code>$ zeroipc show /sensor_data\nSegment: /sensor_data\nSize: 10.0 MB (10485760 bytes)\nTable entries: 5/64 used\nMemory used: 45632 bytes\n</code></pre></p> <p>With structures: <pre><code>$ zeroipc show /sensor_data --structures\nSegment: /sensor_data\n\nTable Information:\n  Version: 1.0\n  Max entries: 64\n  Used entries: 5\n  Next offset: 45632\n\nStructures:\n  NAME              TYPE      OFFSET    SIZE      CAPACITY\n  temperatures      array     1024      4000      1000\n  pressure          array     5024      8000      1000\n  alerts            queue     13024     8192      256\n  config            lazy      21216     8032      -\n  temp_stream       stream    29248     16384     1000\n</code></pre></p>"},{"location":"cli/basic-commands/#dump","title":"dump","text":"<p>Dump raw memory contents in hexadecimal format.</p> <p>Syntax: <pre><code>zeroipc dump &lt;segment_name&gt; [options]\n</code></pre></p> <p>Options: - <code>--offset &lt;n&gt;</code> - Starting offset (default: 0) - <code>--size &lt;n&gt;</code> - Number of bytes (default: 256) - <code>--format &lt;fmt&gt;</code> - Output format: hex, ascii, both (default: both)</p> <p>Example: <pre><code>$ zeroipc dump /sensor_data --offset 0 --size 64\n0000: 5a 49 50 4d 01 00 00 00  05 00 00 00 20 b2 00 00  |ZIPM........ ...|\n0010: 74 65 6d 70 65 72 61 74  75 72 65 73 00 00 00 00  |temperatures....|\n0020: 00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n0030: 00 04 00 00 a0 0f 00 00  70 72 65 73 73 75 72 65  |........pressure|\n</code></pre></p>"},{"location":"cli/basic-commands/#create","title":"create","text":"<p>Create structures directly from the CLI (for testing).</p> <p>Syntax: <pre><code>zeroipc create &lt;type&gt; &lt;segment&gt; &lt;name&gt; [options]\n</code></pre></p> <p>Types: array, queue, stack, ring, map, set, pool, semaphore, barrier, latch</p> <p>Examples:</p> <p>Create an array: <pre><code>zeroipc create array /test test_array --capacity 100 --type int32\n</code></pre></p> <p>Create a queue: <pre><code>zeroipc create queue /test task_queue --capacity 50 --type uint64\n</code></pre></p> <p>Create a semaphore: <pre><code>zeroipc create semaphore /test mutex --initial 1\n</code></pre></p>"},{"location":"cli/basic-commands/#delete","title":"delete","text":"<p>Delete a shared memory segment.</p> <p>Syntax: <pre><code>zeroipc delete &lt;segment_name&gt; [options]\n</code></pre></p> <p>Options: - <code>-f, --force</code> - Don't ask for confirmation - <code>--keep-structures</code> - Only unlink, don't remove from /dev/shm</p> <p>Example: <pre><code>$ zeroipc delete /test_data\nWARNING: This will permanently delete segment /test_data\nContinue? (y/N): y\nDeleted /test_data\n</code></pre></p>"},{"location":"cli/basic-commands/#structure-specific-commands","title":"Structure-Specific Commands","text":""},{"location":"cli/basic-commands/#array","title":"array","text":"<p>Inspect array contents.</p> <p>Syntax: <pre><code>zeroipc array &lt;segment&gt; &lt;name&gt; [options]\n</code></pre></p> <p>Options: - <code>--range &lt;start&gt;:&lt;end&gt;</code> - Show specific range - <code>--limit &lt;n&gt;</code> - Limit output to n elements - <code>--stats</code> - Show statistics (min, max, mean, etc.)</p> <p>Example: <pre><code>$ zeroipc array /sensor_data temperatures --range 0:10\nArray: temperatures\nType: float (inferred from size)\nCapacity: 1000\nElements shown: [0:10]\n\n[0] = 23.45\n[1] = 24.12\n[2] = 22.89\n[3] = 25.01\n[4] = 23.67\n[5] = 24.55\n[6] = 23.98\n[7] = 24.23\n[8] = 23.12\n[9] = 24.87\n</code></pre></p>"},{"location":"cli/basic-commands/#queue","title":"queue","text":"<p>Inspect queue state and contents.</p> <p>Syntax: <pre><code>zeroipc queue &lt;segment&gt; &lt;name&gt; [options]\n</code></pre></p> <p>Options: - <code>--limit &lt;n&gt;</code> - Limit number of elements shown - <code>--stats</code> - Show queue statistics</p> <p>Example: <pre><code>$ zeroipc queue /messages task_queue\nQueue: task_queue\nCapacity: 100\nHead: 15\nTail: 42\nSize: 27/100 (27% full)\n\nContents (oldest to newest):\n[0] = Task{id: 123, priority: 5, ...}\n[1] = Task{id: 124, priority: 3, ...}\n[2] = Task{id: 125, priority: 7, ...}\n...\n[26] = Task{id: 149, priority: 4, ...}\n</code></pre></p>"},{"location":"cli/basic-commands/#semaphore","title":"semaphore","text":"<p>Inspect semaphore state.</p> <p>Syntax: <pre><code>zeroipc semaphore &lt;segment&gt; &lt;name&gt;\n</code></pre></p> <p>Example: <pre><code>$ zeroipc semaphore /sync mutex\nSemaphore: mutex\nType: Binary semaphore (max_count = 1)\nCurrent count: 0 (locked)\nWaiting processes: 2\n</code></pre></p>"},{"location":"cli/basic-commands/#barrier","title":"barrier","text":"<p>Inspect barrier state.</p> <p>Syntax: <pre><code>zeroipc barrier &lt;segment&gt; &lt;name&gt;\n</code></pre></p> <p>Example: <pre><code>$ zeroipc barrier /sync checkpoint\nBarrier: checkpoint\nParticipants: 4\nArrived: 2/4\nGeneration: 15\nStatus: Waiting for 2 more participants\n</code></pre></p>"},{"location":"cli/basic-commands/#latch","title":"latch","text":"<p>Inspect latch state.</p> <p>Syntax: <pre><code>zeroipc latch &lt;segment&gt; &lt;name&gt;\n</code></pre></p> <p>Example: <pre><code>$ zeroipc latch /sync startup\nLatch: startup\nInitial count: 5\nCurrent count: 2\nStatus: Waiting for 2 more count_down calls\n</code></pre></p>"},{"location":"cli/basic-commands/#stream","title":"stream","text":"<p>Inspect stream contents.</p> <p>Syntax: <pre><code>zeroipc stream &lt;segment&gt; &lt;name&gt; [options]\n</code></pre></p> <p>Options: - <code>--tail &lt;n&gt;</code> - Show last n events - <code>--follow</code> - Follow mode (like <code>tail -f</code>)</p> <p>Example: <pre><code>$ zeroipc stream /events sensor_stream --tail 5\nStream: sensor_stream\nCapacity: 1000\nHead: 523\nTail: 528\nEvents shown (most recent):\n\n[528] = {temp: 23.5, humidity: 45.2, timestamp: 1642345678}\n[527] = {temp: 23.4, humidity: 45.5, timestamp: 1642345677}\n[526] = {temp: 23.6, humidity: 45.0, timestamp: 1642345676}\n[525] = {temp: 23.5, humidity: 45.3, timestamp: 1642345675}\n[524] = {temp: 23.7, humidity: 44.8, timestamp: 1642345674}\n</code></pre></p>"},{"location":"cli/basic-commands/#next-steps","title":"Next Steps","text":"<ul> <li>Virtual Filesystem - Interactive navigation</li> <li>Structure Inspection - Detailed structure viewing</li> <li>Monitoring - Real-time monitoring</li> </ul>"},{"location":"cli/monitoring/","title":"Monitoring and Debugging","text":"<p>Learn how to monitor ZeroIPC shared memory in real-time and debug issues effectively.</p>"},{"location":"cli/monitoring/#real-time-monitoring","title":"Real-Time Monitoring","text":""},{"location":"cli/monitoring/#monitor-command","title":"monitor Command","text":"<p>Watch a structure update in real-time (similar to <code>watch</code> or <code>tail -f</code>).</p> <p>Syntax: <pre><code>zeroipc monitor &lt;segment&gt; &lt;structure&gt; [options]\n</code></pre></p> <p>Options: - <code>--interval &lt;ms&gt;</code> - Update interval in milliseconds (default: 1000) - <code>--limit &lt;n&gt;</code> - Limit displayed elements - <code>--diff</code> - Highlight changes</p> <p>Examples:</p> <p>Monitor array: <pre><code>$ zeroipc monitor /sensor_data temperatures --interval 500\nMonitoring: /sensor_data/temperatures (refreshing every 500ms)\nPress Ctrl+C to stop\n\n[14:35:22] temperatures[0] = 23.45\n[14:35:22] temperatures[1] = 24.12\n...\n[14:35:22.5] temperatures[0] = 23.47  &lt;- changed\n[14:35:22.5] temperatures[1] = 24.12\n...\n</code></pre></p> <p>Monitor queue: <pre><code>$ zeroipc monitor /tasks work_queue\nMonitoring: /tasks/work_queue (refreshing every 1000ms)\n\n[14:35:22] Size: 25/100 (25%)\n           Head: 42, Tail: 67\n[14:35:23] Size: 24/100 (24%)  &lt;- dequeued 1\n           Head: 43, Tail: 67\n[14:35:24] Size: 26/100 (26%)  &lt;- enqueued 2\n           Head: 43, Tail: 69\n</code></pre></p> <p>Monitor stream: <pre><code>$ zeroipc stream /events sensor_stream --follow\nFollowing: /events/sensor_stream\nNew events will appear below (Ctrl+C to stop)\n\n[14:35:22.123] {temp: 23.5, pressure: 1013.2}\n[14:35:22.223] {temp: 23.4, pressure: 1013.1}\n[14:35:22.323] {temp: 23.6, pressure: 1013.3}\n^C\n</code></pre></p>"},{"location":"cli/monitoring/#debugging-workflows","title":"Debugging Workflows","text":""},{"location":"cli/monitoring/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"cli/monitoring/#issue-1-structure-not-found","title":"Issue 1: Structure Not Found","text":"<p>Symptoms: <pre><code>$ zeroipc array /data numbers\nError: Structure 'numbers' not found in /data\n</code></pre></p> <p>Debug steps:</p> <ol> <li> <p>List all structures: <pre><code>$ zeroipc show /data --structures\n# Check if structure exists with different name\n</code></pre></p> </li> <li> <p>Check raw table: <pre><code>$ zeroipc show /data --metadata\n# Verify table entries\n</code></pre></p> </li> <li> <p>Check for corruption: <pre><code>$ zeroipc dump /data --offset 0 --size 64\n# Verify magic number: 5a 49 50 4d ('ZIPM')\n</code></pre></p> </li> </ol>"},{"location":"cli/monitoring/#issue-2-incorrect-data-values","title":"Issue 2: Incorrect Data Values","text":"<p>Symptoms: <pre><code>$ zeroipc array /data numbers\n[0] = -2.14748e+09  # Garbage values\n</code></pre></p> <p>Debug steps:</p> <ol> <li> <p>Check type mismatch: <pre><code># Created as int32 but reading as float32?\n$ zeroipc array /data numbers --hint-type int32\n[0] = 42  # Correct!\n</code></pre></p> </li> <li> <p>Verify element size: <pre><code>$ zeroipc show /data --structures\n# Check size field: size / capacity = element_size\n</code></pre></p> </li> <li> <p>Check alignment: <pre><code>$ zeroipc dump /data --offset &lt;structure_offset&gt;\n# Verify data alignment\n</code></pre></p> </li> </ol>"},{"location":"cli/monitoring/#issue-3-memory-corruption","title":"Issue 3: Memory Corruption","text":"<p>Symptoms: <pre><code>$ zeroipc show /data\nError: Invalid table header (bad magic number)\n</code></pre></p> <p>Debug steps:</p> <ol> <li> <p>Check magic number: <pre><code>$ zeroipc dump /data --offset 0 --size 16\n# Should start with: 5a 49 50 4d\n</code></pre></p> </li> <li> <p>Backup if possible: <pre><code>$ cp /dev/shm/data /tmp/data_backup\n</code></pre></p> </li> <li> <p>Try recovery (future feature): <pre><code>$ zeroipc repair /data\n</code></pre></p> </li> </ol>"},{"location":"cli/monitoring/#issue-4-performance-problems","title":"Issue 4: Performance Problems","text":"<p>Symptoms: - Slow enqueue/dequeue operations - High CPU usage - Excessive contention</p> <p>Debug steps:</p> <ol> <li> <p>Check structure utilization: <pre><code>$ zeroipc queue /tasks work_queue --stats\nLoad factor: 0.95 (95/100)  # Too full!\n</code></pre></p> </li> <li> <p>Monitor contention: <pre><code>$ zeroipc monitor /tasks work_queue --interval 100\n# Watch for thrashing (head/tail changing rapidly without progress)\n</code></pre></p> </li> <li> <p>Check for ABA problems: <pre><code># Look for suspicious patterns in lock-free structures\n$ zeroipc monitor /data lock_free_stack\n# Watch for: head reversing, duplicate values, etc.\n</code></pre></p> </li> </ol>"},{"location":"cli/monitoring/#production-monitoring","title":"Production Monitoring","text":""},{"location":"cli/monitoring/#health-checks","title":"Health Checks","text":"<p>Create monitoring scripts:</p> <p>monitor_queues.sh: <pre><code>#!/bin/bash\n# Alert if queues are too full\n\nfor segment in $(zeroipc list | awk '{print $1}'); do\n    queues=$(zeroipc show \"$segment\" --structures | grep queue | awk '{print $2}')\n    for queue in $queues; do\n        utilization=$(zeroipc queue \"$segment\" \"$queue\" --stats | grep \"Load factor\" | awk '{print $3}')\n        if (( $(echo \"$utilization &gt; 0.90\" | bc -l) )); then\n            echo \"WARNING: $segment/$queue is $utilization full\"\n        fi\n    done\ndone\n</code></pre></p> <p>check_semaphores.sh: <pre><code>#!/bin/bash\n# Detect potential deadlocks\n\nfor segment in $(zeroipc list | awk '{print $1}'); do\n    sems=$(zeroipc show \"$segment\" --structures | grep semaphore | awk '{print $2}')\n    for sem in $sems; do\n        waiting=$(zeroipc semaphore \"$segment\" \"$sem\" | grep \"Waiting:\" | awk '{print $2}')\n        if [ \"$waiting\" -gt 5 ]; then\n            echo \"ALERT: $segment/$sem has $waiting processes waiting\"\n        fi\n    done\ndone\n</code></pre></p>"},{"location":"cli/monitoring/#metrics-collection","title":"Metrics Collection","text":"<p>Collect metrics for graphing:</p> <pre><code>#!/bin/bash\n# Collect time-series metrics\n\nwhile true; do\n    timestamp=$(date +%s)\n\n    # Queue sizes\n    size=$(zeroipc queue /tasks work_queue --json | jq '.size')\n    echo \"queue.size,$timestamp,$size\" &gt;&gt; metrics.csv\n\n    # Array statistics\n    mean=$(zeroipc array /sensors temp --stats --json | jq '.mean')\n    echo \"sensor.temp.mean,$timestamp,$mean\" &gt;&gt; metrics.csv\n\n    sleep 60\ndone\n</code></pre>"},{"location":"cli/monitoring/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"cli/monitoring/#1-diff-mode","title":"1. Diff Mode","text":"<p>Compare snapshots to find changes:</p> <pre><code># Take snapshot 1\nzeroipc array /data values &gt; snapshot1.txt\n\n# Wait for changes...\n\n# Take snapshot 2\nzeroipc array /data values &gt; snapshot2.txt\n\n# Compare\ndiff snapshot1.txt snapshot2.txt\n</code></pre>"},{"location":"cli/monitoring/#2-watch-mode","title":"2. Watch Mode","text":"<p>Monitor specific indices:</p> <pre><code># Watch a specific array element\nwatch -n 1 'zeroipc array /data counter --range 0:1'\n\n# Watch queue size\nwatch -n 1 'zeroipc queue /tasks work --stats | grep \"Size:\"'\n</code></pre>"},{"location":"cli/monitoring/#3-log-correlation","title":"3. Log Correlation","text":"<p>Correlate CLI output with application logs:</p> <pre><code># Terminal 1: Monitor structure\nzeroipc monitor /data critical_value\n\n# Terminal 2: Watch application logs\ntail -f /var/log/myapp.log\n\n# Look for correlations between value changes and log events\n</code></pre>"},{"location":"cli/monitoring/#4-memory-forensics","title":"4. Memory Forensics","text":"<p>Analyze memory dumps:</p> <pre><code># Dump entire segment\nzeroipc dump /data --offset 0 --size 1048576 &gt; memory_dump.hex\n\n# Analyze with hex editor or custom tools\nxxd memory_dump.hex | less\n\n# Search for patterns\ngrep -a \"some_pattern\" memory_dump.hex\n</code></pre>"},{"location":"cli/monitoring/#advanced-topics","title":"Advanced Topics","text":""},{"location":"cli/monitoring/#custom-monitoring-scripts","title":"Custom Monitoring Scripts","text":"<p>Python example for custom monitoring:</p> <pre><code>#!/usr/bin/env python3\nimport subprocess\nimport json\nimport time\n\ndef get_queue_stats(segment, queue_name):\n    \"\"\"Get queue statistics as JSON\"\"\"\n    result = subprocess.run(\n        ['zeroipc', 'queue', segment, queue_name, '--stats', '--json'],\n        capture_output=True, text=True\n    )\n    return json.loads(result.stdout)\n\ndef monitor_queue(segment, queue_name, threshold=0.8):\n    \"\"\"Alert if queue exceeds threshold\"\"\"\n    stats = get_queue_stats(segment, queue_name)\n    utilization = stats['size'] / stats['capacity']\n\n    if utilization &gt; threshold:\n        print(f\"ALERT: {segment}/{queue_name} is {utilization:.1%} full\")\n        # Send to monitoring system\n        send_alert(f\"{segment}/{queue_name}\", utilization)\n\nwhile True:\n    monitor_queue('/tasks', 'work_queue')\n    monitor_queue('/events', 'event_queue')\n    time.sleep(10)\n</code></pre>"},{"location":"cli/monitoring/#integration-with-monitoring-systems","title":"Integration with Monitoring Systems","text":"<p>Prometheus Exporter:</p> <pre><code>from prometheus_client import Gauge, start_http_server\nimport subprocess\nimport json\nimport time\n\n# Define metrics\nqueue_size = Gauge('zeroipc_queue_size', 'Queue size', ['segment', 'queue'])\nqueue_utilization = Gauge('zeroipc_queue_util', 'Queue utilization', ['segment', 'queue'])\n\ndef collect_metrics():\n    # Collect from ZeroIPC\n    segments = get_segments()  # Your implementation\n    for seg in segments:\n        queues = get_queues(seg)\n        for q in queues:\n            stats = get_queue_stats(seg, q)\n            queue_size.labels(segment=seg, queue=q).set(stats['size'])\n            queue_utilization.labels(segment=seg, queue=q).set(stats['size'] / stats['capacity'])\n\nif __name__ == '__main__':\n    start_http_server(8000)\n    while True:\n        collect_metrics()\n        time.sleep(15)\n</code></pre>"},{"location":"cli/monitoring/#troubleshooting-checklist","title":"Troubleshooting Checklist","text":"<p>When debugging issues:</p> <ul> <li> Verify segment exists: <code>zeroipc list</code></li> <li> Check segment integrity: <code>zeroipc show /segment</code></li> <li> Verify structure exists: <code>zeroipc show /segment --structures</code></li> <li> Check structure contents: <code>zeroipc &lt;type&gt; /segment structure</code></li> <li> Verify type consistency across languages</li> <li> Check permissions: <code>ls -l /dev/shm/segment</code></li> <li> Monitor for changes: <code>zeroipc monitor /segment structure</code></li> <li> Check raw memory if needed: <code>zeroipc dump /segment</code></li> <li> Verify no corruption: Check magic number and table</li> <li> Review application logs for errors</li> </ul>"},{"location":"cli/monitoring/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Commands - Learn all commands</li> <li>Virtual Filesystem - Interactive exploration</li> <li>Best Practices - Avoid common issues</li> </ul>"},{"location":"cli/structure-inspection/","title":"Structure Inspection","text":"<p>Learn how to inspect and understand each of the 16 ZeroIPC data structures using the CLI tool.</p>"},{"location":"cli/structure-inspection/#overview","title":"Overview","text":"<p>The <code>zeroipc</code> tool provides specialized commands for each structure type, showing type-specific information and contents.</p>"},{"location":"cli/structure-inspection/#array-inspection","title":"Array Inspection","text":"<p>Arrays are the simplest structure\u2014contiguous storage with direct indexing.</p>"},{"location":"cli/structure-inspection/#command","title":"Command","text":"<pre><code>zeroipc array &lt;segment&gt; &lt;name&gt; [options]\n</code></pre>"},{"location":"cli/structure-inspection/#options","title":"Options","text":"<ul> <li><code>--range &lt;start&gt;:&lt;end&gt;</code> - Show specific range of elements</li> <li><code>--limit &lt;n&gt;</code> - Limit to first n elements</li> <li><code>--stats</code> - Calculate statistics (min, max, mean, stddev)</li> <li><code>--format &lt;fmt&gt;</code> - Output format (default, csv, json)</li> </ul>"},{"location":"cli/structure-inspection/#examples","title":"Examples","text":"<p>Basic inspection: <pre><code>$ zeroipc array /sensor_data temperatures\nArray: temperatures\nCapacity: 1000\nElement size: 4 bytes (likely float32)\nTotal size: 4000 bytes\n\n[0] = 23.45\n[1] = 24.12\n[2] = 22.89\n... (showing first 100)\n</code></pre></p> <p>Specific range: <pre><code>$ zeroipc array /sensor_data temperatures --range 100:110\nElements [100:110]:\n[100] = 25.12\n[101] = 25.45\n... \n[109] = 24.33\n</code></pre></p> <p>With statistics: <pre><code>$ zeroipc array /sensor_data temperatures --stats\nArray: temperatures\nCapacity: 1000\n\nStatistics:\n  Min: 20.15\n  Max: 28.93\n  Mean: 24.56\n  Stddev: 1.87\n  Median: 24.50\n</code></pre></p>"},{"location":"cli/structure-inspection/#queue-inspection","title":"Queue Inspection","text":"<p>Lock-free circular buffer with head and tail pointers.</p>"},{"location":"cli/structure-inspection/#command_1","title":"Command","text":"<pre><code>zeroipc queue &lt;segment&gt; &lt;name&gt; [options]\n</code></pre>"},{"location":"cli/structure-inspection/#options_1","title":"Options","text":"<ul> <li><code>--limit &lt;n&gt;</code> - Limit elements shown</li> <li><code>--stats</code> - Show utilization statistics</li> </ul>"},{"location":"cli/structure-inspection/#example","title":"Example","text":"<pre><code>$ zeroipc queue /messages task_queue\nQueue: task_queue\nType: MPMC Circular Buffer\nCapacity: 100\nHead: 42\nTail: 67\nSize: 25/100 (25% full)\n\nContents (FIFO order, oldest first):\n[0] = {id: 1042, priority: 5, data: \"process_image\"}\n[1] = {id: 1043, priority: 3, data: \"backup_data\"}\n...\n[24] = {id: 1066, priority: 7, data: \"send_email\"}\n\nStatistics:\n  Peak size: 87/100 (87%)\n  Enqueue operations: 15,234\n  Dequeue operations: 15,209\n</code></pre>"},{"location":"cli/structure-inspection/#stack-inspection","title":"Stack Inspection","text":"<p>Lock-free LIFO stack.</p>"},{"location":"cli/structure-inspection/#command_2","title":"Command","text":"<pre><code>zeroipc stack &lt;segment&gt; &lt;name&gt; [options]\n</code></pre>"},{"location":"cli/structure-inspection/#example_1","title":"Example","text":"<pre><code>$ zeroipc stack /undo undo_stack\nStack: undo_stack\nCapacity: 50\nTop: 12\nSize: 12/50\n\nContents (LIFO order, top first):\n[top] = {action: \"delete\", object_id: 523}\n[-1]  = {action: \"modify\", object_id: 521, ...}\n[-2]  = {action: \"create\", object_id: 520, ...}\n...\n</code></pre>"},{"location":"cli/structure-inspection/#map-inspection","title":"Map Inspection","text":"<p>Lock-free hash map with linear probing.</p>"},{"location":"cli/structure-inspection/#command_3","title":"Command","text":"<pre><code>zeroipc map &lt;segment&gt; &lt;name&gt; [options]\n</code></pre>"},{"location":"cli/structure-inspection/#options_2","title":"Options","text":"<ul> <li><code>--limit &lt;n&gt;</code> - Limit entries shown</li> <li><code>--stats</code> - Show hash table statistics</li> </ul>"},{"location":"cli/structure-inspection/#example_2","title":"Example","text":"<pre><code>$ zeroipc map /cache session_cache\nMap: session_cache\nBuckets: 256\nLoad factor: 0.45 (115/256)\nCollision rate: 12.3%\n\nEntries:\n  \"user_123\" =&gt; {login_time: 1642345678, role: \"admin\"}\n  \"user_456\" =&gt; {login_time: 1642345690, role: \"user\"}\n  \"user_789\" =&gt; {login_time: 1642345702, role: \"user\"}\n...\n\nStatistics:\n  Max probe length: 5\n  Avg probe length: 1.3\n  Total lookups: 45,234\n  Cache hit rate: 87.5%\n</code></pre>"},{"location":"cli/structure-inspection/#semaphore-inspection","title":"Semaphore Inspection","text":"<p>Cross-process counting semaphore.</p>"},{"location":"cli/structure-inspection/#command_4","title":"Command","text":"<pre><code>zeroipc semaphore &lt;segment&gt; &lt;name&gt;\n</code></pre>"},{"location":"cli/structure-inspection/#example_3","title":"Example","text":"<p>Binary semaphore (mutex): <pre><code>$ zeroipc semaphore /sync mutex\nSemaphore: mutex\nType: Binary (max_count = 1)\nCurrent count: 0 (LOCKED)\nWaiting: 2 processes\nMax count: 1\n\nState: Currently held, 2 processes waiting\n</code></pre></p> <p>Counting semaphore: <pre><code>$ zeroipc semaphore /sync resource_pool\nSemaphore: resource_pool\nType: Counting (max_count = 10)\nCurrent count: 3 (3 available)\nWaiting: 0 processes\nMax count: 10\n\nState: 7/10 resources in use, 3 available\n</code></pre></p>"},{"location":"cli/structure-inspection/#barrier-inspection","title":"Barrier Inspection","text":"<p>Multi-process synchronization barrier.</p>"},{"location":"cli/structure-inspection/#command_5","title":"Command","text":"<pre><code>zeroipc barrier &lt;segment&gt; &lt;name&gt;\n</code></pre>"},{"location":"cli/structure-inspection/#example_4","title":"Example","text":"<pre><code>$ zeroipc barrier /sync checkpoint\nBarrier: checkpoint\nParticipants: 8\nArrived: 5/8\nGeneration: 42\nWaiting: 3 more processes needed\n\nState: Waiting at generation 42\n  Completed cycles: 42\n  Current cycle progress: 5/8 (62.5%)\n</code></pre>"},{"location":"cli/structure-inspection/#stream-inspection","title":"Stream Inspection","text":"<p>Reactive stream with FRP operators.</p>"},{"location":"cli/structure-inspection/#command_6","title":"Command","text":"<pre><code>zeroipc stream &lt;segment&gt; &lt;name&gt; [options]\n</code></pre>"},{"location":"cli/structure-inspection/#options_3","title":"Options","text":"<ul> <li><code>--tail &lt;n&gt;</code> - Show last n events</li> <li><code>--follow</code> - Follow mode (real-time)</li> <li><code>--stats</code> - Show stream statistics</li> </ul>"},{"location":"cli/structure-inspection/#example_5","title":"Example","text":"<pre><code>$ zeroipc stream /events sensor_stream --tail 5\nStream: sensor_stream\nCapacity: 1000\nHead: 12,523\nTail: 12,528\nBackpressure: None\nSubscribers: 3\n\nRecent events (most recent first):\n[12528] {temp: 23.5, pressure: 1013.2, time: 1642345678}\n[12527] {temp: 23.4, pressure: 1013.1, time: 1642345677}\n[12526] {temp: 23.6, pressure: 1013.3, time: 1642345676}\n[12525] {temp: 23.5, pressure: 1013.2, time: 1642345675}\n[12524] {temp: 23.7, pressure: 1013.4, time: 1642345674}\n\nStatistics:\n  Event rate: 10.5 events/sec\n  Dropped events: 0\n  Subscriber lag: max 2 events\n</code></pre>"},{"location":"cli/structure-inspection/#future-inspection","title":"Future Inspection","text":"<p>Asynchronous computation result.</p>"},{"location":"cli/structure-inspection/#command_7","title":"Command","text":"<pre><code>zeroipc future &lt;segment&gt; &lt;name&gt;\n</code></pre>"},{"location":"cli/structure-inspection/#example_6","title":"Example","text":"<p>Pending future: <pre><code>$ zeroipc future /compute calculation\nFuture: calculation\nState: PENDING\nWaiting: 2 processes\n\nThe future has not been set yet.\n</code></pre></p> <p>Completed future: <pre><code>$ zeroipc future /compute calculation\nFuture: calculation\nState: READY\nValue: 42.7865\nSet at: 2024-01-15 14:35:22\n\nThe future has been fulfilled.\n</code></pre></p>"},{"location":"cli/structure-inspection/#best-practices","title":"Best Practices","text":""},{"location":"cli/structure-inspection/#1-start-with-overview","title":"1. Start with Overview","text":"<p>Always start with the segment overview: <pre><code>zeroipc show /segment_name --structures\n</code></pre></p>"},{"location":"cli/structure-inspection/#2-check-structure-type","title":"2. Check Structure Type","text":"<p>Verify the structure type before detailed inspection: <pre><code>zeroipc show /segment --structures | grep structure_name\n</code></pre></p>"},{"location":"cli/structure-inspection/#3-use-appropriate-range","title":"3. Use Appropriate Range","text":"<p>For large arrays, use ranges to avoid overwhelming output: <pre><code># Good: specific range\nzeroipc array /data big_array --range 0:100\n\n# Bad: dumping millions of elements\nzeroipc array /data huge_array\n</code></pre></p>"},{"location":"cli/structure-inspection/#4-monitor-critical-structures","title":"4. Monitor Critical Structures","text":"<p>Use <code>--stats</code> for production monitoring: <pre><code># Check queue utilization\nzeroipc queue /tasks work_queue --stats\n\n# Check map efficiency\nzeroipc map /cache data_cache --stats\n</code></pre></p>"},{"location":"cli/structure-inspection/#5-use-json-for-automation","title":"5. Use JSON for Automation","text":"<p>Parse output programmatically: <pre><code>zeroipc array /data sensors --json | jq '.values | .[0:10]'\n</code></pre></p>"},{"location":"cli/structure-inspection/#next-steps","title":"Next Steps","text":"<ul> <li>Monitoring and Debugging - Real-time monitoring</li> <li>Basic Commands - All available commands</li> <li>Virtual Filesystem - Interactive exploration</li> </ul>"},{"location":"cli/virtual-filesystem/","title":"Virtual Filesystem","text":"<p>The ZeroIPC CLI tool features a virtual filesystem interface that lets you navigate shared memory segments and structures like a directory tree.</p>"},{"location":"cli/virtual-filesystem/#concept","title":"Concept","text":"<p>Shared memory is presented as a hierarchical filesystem:</p> <pre><code>/                          # Root - all shared memory segments\n\u251c\u2500\u2500 sensor_data/          # Shared memory segment\n\u2502   \u251c\u2500\u2500 temperatures      # Array structure\n\u2502   \u251c\u2500\u2500 pressure          # Array structure\n\u2502   \u2514\u2500\u2500 alerts            # Queue structure\n\u2514\u2500\u2500 analytics/            # Another segment\n    \u251c\u2500\u2500 results           # Array structure\n    \u2514\u2500\u2500 cache             # Map structure\n</code></pre>"},{"location":"cli/virtual-filesystem/#entering-repl-mode","title":"Entering REPL Mode","text":"<p>Start the interactive REPL:</p> <pre><code>zeroipc -r\n</code></pre> <p>You'll see: <pre><code>ZeroIPC Interactive Shell v3.0 - Virtual Filesystem Interface\nType 'help' for available commands, 'quit' to exit\n\nzeroipc&gt;\n</code></pre></p>"},{"location":"cli/virtual-filesystem/#navigation-commands","title":"Navigation Commands","text":""},{"location":"cli/virtual-filesystem/#ls-list-contents","title":"ls - List Contents","text":"<p>List contents at current location.</p> <p>Syntax: <pre><code>ls [path]\n</code></pre></p> <p>At root (/): <pre><code>zeroipc&gt; ls\n\n=== Shared Memory Segments ===\nName                Size\n--------------------------------------------------\n/sensor_data        10.0 MB\n/analytics          50.0 MB\n/messages           1.0 MB\n</code></pre></p> <p>In a segment: <pre><code>/sensor_data&gt; ls\n\n=== Table Entries ===\n#   Name                Type        Offset      Size\n---------------------------------------------------------------------------\n0   temperatures        array       1024        4000\n1   pressure            array       5024        8000\n2   alerts              queue       13024       8192\n</code></pre></p> <p>Specific path: <pre><code>zeroipc&gt; ls /sensor_data\n[shows structures in /sensor_data]\n</code></pre></p>"},{"location":"cli/virtual-filesystem/#cd-change-directory","title":"cd - Change Directory","text":"<p>Navigate to different locations.</p> <p>Syntax: <pre><code>cd &lt;path&gt;\n</code></pre></p> <p>Examples:</p> <p>Absolute path: <pre><code>zeroipc&gt; cd /sensor_data\n/sensor_data&gt;\n</code></pre></p> <p>Relative path: <pre><code>/sensor_data&gt; cd ../analytics\n/analytics&gt;\n</code></pre></p> <p>Parent directory: <pre><code>/analytics&gt; cd ..\nzeroipc&gt;\n</code></pre></p> <p>Root: <pre><code>/analytics&gt; cd /\nzeroipc&gt;\n</code></pre></p>"},{"location":"cli/virtual-filesystem/#pwd-print-working-directory","title":"pwd - Print Working Directory","text":"<p>Show current location.</p> <p>Syntax: <pre><code>pwd\n</code></pre></p> <p>Example: <pre><code>/sensor_data&gt; pwd\n/sensor_data\n</code></pre></p>"},{"location":"cli/virtual-filesystem/#cat-display-contents","title":"cat - Display Contents","text":"<p>Show structure contents (like Unix <code>cat</code>).</p> <p>Syntax: <pre><code>cat &lt;structure_name&gt;\ncat &lt;structure_name&gt;[range]\n</code></pre></p> <p>Examples:</p> <p>Show entire array: <pre><code>/sensor_data&gt; cat temperatures\nArray: temperatures\nCapacity: 1000\n[0] = 23.45\n[1] = 24.12\n...\n</code></pre></p> <p>Show specific range: <pre><code>/sensor_data&gt; cat temperatures[0:10]\nArray: temperatures\nElements [0:10]:\n[0] = 23.45\n[1] = 24.12\n...\n[9] = 24.87\n</code></pre></p> <p>Show queue: <pre><code>/messages&gt; cat task_queue\nQueue: task_queue\nSize: 27/100\nHead: 15\nTail: 42\n\nContents:\n[0] = Task{...}\n[1] = Task{...}\n...\n</code></pre></p>"},{"location":"cli/virtual-filesystem/#example-session","title":"Example Session","text":"<p>Here's a complete interactive session:</p> <pre><code>$ zeroipc -r\nZeroIPC Interactive Shell v3.0\nType 'help' for available commands, 'quit' to exit\n\nzeroipc&gt; pwd\n/\n\nzeroipc&gt; ls\n=== Shared Memory Segments ===\n/sensor_data        10.0 MB\n/analytics          50.0 MB\n\nzeroipc&gt; cd /sensor_data\n/sensor_data&gt; ls\n=== Table Entries ===\n#   Name              Type      Offset    Size\n0   temperatures      array     1024      4000\n1   pressure          array     5024      8000\n2   alerts            queue     13024     8192\n\n/sensor_data&gt; cat temperatures[0:5]\nArray: temperatures\nElements [0:5]:\n[0] = 23.45\n[1] = 24.12\n[2] = 22.89\n[3] = 25.01\n[4] = 23.67\n\n/sensor_data&gt; cd /analytics\n/analytics&gt; ls\n=== Table Entries ===\n#   Name          Type      Offset    Size\n0   results       array     1024      80000\n1   cache         map       81024     65536\n\n/analytics&gt; cd /\nzeroipc&gt; quit\nGoodbye!\n</code></pre>"},{"location":"cli/virtual-filesystem/#advanced-features","title":"Advanced Features","text":""},{"location":"cli/virtual-filesystem/#tab-completion","title":"Tab Completion","text":"<p>The REPL supports tab completion (future enhancement):</p> <pre><code>zeroipc&gt; cd /sen&lt;TAB&gt;\n# Completes to: cd /sensor_data\n\n/sensor_data&gt; cat tem&lt;TAB&gt;\n# Completes to: cat temperatures\n</code></pre>"},{"location":"cli/virtual-filesystem/#command-history","title":"Command History","text":"<p>Use arrow keys to navigate command history:</p> <ul> <li>Up arrow: Previous command</li> <li>Down arrow: Next command</li> <li>Ctrl+R: Reverse search</li> </ul>"},{"location":"cli/virtual-filesystem/#shortcuts","title":"Shortcuts","text":"<p>Convenient shortcuts (future enhancement):</p> <pre><code># .. changes to parent directory\n/sensor_data&gt; ..\nzeroipc&gt;\n\n# ~ changes to root\n/sensor_data&gt; ~\nzeroipc&gt;\n\n# ll shows detailed listing\n/sensor_data&gt; ll\n# Same as: ls -l\n</code></pre>"},{"location":"cli/virtual-filesystem/#prompt-customization","title":"Prompt Customization","text":"<p>The prompt shows your current location:</p> <pre><code>zeroipc&gt;           # At root\n/sensor_data&gt;      # In a segment\n</code></pre> <p>Colors (when terminal supports it): - Green: Root - Blue: Segment name - Yellow: Structure name (future)</p>"},{"location":"cli/virtual-filesystem/#repl-commands","title":"REPL Commands","text":""},{"location":"cli/virtual-filesystem/#help","title":"help","text":"<p>Show available commands.</p> <pre><code>zeroipc&gt; help\n\nNavigation Commands:\n  ls [path]              List contents\n  cd &lt;path&gt;              Change directory\n  pwd                    Print working directory\n  cat &lt;name&gt;[range]      Display contents\n\nSegment Commands:\n  show                   Show current segment info\n  create &lt;type&gt; &lt;name&gt;   Create new structure\n\nGeneral Commands:\n  help                   Show this message\n  quit, exit             Exit REPL\n  clear                  Clear screen\n</code></pre>"},{"location":"cli/virtual-filesystem/#show","title":"show","text":"<p>Show detailed information about current segment.</p> <pre><code>/sensor_data&gt; show\nSegment: /sensor_data\nSize: 10.0 MB\nTable: 3/64 entries used\nMemory: 45632 bytes allocated\n</code></pre>"},{"location":"cli/virtual-filesystem/#create","title":"create","text":"<p>Create a new structure in current segment.</p> <pre><code>/sensor_data&gt; create array test_data --capacity 100 --type float\nCreated array 'test_data' with capacity 100\n\n/sensor_data&gt; ls\n# test_data now appears in listing\n</code></pre>"},{"location":"cli/virtual-filesystem/#clear","title":"clear","text":"<p>Clear the screen.</p> <pre><code>zeroipc&gt; clear\n[screen clears]\n</code></pre>"},{"location":"cli/virtual-filesystem/#quit-exit","title":"quit / exit","text":"<p>Exit the REPL.</p> <pre><code>zeroipc&gt; quit\nGoodbye!\n</code></pre>"},{"location":"cli/virtual-filesystem/#tips-and-tricks","title":"Tips and Tricks","text":""},{"location":"cli/virtual-filesystem/#1-quick-navigation","title":"1. Quick Navigation","text":"<p>Jump directly to any path: <pre><code>zeroipc&gt; cd /sensor_data\n/sensor_data&gt; cat temperatures[0:10]\n\n# Or combine:\nzeroipc&gt; cat /sensor_data/temperatures[0:10]\n</code></pre></p>"},{"location":"cli/virtual-filesystem/#2-path-verification","title":"2. Path Verification","text":"<p>Always use <code>pwd</code> to verify location: <pre><code>/somewhere&gt; pwd\n/sensor_data\n</code></pre></p>"},{"location":"cli/virtual-filesystem/#3-exploring-unknown-segments","title":"3. Exploring Unknown Segments","text":"<pre><code>zeroipc&gt; ls           # See what exists\nzeroipc&gt; cd /mystery  # Navigate to unknown segment\n/mystery&gt; ls          # See what's inside\n/mystery&gt; cat struct1 # Examine structures\n</code></pre>"},{"location":"cli/virtual-filesystem/#4-quick-checks","title":"4. Quick Checks","text":"<pre><code># Quick check of all segments\nzeroipc&gt; ls\n/sensor_data        10.0 MB\n/analytics          50.0 MB\n\n# Quick check of specific structure\nzeroipc&gt; cat /sensor_data/temperatures[0]\n[0] = 23.45\n</code></pre>"},{"location":"cli/virtual-filesystem/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Commands - Learn all commands</li> <li>Structure Inspection - Deep dive into structures  </li> <li>Monitoring - Real-time monitoring</li> </ul>"},{"location":"contributing/","title":"Contributing to ZeroIPC","text":"<p>We welcome contributions! This guide will help you get started.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>Bug Reports - Help us identify issues</li> <li>Feature Requests - Suggest new capabilities</li> <li>Documentation - Improve guides and examples</li> <li>Code Contributions - Fix bugs or add features</li> <li>Language Implementations - Add support for new languages</li> <li>Examples - Share real-world usage patterns</li> </ul>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#1-development-setup","title":"1. Development Setup","text":"<p>Follow the development setup guide to:</p> <ul> <li>Clone the repository</li> <li>Install dependencies</li> <li>Build all implementations</li> <li>Run the test suite</li> </ul>"},{"location":"contributing/#2-find-an-issue","title":"2. Find an Issue","text":"<p>Good first contributions:</p> <ul> <li>Issues labeled <code>good-first-issue</code></li> <li>Documentation improvements</li> <li>Example programs</li> <li>Test coverage improvements</li> </ul>"},{"location":"contributing/#3-make-changes","title":"3. Make Changes","text":"<p>Follow our:</p> <ul> <li>Testing Guidelines - Write and run tests</li> <li>Code Style - Match existing conventions</li> <li>Commit Messages - Clear and descriptive</li> </ul>"},{"location":"contributing/#4-submit-pull-request","title":"4. Submit Pull Request","text":"<ul> <li>Create feature branch</li> <li>Write tests</li> <li>Update documentation</li> <li>Submit PR with clear description</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers</li> <li>Focus on constructive feedback</li> <li>Credit others' work</li> </ul>"},{"location":"contributing/#language-implementation","title":"Language Implementation","text":"<p>Want to add Rust, Go, or another language?</p> <p>See Adding Language Support for:</p> <ul> <li>Binary format requirements</li> <li>Minimum structure set</li> <li>Cross-language testing</li> <li>Documentation requirements</li> </ul>"},{"location":"contributing/#documentation-style","title":"Documentation Style","text":"<p>When writing documentation:</p> <ul> <li>Be clear - Simple language, avoid jargon</li> <li>Be complete - Include examples and edge cases</li> <li>Be correct - Test code examples</li> <li>Be consistent - Follow existing style</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Discussions - Ask questions</li> <li>GitHub Issues - Report bugs</li> <li>Pull Requests - Get code review</li> </ul> <p>Thank you for contributing to ZeroIPC!</p>"},{"location":"data_structures/","title":"POSIX Shared Memory Data Structures","text":""},{"location":"data_structures/#overview","title":"Overview","text":"<p>This library provides a comprehensive set of lock-free data structures designed for high-performance inter-process communication (IPC) using POSIX shared memory. Each structure is carefully crafted for use in demanding applications like n-body simulations, real-time systems, and distributed computing.</p>"},{"location":"data_structures/#learning-path","title":"Learning Path","text":"<p>We recommend following this progression to understand the architecture:</p>"},{"location":"data_structures/#1-foundations","title":"1. Foundations","text":"<ul> <li>shm_atomic - Atomic operations in shared memory</li> <li>Memory Model - Understanding memory ordering and coherency</li> </ul>"},{"location":"data_structures/#2-basic-structures","title":"2. Basic Structures","text":"<ul> <li>shm_array - Fixed-size contiguous arrays</li> <li>shm_queue - Lock-free FIFO queue</li> <li>shm_stack - Lock-free LIFO stack</li> </ul>"},{"location":"data_structures/#3-advanced-structures","title":"3. Advanced Structures","text":"<ul> <li>shm_ring_buffer - Circular buffer for streaming</li> <li>shm_object_pool - Memory pool for dynamic allocation</li> <li>shm_hash_map - Concurrent hash table</li> </ul>"},{"location":"data_structures/#4-specialized-components","title":"4. Specialized Components","text":"<ul> <li>shm_simd - SIMD operations for vectorized computing</li> <li>shm_graph - Graph structures for network algorithms</li> </ul>"},{"location":"data_structures/#design-principles","title":"Design Principles","text":""},{"location":"data_structures/#lock-free-algorithms","title":"Lock-Free Algorithms","text":"<p>All data structures use atomic operations to ensure: - Progress Guarantee: At least one thread always makes progress - No Deadlocks: Impossible by design - Scalability: Performance scales with CPU cores - Low Latency: No kernel transitions for locks</p>"},{"location":"data_structures/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Cache-Line Alignment: Prevents false sharing</li> <li>Compact Layout: Minimal metadata overhead</li> <li>Zero-Copy: Direct memory access between processes</li> <li>Stack Allocation: No heap fragmentation</li> </ul>"},{"location":"data_structures/#type-safety","title":"Type Safety","text":"<ul> <li>Compile-Time Checks: <code>requires std::is_trivially_copyable_v&lt;T&gt;</code></li> <li>RAII: Automatic resource management</li> <li>Strong Types: No void* or unsafe casts</li> </ul>"},{"location":"data_structures/#use-case-n-body-simulation","title":"Use Case: N-Body Simulation","text":"<p>For a large-scale n-body simulation with multiple processes:</p> <pre><code>// Shared memory segment for simulation\nposix_shm shm(\"/nbody_sim\", 100 * 1024 * 1024);  // 100MB\n\n// Particle data\nstruct Particle {\n    float pos[3];\n    float vel[3];\n    float mass;\n};\n\n// Shared data structures\nshm_array&lt;Particle&gt; particles(shm, \"particles\", 1000000);\nshm_atomic&lt;uint64_t&gt; iteration(shm, \"iteration\", 0);\nshm_queue&lt;uint32_t&gt; work_queue(shm, \"work_queue\", 10000);\nshm_atomic&lt;double&gt; total_energy(shm, \"total_energy\", 0.0);\n\n// Process 1: Physics simulation\nvoid physics_worker() {\n    while (running) {\n        uint32_t particle_id;\n        if (work_queue.dequeue(particle_id)) {\n            compute_forces(particles[particle_id]);\n            update_position(particles[particle_id]);\n        }\n    }\n}\n\n// Process 2: Work distribution\nvoid scheduler() {\n    while (running) {\n        for (uint32_t i = 0; i &lt; particles.size(); ++i) {\n            work_queue.enqueue(i);\n        }\n        iteration.fetch_add(1);\n        wait_for_completion();\n    }\n}\n\n// Process 3: Monitoring\nvoid monitor() {\n    while (running) {\n        uint64_t iter = iteration.load();\n        double energy = total_energy.load();\n        std::cout &lt;&lt; \"Iteration: \" &lt;&lt; iter \n                  &lt;&lt; \", Energy: \" &lt;&lt; energy &lt;&lt; std::endl;\n        sleep(1);\n    }\n}\n</code></pre>"},{"location":"data_structures/#performance-characteristics","title":"Performance Characteristics","text":"Structure Insert Remove Access Space Use Case shm_atomic O(1) O(1) O(1) O(1) Counters, flags shm_array - - O(1) O(n) Fixed collections shm_queue O(1)* O(1)* - O(n) Task distribution shm_stack O(1)* O(1)* - O(n) Undo/redo, recursion shm_ring_buffer O(1) O(1) O(1) O(n) Data streaming shm_object_pool O(1)* O(1)* O(1) O(n) Dynamic allocation <p>* Amortized, may retry under contention</p>"},{"location":"data_structures/#thread-safety-guarantees","title":"Thread Safety Guarantees","text":"<p>All structures provide: - Thread-safe read/write operations - Process-safe through shared memory - Signal-safe for async signal handlers (with care) - Fork-safe with proper initialization</p>"},{"location":"data_structures/#memory-requirements","title":"Memory Requirements","text":""},{"location":"data_structures/#overhead-per-structure","title":"Overhead per Structure","text":"<ul> <li>Metadata table entry: 64 bytes</li> <li>Cache line alignment: up to 63 bytes padding</li> <li>Structure header: 64-128 bytes (structure-dependent)</li> </ul>"},{"location":"data_structures/#example-calculation","title":"Example Calculation","text":"<p>For 1 million particles (24 bytes each): - Data: 24 MB - shm_table: 4 KB (default size) - shm_array header: 64 bytes - Total: ~24.004 MB</p>"},{"location":"data_structures/#building-your-own-structures","title":"Building Your Own Structures","text":"<p>To create custom lock-free structures:</p> <ol> <li> <p>Inherit from shm_span <pre><code>template&lt;typename T&gt;\nclass my_structure : public shm_span&lt;T, posix_shm&gt; {\n    // Your implementation\n};\n</code></pre></p> </li> <li> <p>Use atomic operations <pre><code>std::atomic&lt;size_t&gt; index;\nindex.fetch_add(1, std::memory_order_relaxed);\n</code></pre></p> </li> <li> <p>Ensure trivially copyable <pre><code>static_assert(std::is_trivially_copyable_v&lt;T&gt;);\n</code></pre></p> </li> <li> <p>Register in shm_table <pre><code>table-&gt;add(name, offset, size, sizeof(T), count);\n</code></pre></p> </li> </ol>"},{"location":"data_structures/#testing-and-validation","title":"Testing and Validation","text":"<p>Each structure includes: - Unit tests with Catch2 - Stress tests for concurrency - Cross-process validation - Memory leak detection - Performance benchmarks</p> <p>Run tests: <pre><code>make test\n./build/tests/posix_shm_tests\n</code></pre></p>"},{"location":"data_structures/#contributing","title":"Contributing","text":"<p>We welcome contributions! Areas of interest: - Additional lock-free structures - Performance optimizations - Platform-specific enhancements - Documentation improvements - Example applications</p>"},{"location":"data_structures/#references","title":"References","text":""},{"location":"data_structures/#books","title":"Books","text":"<ul> <li>\"The Art of Multiprocessor Programming\" - Herlihy &amp; Shavit</li> <li>\"C++ Concurrency in Action\" - Anthony Williams</li> <li>\"Perfbook\" - Paul McKenney</li> </ul>"},{"location":"data_structures/#papers","title":"Papers","text":"<ul> <li>\"Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms\" - Michael &amp; Scott</li> <li>\"Hazard Pointers: Safe Memory Reclamation for Lock-Free Objects\" - Michael</li> <li>\"A Pragmatic Implementation of Non-Blocking Linked-Lists\" - Harris</li> </ul>"},{"location":"data_structures/#online-resources","title":"Online Resources","text":"<ul> <li>Lock-Free Programming</li> <li>C++ Memory Model</li> <li>Intel Threading Building Blocks</li> </ul>"},{"location":"data_structures/computational_structures/","title":"Computational Data Structures: When Data Becomes Code","text":""},{"location":"data_structures/computational_structures/#overview","title":"Overview","text":"<p>Beyond traditional collections, we can create shared memory structures that represent computation itself. These blur the line between data and code, enabling distributed computation patterns, lazy evaluation, and functional programming paradigms in shared memory.</p>"},{"location":"data_structures/computational_structures/#proposed-computational-structures","title":"Proposed Computational Structures","text":""},{"location":"data_structures/computational_structures/#1-shm_future-asynchronous-computation-results","title":"1. shm_future: Asynchronous Computation Results <pre><code>template&lt;typename T&gt;\nclass shm_future {\n    enum State { PENDING, COMPUTING, READY, ERROR };\n\n    struct FutureData {\n        std::atomic&lt;State&gt; state{PENDING};\n        T value;\n        char error_msg[256];\n        std::atomic&lt;uint32_t&gt; waiters{0};\n    };\n\n    shm_span&lt;FutureData&gt; data;\n\npublic:\n    // Producer sets the value\n    void set_value(const T&amp; val) {\n        data-&gt;value = val;\n        data-&gt;state.store(READY, std::memory_order_release);\n        wake_waiters();\n    }\n\n    // Consumer waits for value\n    T get() {\n        while (data-&gt;state.load(std::memory_order_acquire) != READY) {\n            wait();\n        }\n        return data-&gt;value;\n    }\n\n    // Continuation\n    template&lt;typename F&gt;\n    auto then(F&amp;&amp; func) -&gt; shm_future&lt;decltype(func(T{}))&gt; {\n        using U = decltype(func(T{}));\n        auto next = shm_future&lt;U&gt;(shm, generate_name());\n\n        // Store continuation\n        on_ready([func, next]() {\n            next.set_value(func(this-&gt;get()));\n        });\n\n        return next;\n    }\n};\n\n// Usage in n-body simulation\nshm_future&lt;ForceField&gt; force_calculation(shm, \"forces\");\nshm_future&lt;Positions&gt; position_update = \n    force_calculation.then([](const ForceField&amp; forces) {\n        return integrate_positions(forces);\n    });\n</code></pre>","text":""},{"location":"data_structures/computational_structures/#2-shm_lazy-lazy-evaluation","title":"2. shm_lazy: Lazy Evaluation <pre><code>template&lt;typename T&gt;\nclass shm_lazy {\n    mutable std::atomic&lt;bool&gt; computed{false};\n    mutable T value;\n    std::function&lt;T()&gt; computation;\n\npublic:\n    shm_lazy(std::function&lt;T()&gt; comp) : computation(comp) {}\n\n    const T&amp; force() const {\n        bool expected = false;\n        if (computed.compare_exchange_strong(expected, true)) {\n            value = computation();\n        } else {\n            // Wait for computation by another thread\n            while (!computed.load()) {\n                std::this_thread::yield();\n            }\n        }\n        return value;\n    }\n\n    // Monadic operations\n    template&lt;typename F&gt;\n    auto map(F&amp;&amp; f) -&gt; shm_lazy&lt;decltype(f(T{}))&gt; {\n        return shm_lazy([=]() { return f(this-&gt;force()); });\n    }\n\n    template&lt;typename F&gt;\n    auto flatMap(F&amp;&amp; f) -&gt; decltype(f(T{})) {\n        return f(this-&gt;force());\n    }\n};\n\n// Lazy computation tree\nshm_lazy&lt;double&gt; total_energy = shm_lazy([&amp;]() {\n    return compute_kinetic_energy() + compute_potential_energy();\n});\n\n// Only computed when needed\nif (should_check_energy_conservation()) {\n    double e = total_energy.force();\n}\n</code></pre>","text":""},{"location":"data_structures/computational_structures/#3-shm_closure-first-class-functions-in-shared-memory","title":"3. shm_closure: First-Class Functions in Shared Memory <pre><code>template&lt;typename Sig&gt;\nclass shm_closure;\n\ntemplate&lt;typename R, typename... Args&gt;\nclass shm_closure&lt;R(Args...)&gt; {\n    enum OpCode {\n        ADD, MUL, COMPOSE, PARTIAL, CUSTOM\n    };\n\n    struct ClosureData {\n        OpCode op;\n        union {\n            struct { double a, b; } binary_op;\n            struct { size_t f_idx, g_idx; } composition;\n            struct { size_t func_idx; double bound_arg; } partial;\n            struct { char code[256]; } custom;\n        };\n    };\n\npublic:\n    R operator()(Args... args) {\n        switch (data-&gt;op) {\n            case ADD: return data-&gt;binary_op.a + args...;\n            case MUL: return data-&gt;binary_op.a * args...;\n            case COMPOSE: {\n                auto f = shm_closure(shm, data-&gt;composition.f_idx);\n                auto g = shm_closure(shm, data-&gt;composition.g_idx);\n                return f(g(args...));\n            }\n            case PARTIAL: {\n                auto f = shm_closure(shm, data-&gt;partial.func_idx);\n                return f(data-&gt;partial.bound_arg, args...);\n            }\n            case CUSTOM:\n                return interpret(data-&gt;custom.code, args...);\n        }\n    }\n\n    // Function composition\n    template&lt;typename G&gt;\n    auto compose(const shm_closure&lt;G&gt;&amp; g) {\n        // Store composition in shared memory\n        return shm_closure(COMPOSE, this-&gt;index, g.index);\n    }\n\n    // Partial application\n    auto bind(double arg) {\n        return shm_closure(PARTIAL, this-&gt;index, arg);\n    }\n};\n\n// Usage\nshm_closure&lt;double(double)&gt; gravity_force(shm, \"gravity\");\nshm_closure&lt;double(double)&gt; damping(shm, \"damping\");\n\nauto combined_force = gravity_force.compose(damping);\ndouble f = combined_force(distance);\n</code></pre>","text":""},{"location":"data_structures/computational_structures/#4-shm_stream-reactive-streams","title":"4. shm_stream: Reactive Streams <pre><code>template&lt;typename T&gt;\nclass shm_stream {\n    shm_ring_buffer&lt;T&gt; buffer;\n    shm_atomic&lt;uint64_t&gt; sequence;\n    std::vector&lt;shm_future&lt;void&gt;&gt; subscribers;\n\npublic:\n    void emit(const T&amp; value) {\n        buffer.push(value);\n        uint64_t seq = sequence.fetch_add(1);\n\n        // Notify subscribers\n        for (auto&amp; sub : subscribers) {\n            sub.notify(seq, value);\n        }\n    }\n\n    // Functional transformations\n    template&lt;typename F&gt;\n    auto map(F&amp;&amp; f) -&gt; shm_stream&lt;decltype(f(T{}))&gt; {\n        using U = decltype(f(T{}));\n        shm_stream&lt;U&gt; output(shm, generate_name());\n\n        this-&gt;subscribe([f, output](const T&amp; val) {\n            output.emit(f(val));\n        });\n\n        return output;\n    }\n\n    template&lt;typename F&gt;\n    auto filter(F&amp;&amp; predicate) -&gt; shm_stream&lt;T&gt; {\n        shm_stream&lt;T&gt; output(shm, generate_name());\n\n        this-&gt;subscribe([predicate, output](const T&amp; val) {\n            if (predicate(val)) {\n                output.emit(val);\n            }\n        });\n\n        return output;\n    }\n\n    // Windowing\n    auto window(size_t size) -&gt; shm_stream&lt;std::vector&lt;T&gt;&gt; {\n        shm_stream&lt;std::vector&lt;T&gt;&gt; output(shm, generate_name());\n        std::vector&lt;T&gt; window_buffer;\n\n        this-&gt;subscribe([output, window_buffer, size](const T&amp; val) mutable {\n            window_buffer.push_back(val);\n            if (window_buffer.size() == size) {\n                output.emit(window_buffer);\n                window_buffer.clear();\n            }\n        });\n\n        return output;\n    }\n};\n\n// Reactive n-body simulation pipeline\nshm_stream&lt;Particle&gt; particle_updates(shm, \"updates\");\n\nauto collisions = particle_updates\n    .window(2)  // Pairs\n    .filter([](const auto&amp; pair) { \n        return distance(pair[0], pair[1]) &lt; threshold; \n    })\n    .map([](const auto&amp; pair) { \n        return compute_collision(pair[0], pair[1]); \n    });\n\ncollisions.subscribe([](const Collision&amp; c) {\n    render_collision_effect(c);\n});\n</code></pre>","text":""},{"location":"data_structures/computational_structures/#5-shm_fsm-finite-state-machines","title":"5. shm_fsm: Finite State Machines <pre><code>template&lt;typename State, typename Event&gt;\nclass shm_fsm {\n    using Handler = std::function&lt;State(State, Event)&gt;;\n\n    struct Transition {\n        State from;\n        Event trigger;\n        State to;\n        size_t action_idx;  // Index to action in shared memory\n    };\n\n    shm_array&lt;Transition&gt; transitions;\n    shm_atomic&lt;State&gt; current_state;\n    shm_queue&lt;Event&gt; event_queue;\n\npublic:\n    void process_event(const Event&amp; event) {\n        State state = current_state.load();\n\n        for (const auto&amp; trans : transitions) {\n            if (trans.from == state &amp;&amp; trans.trigger == event) {\n                // Execute action\n                execute_action(trans.action_idx);\n\n                // Transition\n                current_state.store(trans.to);\n\n                // Log transition\n                log_transition(state, event, trans.to);\n                break;\n            }\n        }\n    }\n\n    void run() {\n        while (running) {\n            if (auto event = event_queue.dequeue()) {\n                process_event(*event);\n            }\n        }\n    }\n};\n\n// Simulation state machine\nenum SimState { INIT, RUNNING, PAUSED, STEPPING, STOPPED };\nenum SimEvent { START, PAUSE, STEP, STOP, RESET };\n\nshm_fsm&lt;SimState, SimEvent&gt; sim_fsm(shm, \"sim_fsm\");\nsim_fsm.add_transition(INIT, START, RUNNING, []() { \n    initialize_particles(); \n});\nsim_fsm.add_transition(RUNNING, PAUSE, PAUSED, []() { \n    save_checkpoint(); \n});\n</code></pre>","text":""},{"location":"data_structures/computational_structures/#6-shm_dag-computation-dag","title":"6. shm_dag: Computation DAG <pre><code>template&lt;typename T&gt;\nclass shm_dag {\n    struct Node {\n        std::string name;\n        std::vector&lt;size_t&gt; dependencies;\n        std::function&lt;T()&gt; computation;\n        shm_future&lt;T&gt; result;\n        std::atomic&lt;bool&gt; computed{false};\n    };\n\n    shm_array&lt;Node&gt; nodes;\n    shm_queue&lt;size_t&gt; ready_queue;\n\npublic:\n    size_t add_node(const std::string&amp; name, \n                    std::function&lt;T()&gt; comp,\n                    std::vector&lt;std::string&gt; deps = {}) {\n        Node node{name, resolve_deps(deps), comp};\n        return nodes.push_back(node);\n    }\n\n    void execute() {\n        // Topological sort\n        auto order = topological_sort();\n\n        // Parallel execution respecting dependencies\n        #pragma omp parallel\n        {\n            while (auto node_idx = ready_queue.dequeue()) {\n                auto&amp; node = nodes[*node_idx];\n\n                // Wait for dependencies\n                for (size_t dep : node.dependencies) {\n                    nodes[dep].result.wait();\n                }\n\n                // Execute\n                T result = node.computation();\n                node.result.set_value(result);\n                node.computed.store(true);\n\n                // Notify dependents\n                notify_dependents(*node_idx);\n            }\n        }\n    }\n};\n\n// Build computation graph for n-body\nshm_dag&lt;void&gt; simulation_dag(shm, \"sim_dag\");\n\nauto forces = simulation_dag.add_node(\"forces\", \n    []() { compute_all_forces(); });\n\nauto positions = simulation_dag.add_node(\"positions\",\n    []() { integrate_positions(); }, {\"forces\"});\n\nauto collisions = simulation_dag.add_node(\"collisions\",\n    []() { detect_collisions(); }, {\"positions\"});\n\nauto energy = simulation_dag.add_node(\"energy\",\n    []() { compute_total_energy(); }, {\"positions\"});\n\nsimulation_dag.execute();\n</code></pre>","text":""},{"location":"data_structures/computational_structures/#7-shm_memo-memoization-table","title":"7. shm_memo: Memoization Table <pre><code>template&lt;typename Key, typename Value&gt;\nclass shm_memo {\n    shm_hash_map&lt;Key, Value&gt; cache;\n    shm_atomic&lt;size_t&gt; hits{0};\n    shm_atomic&lt;size_t&gt; misses{0};\n\npublic:\n    template&lt;typename F&gt;\n    Value compute(const Key&amp; key, F&amp;&amp; expensive_function) {\n        // Check cache\n        if (auto cached = cache.find(key)) {\n            hits.fetch_add(1);\n            return *cached;\n        }\n\n        // Compute and cache\n        misses.fetch_add(1);\n        Value result = expensive_function(key);\n        cache.insert(key, result);\n        return result;\n    }\n\n    double hit_rate() const {\n        size_t h = hits.load();\n        size_t m = misses.load();\n        return h / double(h + m);\n    }\n};\n\n// Memoize expensive distance calculations\nshm_memo&lt;std::pair&lt;uint32_t, uint32_t&gt;, float&gt; distance_cache(shm, \"distances\");\n\nfloat get_distance(uint32_t i, uint32_t j) {\n    if (i &gt; j) std::swap(i, j);  // Canonical order\n\n    return distance_cache.compute({i, j}, [i, j]() {\n        return compute_euclidean_distance(particles[i], particles[j]);\n    });\n}\n</code></pre>","text":""},{"location":"data_structures/computational_structures/#implementation-considerations","title":"Implementation Considerations","text":""},{"location":"data_structures/computational_structures/#memory-management","title":"Memory Management <ul> <li>Function pointers can't cross process boundaries</li> <li>Store computation as data (opcodes, AST, bytecode)</li> <li>Use indices into shared lookup tables</li> </ul>","text":""},{"location":"data_structures/computational_structures/#synchronization","title":"Synchronization <ul> <li>Atomic state transitions</li> <li>Lock-free algorithms where possible</li> <li>Event-driven architecture</li> </ul>","text":""},{"location":"data_structures/computational_structures/#performance","title":"Performance <ul> <li>Cache-align computational state</li> <li>Batch operations</li> <li>Use work-stealing for parallel execution</li> </ul>","text":""},{"location":"data_structures/computational_structures/#use-cases","title":"Use Cases","text":"<ol> <li>Lazy Physics - Compute forces only when needed</li> <li>Reactive Visualization - Stream processing for real-time updates</li> <li>Workflow Orchestration - DAG-based simulation pipelines</li> <li>Checkpointing - State machines for simulation control</li> <li>Caching - Memoization of expensive computations</li> </ol>"},{"location":"data_structures/computational_structures/#future-directions","title":"Future Directions","text":"<ul> <li>shm_coroutine - Suspendable computations</li> <li>shm_transducer - Composable algorithmic transformations</li> <li>shm_genetic - Genetic algorithms with shared population</li> </ul> <p>These computational structures transform shared memory from passive data storage into active computation engines, enabling new patterns for distributed computing, functional programming, and reactive systems in high-performance simulations.</p>"},{"location":"data_structures/shm_array/","title":"shm_array: Fixed-Size Arrays in Shared Memory","text":""},{"location":"data_structures/shm_array/#overview","title":"Overview","text":"<p><code>shm_array&lt;T&gt;</code> provides a fixed-size, contiguous array in POSIX shared memory with O(1) random access. It's the simplest and most cache-friendly shared memory data structure, ideal for storing particle data, matrices, and other fixed collections in n-body simulations.</p>"},{"location":"data_structures/shm_array/#architecture","title":"Architecture","text":""},{"location":"data_structures/shm_array/#memory-layout","title":"Memory Layout","text":"<pre><code>[shm_table metadata]\n[ArrayHeader]\n  \u251c\u2500 size_t count            // Number of elements\n  \u2514\u2500 padding[56]             // Cache line alignment\n[T][T][T]...[T]              // Contiguous elements\n</code></pre>"},{"location":"data_structures/shm_array/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li>Contiguous Memory: Optimal cache locality and prefetching</li> <li>Zero Overhead: Direct memory access after initialization</li> <li>SIMD-Friendly: Natural alignment for vectorized operations</li> <li>Compare-and-Swap: Atomic operations on individual elements</li> </ol>"},{"location":"data_structures/shm_array/#api-reference","title":"API Reference","text":""},{"location":"data_structures/shm_array/#construction","title":"Construction","text":"<pre><code>// Create new array\nposix_shm shm(\"/nbody\", 100 * 1024 * 1024);\nshm_array&lt;Particle&gt; particles(shm, \"particles\", 1000000);\n\n// Open existing array\nshm_array&lt;Particle&gt; existing(shm, \"particles\");\n\n// Check existence\nif (shm_array&lt;Particle&gt;::exists(shm, \"particles\")) {\n    shm_array&lt;Particle&gt; arr(shm, \"particles\");\n}\n</code></pre>"},{"location":"data_structures/shm_array/#element-access","title":"Element Access","text":"<pre><code>// Direct indexing\nparticles[42].position = {1.0f, 2.0f, 3.0f};\nfloat mass = particles[42].mass;\n\n// Bounds-checked access\nParticle&amp; p = particles.at(42);  // Throws if out of bounds\n\n// Get raw pointer for bulk operations\nParticle* data = particles.data();\nmemcpy(data, source, sizeof(Particle) * count);\n\n// Size and iteration\nsize_t n = particles.size();\nfor (size_t i = 0; i &lt; particles.size(); ++i) {\n    process(particles[i]);\n}\n</code></pre>"},{"location":"data_structures/shm_array/#atomic-operations","title":"Atomic Operations","text":"<pre><code>// Compare-and-swap on element\nParticle expected = particles[i];\nParticle desired = compute_new_state(expected);\nbool success = particles.compare_exchange(i, expected, desired);\n\n// Atomic update with retry\nvoid atomic_update(size_t idx, const Particle&amp; update) {\n    Particle current = particles[idx];\n    while (!particles.compare_exchange(idx, current, update)) {\n        current = particles[idx];\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_array/#simd-operations","title":"SIMD Operations","text":"<pre><code>// Bulk operations with SIMD\nvoid update_positions(shm_array&lt;float3&gt;&amp; positions, \n                      const shm_array&lt;float3&gt;&amp; velocities,\n                      float dt) {\n    float* pos = reinterpret_cast&lt;float*&gt;(positions.data());\n    const float* vel = reinterpret_cast&lt;const float*&gt;(velocities.data());\n    size_t n = positions.size() * 3;\n\n    // Process 8 floats at a time with AVX\n    for (size_t i = 0; i &lt; n; i += 8) {\n        __m256 p = _mm256_load_ps(&amp;pos[i]);\n        __m256 v = _mm256_load_ps(&amp;vel[i]);\n        __m256 dt_vec = _mm256_set1_ps(dt);\n        p = _mm256_fmadd_ps(v, dt_vec, p);\n        _mm256_store_ps(&amp;pos[i], p);\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_array/#use-cases-in-n-body-simulation","title":"Use Cases in N-Body Simulation","text":""},{"location":"data_structures/shm_array/#1-particle-storage-array-of-structures","title":"1. Particle Storage (Array of Structures)","text":"<pre><code>struct Particle {\n    float3 position;\n    float3 velocity;\n    float mass;\n    float radius;\n};\n\nshm_array&lt;Particle&gt; particles(shm, \"particles\", 1000000);\n\n// Update particle physics\nvoid update_particle(size_t idx, float dt) {\n    Particle&amp; p = particles[idx];\n    p.position += p.velocity * dt;\n\n    // Apply boundary conditions\n    for (int i = 0; i &lt; 3; ++i) {\n        if (p.position[i] &lt; 0 || p.position[i] &gt; box_size) {\n            p.velocity[i] = -p.velocity[i];\n        }\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_array/#2-structure-of-arrays-soa-for-simd","title":"2. Structure of Arrays (SoA) for SIMD","text":"<pre><code>// Better cache usage and SIMD efficiency\nstruct ParticleSystem {\n    shm_array&lt;float&gt; pos_x, pos_y, pos_z;\n    shm_array&lt;float&gt; vel_x, vel_y, vel_z;\n    shm_array&lt;float&gt; mass;\n\n    ParticleSystem(posix_shm&amp; shm, size_t n)\n        : pos_x(shm, \"pos_x\", n), pos_y(shm, \"pos_y\", n), pos_z(shm, \"pos_z\", n),\n          vel_x(shm, \"vel_x\", n), vel_y(shm, \"vel_y\", n), vel_z(shm, \"vel_z\", n),\n          mass(shm, \"mass\", n) {}\n\n    void update_positions_simd(float dt) {\n        #pragma omp parallel for simd\n        for (size_t i = 0; i &lt; pos_x.size(); ++i) {\n            pos_x[i] += vel_x[i] * dt;\n            pos_y[i] += vel_y[i] * dt;\n            pos_z[i] += vel_z[i] * dt;\n        }\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_array/#3-spatial-grid","title":"3. Spatial Grid","text":"<pre><code>template&lt;typename T&gt;\nclass SpatialGrid {\n    shm_array&lt;std::vector&lt;uint32_t&gt;&gt; cells;\n    float cell_size;\n    uint32_t grid_dim;\n\npublic:\n    SpatialGrid(posix_shm&amp; shm, float world_size, float cell_sz)\n        : cell_size(cell_sz),\n          grid_dim(world_size / cell_sz),\n          cells(shm, \"grid\", grid_dim * grid_dim * grid_dim) {}\n\n    uint32_t cell_index(const float3&amp; pos) {\n        uint32_t x = pos.x / cell_size;\n        uint32_t y = pos.y / cell_size;\n        uint32_t z = pos.z / cell_size;\n        return x + y * grid_dim + z * grid_dim * grid_dim;\n    }\n\n    void insert(uint32_t particle_id, const float3&amp; pos) {\n        uint32_t idx = cell_index(pos);\n        cells[idx].push_back(particle_id);\n    }\n\n    std::vector&lt;uint32_t&gt; neighbors(const float3&amp; pos) {\n        std::vector&lt;uint32_t&gt; result;\n        int cx = pos.x / cell_size;\n        int cy = pos.y / cell_size;\n        int cz = pos.z / cell_size;\n\n        // Check 27 neighboring cells\n        for (int dx = -1; dx &lt;= 1; ++dx) {\n            for (int dy = -1; dy &lt;= 1; ++dy) {\n                for (int dz = -1; dz &lt;= 1; ++dz) {\n                    int x = cx + dx, y = cy + dy, z = cz + dz;\n                    if (x &gt;= 0 &amp;&amp; x &lt; grid_dim &amp;&amp; \n                        y &gt;= 0 &amp;&amp; y &lt; grid_dim &amp;&amp; \n                        z &gt;= 0 &amp;&amp; z &lt; grid_dim) {\n                        uint32_t idx = x + y * grid_dim + z * grid_dim * grid_dim;\n                        result.insert(result.end(), \n                                    cells[idx].begin(), cells[idx].end());\n                    }\n                }\n            }\n        }\n        return result;\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_array/#4-matrix-operations","title":"4. Matrix Operations","text":"<pre><code>template&lt;typename T, size_t Rows, size_t Cols&gt;\nclass Matrix {\n    shm_array&lt;T&gt; data;\n\npublic:\n    Matrix(posix_shm&amp; shm, const std::string&amp; name)\n        : data(shm, name, Rows * Cols) {}\n\n    T&amp; operator()(size_t row, size_t col) {\n        return data[row * Cols + col];\n    }\n\n    // Matrix multiplication with blocking for cache\n    void multiply(const Matrix&amp; A, const Matrix&amp; B, Matrix&amp; C) {\n        constexpr size_t BLOCK = 64 / sizeof(T);\n\n        for (size_t ii = 0; ii &lt; Rows; ii += BLOCK) {\n            for (size_t jj = 0; jj &lt; Cols; jj += BLOCK) {\n                for (size_t kk = 0; kk &lt; Cols; kk += BLOCK) {\n                    // Process block\n                    for (size_t i = ii; i &lt; std::min(ii + BLOCK, Rows); ++i) {\n                        for (size_t j = jj; j &lt; std::min(jj + BLOCK, Cols); ++j) {\n                            T sum = 0;\n                            for (size_t k = kk; k &lt; std::min(kk + BLOCK, Cols); ++k) {\n                                sum += A(i, k) * B(k, j);\n                            }\n                            C(i, j) += sum;\n                        }\n                    }\n                }\n            }\n        }\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_array/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"data_structures/shm_array/#memory-access-patterns","title":"Memory Access Patterns","text":"Pattern Performance Cache Behavior Sequential Optimal Prefetch friendly Strided Good Depends on stride Random Poor Cache thrashing SIMD aligned Optimal Vectorized"},{"location":"data_structures/shm_array/#throughput","title":"Throughput","text":"<ul> <li>Sequential Read: Memory bandwidth limited (~50 GB/s)</li> <li>Sequential Write: Memory bandwidth limited (~30 GB/s)</li> <li>Random Access: ~10M ops/sec (cache misses)</li> <li>SIMD Operations: 4-16x speedup for suitable operations</li> </ul>"},{"location":"data_structures/shm_array/#cache-effects","title":"Cache Effects","text":"<pre><code>// Poor: Column-major access in row-major layout\nfor (size_t col = 0; col &lt; COLS; ++col) {\n    for (size_t row = 0; row &lt; ROWS; ++row) {\n        process(matrix[row * COLS + col]);  // Cache miss every iteration\n    }\n}\n\n// Good: Row-major access\nfor (size_t row = 0; row &lt; ROWS; ++row) {\n    for (size_t col = 0; col &lt; COLS; ++col) {\n        process(matrix[row * COLS + col]);  // Sequential, cache friendly\n    }\n}\n\n// Best: Blocked for cache\nfor (size_t rb = 0; rb &lt; ROWS; rb += BLOCK) {\n    for (size_t cb = 0; cb &lt; COLS; cb += BLOCK) {\n        // Process block that fits in cache\n        for (size_t r = rb; r &lt; rb + BLOCK; ++r) {\n            for (size_t c = cb; c &lt; cb + BLOCK; ++c) {\n                process(matrix[r * COLS + c]);\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_array/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"data_structures/shm_array/#1-memory-mapping-views","title":"1. Memory Mapping Views","text":"<pre><code>template&lt;typename T&gt;\nclass ArrayView {\n    T* ptr;\n    size_t len;\n\npublic:\n    ArrayView(shm_array&lt;T&gt;&amp; arr, size_t start, size_t count)\n        : ptr(arr.data() + start), len(count) {}\n\n    T&amp; operator[](size_t i) { return ptr[i]; }\n    size_t size() const { return len; }\n\n    // Subview\n    ArrayView slice(size_t start, size_t count) {\n        return ArrayView(ptr + start, count);\n    }\n};\n\n// Partition array for parallel processing\nvoid parallel_process(shm_array&lt;float&gt;&amp; data) {\n    size_t chunk_size = data.size() / num_threads;\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        ArrayView&lt;float&gt; chunk(data, tid * chunk_size, chunk_size);\n\n        for (size_t i = 0; i &lt; chunk.size(); ++i) {\n            chunk[i] = expensive_computation(chunk[i]);\n        }\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_array/#2-copy-on-write-semantics","title":"2. Copy-on-Write Semantics","text":"<pre><code>template&lt;typename T&gt;\nclass COWArray {\n    shm_array&lt;T&gt; data;\n    shm_array&lt;uint32_t&gt; version;\n    shm_atomic&lt;uint32_t&gt; global_version;\n\npublic:\n    T read(size_t idx) {\n        return data[idx];\n    }\n\n    void write(size_t idx, const T&amp; value) {\n        uint32_t current_ver = global_version.load();\n        if (version[idx] &lt; current_ver) {\n            // First write in this version\n            save_undo(idx, data[idx]);\n            version[idx] = current_ver;\n        }\n        data[idx] = value;\n    }\n\n    void new_version() {\n        global_version.fetch_add(1);\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_array/#3-parallel-reduction","title":"3. Parallel Reduction","text":"<pre><code>template&lt;typename T, typename Op&gt;\nT parallel_reduce(shm_array&lt;T&gt;&amp; arr, T init, Op op) {\n    const size_t CACHE_LINE = 64;\n    const size_t PADDING = CACHE_LINE / sizeof(T);\n\n    // Per-thread partial sums with padding to avoid false sharing\n    shm_array&lt;T&gt; partials(shm, \"partials\", num_threads * PADDING);\n\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        size_t chunk_size = arr.size() / num_threads;\n        size_t start = tid * chunk_size;\n        size_t end = (tid == num_threads - 1) ? arr.size() : start + chunk_size;\n\n        T local_sum = init;\n        for (size_t i = start; i &lt; end; ++i) {\n            local_sum = op(local_sum, arr[i]);\n        }\n\n        partials[tid * PADDING] = local_sum;\n    }\n\n    // Final reduction\n    T result = init;\n    for (int i = 0; i &lt; num_threads; ++i) {\n        result = op(result, partials[i * PADDING]);\n    }\n    return result;\n}\n\n// Usage\nfloat total_mass = parallel_reduce(masses, 0.0f, std::plus&lt;float&gt;());\n</code></pre>"},{"location":"data_structures/shm_array/#comparison-with-other-structures","title":"Comparison with Other Structures","text":"Aspect shm_array std::vector shm_queue shm_ring_buffer Size Fixed Dynamic Bounded Bounded Access O(1) O(1) No random O(1) Cache Optimal Good Good Good Growth No Yes No No SIMD Natural Good Poor Good Memory Contiguous Contiguous Contiguous Circular"},{"location":"data_structures/shm_array/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"data_structures/shm_array/#1-false-sharing","title":"1. False Sharing","text":"<pre><code>// WRONG: Multiple threads updating adjacent elements\nshm_array&lt;Counter&gt; counters(shm, \"counters\", num_threads);\n#pragma omp parallel\n{\n    int tid = omp_get_thread_num();\n    for (int i = 0; i &lt; 1000000; ++i) {\n        counters[tid].value++;  // False sharing!\n    }\n}\n\n// CORRECT: Padding to cache line\nstruct PaddedCounter {\n    std::atomic&lt;uint64_t&gt; value;\n    char padding[56];  // Total 64 bytes\n};\nshm_array&lt;PaddedCounter&gt; counters(shm, \"counters\", num_threads);\n</code></pre>"},{"location":"data_structures/shm_array/#2-array-of-structures-vs-structure-of-arrays","title":"2. Array of Structures vs Structure of Arrays","text":"<pre><code>// AoS: Poor cache usage for single field access\nstruct Particle { float x, y, z, vx, vy, vz, m; };\nshm_array&lt;Particle&gt; particles(shm, \"aos\", n);\n\n// Accessing only positions wastes cache\nfor (size_t i = 0; i &lt; n; ++i) {\n    process_position(particles[i].x, particles[i].y, particles[i].z);\n    // Loads 28 bytes but uses only 12\n}\n\n// SoA: Better cache usage\nstruct Particles {\n    shm_array&lt;float&gt; x, y, z, vx, vy, vz, m;\n};\n\n// Accessing positions is cache-efficient\nfor (size_t i = 0; i &lt; n; ++i) {\n    process_position(x[i], y[i], z[i]);\n    // Each array access is cache-friendly\n}\n</code></pre>"},{"location":"data_structures/shm_array/#testing-strategies","title":"Testing Strategies","text":"<pre><code>TEST_CASE(\"Array handles concurrent CAS\") {\n    shm_array&lt;std::atomic&lt;int&gt;&gt; arr(shm, \"atomic\", 100);\n\n    // Initialize\n    for (int i = 0; i &lt; 100; ++i) {\n        arr[i].store(0);\n    }\n\n    // Concurrent increments\n    std::vector&lt;std::thread&gt; threads;\n    for (int t = 0; t &lt; 10; ++t) {\n        threads.emplace_back([&amp;]() {\n            for (int iter = 0; iter &lt; 1000; ++iter) {\n                for (int i = 0; i &lt; 100; ++i) {\n                    arr[i].fetch_add(1);\n                }\n            }\n        });\n    }\n\n    for (auto&amp; t : threads) t.join();\n\n    // Verify\n    for (int i = 0; i &lt; 100; ++i) {\n        REQUIRE(arr[i].load() == 10000);\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_array/#references","title":"References","text":"<ul> <li>What Every Programmer Should Know About Memory - Ulrich Drepper</li> <li>Gallery of Processor Cache Effects - Igor Ostrovsky</li> <li>False Sharing - Martin Thompson</li> </ul>"},{"location":"data_structures/shm_atomic/","title":"shm_atomic: Lock-Free Atomic Operations in Shared Memory","text":""},{"location":"data_structures/shm_atomic/#overview","title":"Overview","text":"<p><code>shm_atomic&lt;T&gt;</code> provides the fundamental building block for lock-free inter-process communication in POSIX shared memory. It wraps a single value of type <code>T</code> with atomic operations, enabling safe concurrent access from multiple processes without explicit locking.</p>"},{"location":"data_structures/shm_atomic/#why-start-with-atomics","title":"Why Start with Atomics?","text":"<p>In the hierarchy of concurrent data structures, atomic operations form the foundation:</p> <ol> <li>Hardware Level: Modern CPUs provide atomic instructions (CAS, fetch-and-add, etc.)</li> <li>Language Level: C++ <code>std::atomic</code> exposes these as a portable interface</li> <li>IPC Level: <code>shm_atomic</code> extends this to shared memory between processes</li> </ol> <p>For n-body simulations and other high-performance computing applications, atomic operations enable: - Lock-free progress guarantees - Minimal synchronization overhead - Cache-friendly memory access patterns - Scalable multi-process coordination</p>"},{"location":"data_structures/shm_atomic/#technical-design","title":"Technical Design","text":""},{"location":"data_structures/shm_atomic/#memory-layout","title":"Memory Layout","text":"<pre><code>[shm_table metadata]\n[shm_atomic header]\n  \u251c\u2500 atomic&lt;T&gt; value\n  \u2514\u2500 padding (cache line alignment)\n</code></pre>"},{"location":"data_structures/shm_atomic/#key-features","title":"Key Features","text":"<ul> <li>Cache Line Alignment: Prevents false sharing between processes</li> <li>Memory Ordering: Configurable memory ordering (relaxed, acquire, release, seq_cst)</li> <li>Type Safety: Compile-time checks for trivially copyable types</li> <li>Zero Overhead: Direct hardware atomic operations</li> </ul>"},{"location":"data_structures/shm_atomic/#api-reference","title":"API Reference","text":""},{"location":"data_structures/shm_atomic/#construction","title":"Construction","text":"<pre><code>// Create new atomic value in shared memory\nposix_shm shm(\"/simulation\", 1024 * 1024);\nshm_atomic&lt;uint64_t&gt; counter(shm, \"particle_count\", 0);\n\n// Open existing atomic value\nshm_atomic&lt;uint64_t&gt; existing(shm, \"particle_count\");\n</code></pre>"},{"location":"data_structures/shm_atomic/#basic-operations","title":"Basic Operations","text":"<pre><code>// Store (atomically write)\ncounter.store(1000);\n\n// Load (atomically read)\nuint64_t count = counter.load();\n\n// Exchange (swap values atomically)\nuint64_t old = counter.exchange(2000);\n\n// Compare and swap (CAS)\nuint64_t expected = 1000;\nbool success = counter.compare_exchange_strong(expected, 2000);\n</code></pre>"},{"location":"data_structures/shm_atomic/#arithmetic-operations-for-integral-types","title":"Arithmetic Operations (for integral types)","text":"<pre><code>// Atomic increment/decrement\ncounter.fetch_add(1);    // Returns old value\ncounter.fetch_sub(1);\n\n// Atomic bitwise operations  \ncounter.fetch_and(0xFF);\ncounter.fetch_or(0x100);\ncounter.fetch_xor(0x200);\n\n// Operators (return new value)\n++counter;  // Pre-increment\ncounter++;  // Post-increment\ncounter += 5;\n</code></pre>"},{"location":"data_structures/shm_atomic/#use-cases-in-n-body-simulation","title":"Use Cases in N-Body Simulation","text":""},{"location":"data_structures/shm_atomic/#1-global-particle-counter","title":"1. Global Particle Counter","text":"<pre><code>// Process 1: Particle generator\nshm_atomic&lt;size_t&gt; total_particles(shm, \"total_particles\", 0);\nfor (int i = 0; i &lt; batch_size; ++i) {\n    create_particle();\n    total_particles.fetch_add(1, std::memory_order_relaxed);\n}\n\n// Process 2: Monitor\nwhile (running) {\n    size_t count = total_particles.load(std::memory_order_relaxed);\n    std::cout &lt;&lt; \"Particles: \" &lt;&lt; count &lt;&lt; std::endl;\n    sleep(1);\n}\n</code></pre>"},{"location":"data_structures/shm_atomic/#2-synchronization-barrier","title":"2. Synchronization Barrier","text":"<pre><code>shm_atomic&lt;uint32_t&gt; barrier(shm, \"sync_barrier\", 0);\nconst uint32_t num_processes = 4;\n\n// Each process increments and waits\nuint32_t my_turn = barrier.fetch_add(1) + 1;\nif (my_turn == num_processes) {\n    // Last process resets for next barrier\n    barrier.store(0);\n    // Wake others...\n} else {\n    // Wait for barrier completion\n    while (barrier.load() != 0) {\n        std::this_thread::yield();\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_atomic/#3-lock-free-statistics","title":"3. Lock-Free Statistics","text":"<pre><code>struct SimStats {\n    shm_atomic&lt;uint64_t&gt; collisions;\n    shm_atomic&lt;double&gt; total_energy;\n    shm_atomic&lt;uint64_t&gt; iterations;\n};\n\n// Multiple processes update statistics concurrently\nstats.collisions.fetch_add(local_collisions);\nstats.iterations.fetch_add(1);\n\n// Atomic floating-point requires CAS loop\ndouble current = stats.total_energy.load();\ndouble desired;\ndo {\n    desired = current + local_energy;\n} while (!stats.total_energy.compare_exchange_weak(current, desired));\n</code></pre>"},{"location":"data_structures/shm_atomic/#4-state-machine-coordination","title":"4. State Machine Coordination","text":"<pre><code>enum SimState : uint32_t {\n    INITIALIZING = 0,\n    RUNNING = 1,\n    PAUSED = 2,\n    STOPPING = 3\n};\n\nshm_atomic&lt;SimState&gt; state(shm, \"sim_state\", INITIALIZING);\n\n// Controller process\nstate.store(RUNNING);\n\n// Worker processes\nwhile (state.load() == RUNNING) {\n    simulate_timestep();\n}\n</code></pre>"},{"location":"data_structures/shm_atomic/#performance-considerations","title":"Performance Considerations","text":""},{"location":"data_structures/shm_atomic/#memory-ordering","title":"Memory Ordering","text":"<p>Choose the weakest ordering that maintains correctness:</p> <pre><code>// Relaxed: No synchronization, only atomicity\ncounter.fetch_add(1, std::memory_order_relaxed);  // Fastest\n\n// Acquire-Release: Synchronizes with other atomic operations\nflag.store(true, std::memory_order_release);\nif (flag.load(std::memory_order_acquire)) { ... }\n\n// Sequential Consistency: Total order across all threads\nstate.store(READY, std::memory_order_seq_cst);  // Slowest but safest\n</code></pre>"},{"location":"data_structures/shm_atomic/#cache-coherency","title":"Cache Coherency","text":"<ul> <li>False Sharing: Ensure atomics in different cache lines</li> <li>Contention: Use relaxed ordering for high-frequency updates</li> <li>NUMA: Consider process-to-CPU pinning for large systems</li> </ul>"},{"location":"data_structures/shm_atomic/#lock-free-vs-wait-free","title":"Lock-Free vs Wait-Free","text":"<p><code>shm_atomic</code> operations are: - Lock-free: At least one thread makes progress - Wait-free: Every thread completes in bounded time (for most operations)</p> <p>CAS loops are lock-free but not wait-free: <pre><code>// Lock-free but not wait-free\nwhile (!atomic.compare_exchange_weak(expected, desired)) {\n    expected = atomic.load();\n}\n</code></pre></p>"},{"location":"data_structures/shm_atomic/#integration-with-other-data-structures","title":"Integration with Other Data Structures","text":"<p><code>shm_atomic</code> forms the basis for more complex lock-free structures:</p> <ol> <li>shm_queue: Uses atomic head/tail pointers</li> <li>shm_stack: Uses atomic top pointer  </li> <li>shm_object_pool: Uses atomic free list</li> <li>shm_ring_buffer: Uses atomic read/write indices</li> </ol> <p>Example building a simple spinlock: <pre><code>class shm_spinlock {\n    shm_atomic&lt;bool&gt; locked;\npublic:\n    void lock() {\n        bool expected = false;\n        while (!locked.compare_exchange_weak(expected, true)) {\n            expected = false;\n            std::this_thread::yield();\n        }\n    }\n\n    void unlock() {\n        locked.store(false);\n    }\n};\n</code></pre></p>"},{"location":"data_structures/shm_atomic/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"data_structures/shm_atomic/#1-aba-problem","title":"1. ABA Problem","text":"<pre><code>// Thread 1 reads A\nT* old_head = head.load();\n// Thread 2: changes A\u2192B\u2192A\n// Thread 1: CAS succeeds but state has changed!\nhead.compare_exchange_strong(old_head, new_head);\n</code></pre> <p>Solution: Use hazard pointers or epoch-based reclamation.</p>"},{"location":"data_structures/shm_atomic/#2-memory-ordering-bugs","title":"2. Memory Ordering Bugs","text":"<pre><code>// WRONG: data race\ndata = 42;\nflag.store(true, std::memory_order_relaxed);  \n\n// CORRECT: proper synchronization\ndata = 42;\nflag.store(true, std::memory_order_release);\n</code></pre>"},{"location":"data_structures/shm_atomic/#3-overflowunderflow","title":"3. Overflow/Underflow","text":"<pre><code>shm_atomic&lt;uint32_t&gt; counter(shm, \"counter\");\n// Check for overflow\nuint32_t old = counter.load();\nif (old == UINT32_MAX) {\n    // Handle overflow\n}\ncounter.fetch_add(1);\n</code></pre>"},{"location":"data_structures/shm_atomic/#testing-strategies","title":"Testing Strategies","text":""},{"location":"data_structures/shm_atomic/#unit-tests","title":"Unit Tests","text":"<pre><code>TEST_CASE(\"shm_atomic basic operations\") {\n    posix_shm shm(\"/test\", 1024);\n    shm_atomic&lt;int&gt; atom(shm, \"test_atom\", 42);\n\n    REQUIRE(atom.load() == 42);\n    REQUIRE(atom.exchange(100) == 42);\n    REQUIRE(atom.load() == 100);\n\n    int expected = 100;\n    REQUIRE(atom.compare_exchange_strong(expected, 200));\n    REQUIRE(atom.load() == 200);\n}\n</code></pre>"},{"location":"data_structures/shm_atomic/#stress-testing","title":"Stress Testing","text":"<pre><code>TEST_CASE(\"shm_atomic concurrent increments\") {\n    shm_atomic&lt;uint64_t&gt; counter(shm, \"counter\", 0);\n    const int num_threads = 8;\n    const int increments = 1000000;\n\n    std::vector&lt;std::thread&gt; threads;\n    for (int i = 0; i &lt; num_threads; ++i) {\n        threads.emplace_back([&amp;]() {\n            for (int j = 0; j &lt; increments; ++j) {\n                counter.fetch_add(1, std::memory_order_relaxed);\n            }\n        });\n    }\n\n    for (auto&amp; t : threads) t.join();\n    REQUIRE(counter.load() == num_threads * increments);\n}\n</code></pre>"},{"location":"data_structures/shm_atomic/#further-reading","title":"Further Reading","text":"<ul> <li>C++ Memory Model</li> <li>Lock-Free Programming</li> <li>The Art of Multiprocessor Programming</li> <li>Intel x86 Memory Ordering</li> </ul>"},{"location":"data_structures/shm_atomic/#next-building-on-atomics","title":"Next: Building on Atomics","text":"<p>With <code>shm_atomic</code> as our foundation, we can build increasingly sophisticated data structures:</p> <ol> <li>shm_queue: FIFO queue with atomic head/tail</li> <li>shm_stack: LIFO stack with atomic top</li> <li>shm_ring_buffer: Circular buffer for streaming</li> <li>shm_object_pool: Lock-free memory pool</li> </ol> <p>Each builds on atomic operations to provide safe, efficient inter-process communication for high-performance computing applications.</p>"},{"location":"data_structures/shm_object_pool/","title":"shm_object_pool: Lock-Free Memory Pool for Dynamic Allocation","text":""},{"location":"data_structures/shm_object_pool/#overview","title":"Overview","text":"<p><code>shm_object_pool&lt;T&gt;</code> implements a lock-free object pool in shared memory, providing fast, deterministic allocation and deallocation of fixed-size objects. It eliminates heap fragmentation and allocation overhead, making it ideal for high-frequency object creation/destruction in simulations, particularly for particles, collision events, and temporary computation structures.</p>"},{"location":"data_structures/shm_object_pool/#architecture","title":"Architecture","text":""},{"location":"data_structures/shm_object_pool/#memory-layout","title":"Memory Layout","text":"<pre><code>[shm_table metadata]\n[PoolHeader]\n  \u251c\u2500 atomic&lt;uint32_t&gt; free_list_head  // Head of free list\n  \u251c\u2500 size_t capacity                  // Total objects\n  \u251c\u2500 size_t object_size               // sizeof(T) + metadata\n  \u251c\u2500 atomic&lt;uint64_t&gt; allocations     // Statistics\n  \u251c\u2500 atomic&lt;uint64_t&gt; deallocations   // Statistics\n  \u2514\u2500 padding[32]                      // Cache alignment\n[Block][Block][Block]...[Block]       // Fixed-size blocks\n\nBlock Layout:\n[atomic&lt;uint32_t&gt; next]  // Next free block index\n[T object]               // Actual object storage\n[padding]                // Alignment padding\n</code></pre>"},{"location":"data_structures/shm_object_pool/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li>Free List: Lock-free stack of available blocks</li> <li>ABA Prevention: Generation counters in pointers</li> <li>Fixed Size: No fragmentation, O(1) allocation</li> <li>Lazy Initialization: Objects constructed on allocation</li> <li>Memory Tagging: Debug mode tracks allocations</li> </ol>"},{"location":"data_structures/shm_object_pool/#api-reference","title":"API Reference","text":""},{"location":"data_structures/shm_object_pool/#construction","title":"Construction","text":"<pre><code>// Create new pool\nposix_shm shm(\"/simulation\", 100 * 1024 * 1024);\nshm_object_pool&lt;Particle&gt; particle_pool(shm, \"particles\", 100000);\n\n// Open existing pool\nshm_object_pool&lt;Particle&gt; existing(shm, \"particles\");\n\n// Check existence\nif (shm_object_pool&lt;Particle&gt;::exists(shm, \"particles\")) {\n    shm_object_pool&lt;Particle&gt; pool(shm, \"particles\");\n}\n</code></pre>"},{"location":"data_structures/shm_object_pool/#allocation-and-deallocation","title":"Allocation and Deallocation","text":"<pre><code>// Allocate object (returns handle)\nauto handle = pool.allocate();\nif (handle) {\n    Particle&amp; p = pool.get(handle);\n    p.initialize();\n}\n\n// Allocate with constructor arguments\nauto handle = pool.emplace(mass, position, velocity);\n\n// Deallocate\npool.deallocate(handle);\n\n// Get object from handle\nParticle&amp; particle = pool.get(handle);\nconst Particle&amp; particle = pool.get(handle);\n\n// Check if handle is valid\nif (pool.is_valid(handle)) {\n    process(pool.get(handle));\n}\n</code></pre>"},{"location":"data_structures/shm_object_pool/#pool-management","title":"Pool Management","text":"<pre><code>// Pool statistics\nsize_t used = pool.used_count();\nsize_t free = pool.free_count();\nsize_t capacity = pool.capacity();\ndouble utilization = pool.utilization();\n\n// Bulk operations\nstd::vector&lt;Handle&gt; handles;\nsize_t allocated = pool.allocate_batch(100, handles);\n\npool.deallocate_batch(handles);\n\n// Reset pool (deallocate all)\npool.reset();\n</code></pre>"},{"location":"data_structures/shm_object_pool/#lock-free-algorithm","title":"Lock-Free Algorithm","text":""},{"location":"data_structures/shm_object_pool/#allocation-free-list-pop","title":"Allocation (Free List Pop)","text":"<pre><code>Handle allocate() {\n    uint64_t old_head = free_list_head.load();\n\n    do {\n        uint32_t index = get_index(old_head);\n        uint32_t gen = get_generation(old_head);\n\n        if (index == NULL_INDEX) {\n            return Handle{};  // Pool exhausted\n        }\n\n        // Read next pointer\n        Block&amp; block = blocks[index];\n        uint32_t next = block.next.load();\n\n        // Create new head with incremented generation\n        uint64_t new_head = make_pointer(next, gen + 1);\n\n        // Try to swing head to next\n        if (free_list_head.compare_exchange_weak(old_head, new_head)) {\n            allocations.fetch_add(1);\n            return Handle{index, gen};\n        }\n        // CAS failed, retry with updated old_head\n    } while (true);\n}\n</code></pre>"},{"location":"data_structures/shm_object_pool/#deallocation-free-list-push","title":"Deallocation (Free List Push)","text":"<pre><code>void deallocate(Handle handle) {\n    uint32_t index = handle.index;\n    uint64_t old_head = free_list_head.load();\n\n    do {\n        uint32_t old_index = get_index(old_head);\n        uint32_t gen = get_generation(old_head);\n\n        // Set block's next to current head\n        blocks[index].next.store(old_index);\n\n        // Create new head pointing to deallocated block\n        uint64_t new_head = make_pointer(index, gen + 1);\n\n        // Try to swing head to deallocated block\n        if (free_list_head.compare_exchange_weak(old_head, new_head)) {\n            deallocations.fetch_add(1);\n            return;\n        }\n        // CAS failed, retry\n    } while (true);\n}\n</code></pre>"},{"location":"data_structures/shm_object_pool/#aba-prevention","title":"ABA Prevention","text":"<pre><code>// 64-bit pointer: [32-bit generation][32-bit index]\nstruct TaggedPointer {\n    uint32_t index : 32;\n    uint32_t generation : 32;\n};\n\nuint64_t make_pointer(uint32_t idx, uint32_t gen) {\n    return (uint64_t(gen) &lt;&lt; 32) | idx;\n}\n</code></pre>"},{"location":"data_structures/shm_object_pool/#use-cases-in-n-body-simulation","title":"Use Cases in N-Body Simulation","text":""},{"location":"data_structures/shm_object_pool/#1-dynamic-particle-management","title":"1. Dynamic Particle Management","text":"<pre><code>class ParticleSystem {\n    shm_object_pool&lt;Particle&gt; pool;\n    shm_array&lt;Handle&gt; active_particles;\n\npublic:\n    Handle spawn_particle(float3 pos, float3 vel, float mass) {\n        auto handle = pool.emplace(pos, vel, mass);\n        if (handle) {\n            active_particles.push_back(handle);\n\n            // Initialize particle physics\n            Particle&amp; p = pool.get(handle);\n            p.force = compute_initial_force(p);\n            p.lifetime = MAX_LIFETIME;\n        }\n        return handle;\n    }\n\n    void destroy_particle(Handle handle) {\n        // Remove from active list\n        auto it = std::find(active_particles.begin(), \n                           active_particles.end(), handle);\n        if (it != active_particles.end()) {\n            *it = active_particles.back();\n            active_particles.pop_back();\n        }\n\n        // Return to pool\n        pool.deallocate(handle);\n    }\n\n    void update(float dt) {\n        // Process particles, destroy expired ones\n        for (size_t i = 0; i &lt; active_particles.size(); ) {\n            Handle h = active_particles[i];\n            Particle&amp; p = pool.get(h);\n\n            p.lifetime -= dt;\n            if (p.lifetime &lt;= 0) {\n                destroy_particle(h);\n                // Don't increment i, we swapped with back\n            } else {\n                update_physics(p, dt);\n                ++i;\n            }\n        }\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_object_pool/#2-collision-event-pool","title":"2. Collision Event Pool","text":"<pre><code>struct CollisionEvent {\n    Handle particle1, particle2;\n    float3 position;\n    float time;\n    float impulse;\n};\n\nclass CollisionSystem {\n    shm_object_pool&lt;CollisionEvent&gt; event_pool;\n    shm_queue&lt;Handle&gt; pending_events;\n\npublic:\n    void detect_collisions() {\n        // Spatial hashing for broad phase\n        for (auto&amp; cell : spatial_grid) {\n            for (size_t i = 0; i &lt; cell.size(); ++i) {\n                for (size_t j = i + 1; j &lt; cell.size(); ++j) {\n                    if (will_collide(cell[i], cell[j])) {\n                        // Allocate collision event\n                        auto event_handle = event_pool.allocate();\n                        if (!event_handle) {\n                            // Pool exhausted, skip\n                            stats.dropped_collisions++;\n                            continue;\n                        }\n\n                        CollisionEvent&amp; event = event_pool.get(event_handle);\n                        event.particle1 = cell[i];\n                        event.particle2 = cell[j];\n                        event.position = compute_collision_point();\n                        event.time = compute_collision_time();\n                        event.impulse = compute_impulse();\n\n                        pending_events.enqueue(event_handle);\n                    }\n                }\n            }\n        }\n    }\n\n    void resolve_collisions() {\n        while (auto event_handle = pending_events.dequeue()) {\n            CollisionEvent&amp; event = event_pool.get(*event_handle);\n\n            // Apply impulses\n            apply_impulse(event.particle1, event.impulse);\n            apply_impulse(event.particle2, -event.impulse);\n\n            // Visual effects\n            spawn_collision_effect(event.position);\n\n            // Return event to pool\n            event_pool.deallocate(*event_handle);\n        }\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_object_pool/#3-temporary-computation-nodes","title":"3. Temporary Computation Nodes","text":"<pre><code>struct ComputeNode {\n    enum Type { FORCE, INTEGRATE, COLLISION };\n    Type type;\n    Handle input_particles[8];\n    Handle output_buffer;\n    std::function&lt;void()&gt; compute;\n};\n\nclass ComputeGraph {\n    shm_object_pool&lt;ComputeNode&gt; node_pool;\n    shm_dag&lt;Handle&gt; dependency_graph;\n\npublic:\n    Handle create_force_node(std::vector&lt;Handle&gt; particles) {\n        auto node_handle = node_pool.allocate();\n        ComputeNode&amp; node = node_pool.get(node_handle);\n\n        node.type = ComputeNode::FORCE;\n        std::copy(particles.begin(), particles.end(), node.input_particles);\n        node.compute = [&amp;]() {\n            compute_nbody_forces(node.input_particles);\n        };\n\n        dependency_graph.add_node(node_handle);\n        return node_handle;\n    }\n\n    void execute() {\n        dependency_graph.execute_parallel([this](Handle h) {\n            ComputeNode&amp; node = node_pool.get(h);\n            node.compute();\n        });\n\n        // Clean up executed nodes\n        for (auto h : dependency_graph.get_executed()) {\n            node_pool.deallocate(h);\n        }\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_object_pool/#4-memory-efficient-octree","title":"4. Memory-Efficient Octree","text":"<pre><code>template&lt;typename T&gt;\nclass PooledOctree {\n    struct Node {\n        T data;\n        Handle children[8];\n        float3 center;\n        float size;\n        bool is_leaf;\n    };\n\n    shm_object_pool&lt;Node&gt; node_pool;\n    Handle root;\n\npublic:\n    Handle create_node(const T&amp; data, float3 center, float size) {\n        auto handle = node_pool.allocate();\n        Node&amp; node = node_pool.get(handle);\n\n        node.data = data;\n        node.center = center;\n        node.size = size;\n        node.is_leaf = true;\n        std::fill(node.children, node.children + 8, Handle{});\n\n        return handle;\n    }\n\n    void subdivide(Handle node_handle) {\n        Node&amp; node = node_pool.get(node_handle);\n        if (!node.is_leaf) return;\n\n        float half_size = node.size / 2;\n        for (int i = 0; i &lt; 8; ++i) {\n            float3 child_center = compute_octant_center(node.center, half_size, i);\n            node.children[i] = create_node(T{}, child_center, half_size);\n        }\n\n        node.is_leaf = false;\n    }\n\n    void collapse_empty_nodes() {\n        std::function&lt;bool(Handle)&gt; collapse = [&amp;](Handle h) -&gt; bool {\n            if (!h) return true;\n\n            Node&amp; node = node_pool.get(h);\n            if (node.is_leaf) {\n                return node.data.is_empty();\n            }\n\n            bool all_empty = true;\n            for (int i = 0; i &lt; 8; ++i) {\n                if (collapse(node.children[i])) {\n                    if (node.children[i]) {\n                        node_pool.deallocate(node.children[i]);\n                        node.children[i] = Handle{};\n                    }\n                } else {\n                    all_empty = false;\n                }\n            }\n\n            if (all_empty) {\n                node.is_leaf = true;\n            }\n            return all_empty &amp;&amp; node.data.is_empty();\n        };\n\n        collapse(root);\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_object_pool/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"data_structures/shm_object_pool/#1-hazard-pointers-for-safe-reclamation","title":"1. Hazard Pointers for Safe Reclamation","text":"<pre><code>template&lt;typename T&gt;\nclass HazardPool {\n    shm_object_pool&lt;T&gt; pool;\n    shm_array&lt;std::atomic&lt;Handle&gt;&gt; hazard_pointers;\n    shm_queue&lt;Handle&gt; retire_list;\n\npublic:\n    class HazardGuard {\n        std::atomic&lt;Handle&gt;&amp; hp;\n    public:\n        HazardGuard(std::atomic&lt;Handle&gt;&amp; hazard, Handle h) \n            : hp(hazard) {\n            hp.store(h);\n        }\n        ~HazardGuard() {\n            hp.store(Handle{});\n        }\n    };\n\n    void retire(Handle h) {\n        retire_list.enqueue(h);\n\n        if (retire_list.size() &gt; 2 * hazard_pointers.size()) {\n            scan();\n        }\n    }\n\n    void scan() {\n        // Collect hazard pointers\n        std::set&lt;Handle&gt; hazards;\n        for (auto&amp; hp : hazard_pointers) {\n            Handle h = hp.load();\n            if (h) hazards.insert(h);\n        }\n\n        // Reclaim non-hazardous objects\n        std::queue&lt;Handle&gt; tmp;\n        while (auto h = retire_list.dequeue()) {\n            if (hazards.find(*h) == hazards.end()) {\n                pool.deallocate(*h);\n            } else {\n                tmp.push(*h);\n            }\n        }\n\n        // Re-enqueue still hazardous\n        while (!tmp.empty()) {\n            retire_list.enqueue(tmp.front());\n            tmp.pop();\n        }\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_object_pool/#2-typed-pool-with-variants","title":"2. Typed Pool with Variants","text":"<pre><code>class VariantPool {\n    struct Block {\n        enum Type { PARTICLE, FORCE, COLLISION, EMPTY };\n        std::atomic&lt;Type&gt; type{EMPTY};\n        union {\n            Particle particle;\n            Force force;\n            CollisionEvent collision;\n        };\n    };\n\n    shm_object_pool&lt;Block&gt; pool;\n\npublic:\n    template&lt;typename T&gt;\n    Handle allocate() {\n        auto h = pool.allocate();\n        if (h) {\n            Block&amp; b = pool.get(h);\n            if constexpr (std::is_same_v&lt;T, Particle&gt;) {\n                new (&amp;b.particle) Particle();\n                b.type = Block::PARTICLE;\n            } else if constexpr (std::is_same_v&lt;T, Force&gt;) {\n                new (&amp;b.force) Force();\n                b.type = Block::FORCE;\n            }\n            // ...\n        }\n        return h;\n    }\n\n    template&lt;typename T&gt;\n    T&amp; get(Handle h) {\n        Block&amp; b = pool.get(h);\n        if constexpr (std::is_same_v&lt;T, Particle&gt;) {\n            assert(b.type == Block::PARTICLE);\n            return b.particle;\n        }\n        // ...\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_object_pool/#3-generational-handles","title":"3. Generational Handles","text":"<pre><code>template&lt;typename T&gt;\nclass GenerationalPool {\n    struct Slot {\n        T object;\n        uint32_t generation;\n        bool allocated;\n    };\n\n    shm_array&lt;Slot&gt; slots;\n    shm_stack&lt;uint32_t&gt; free_indices;\n\npublic:\n    struct Handle {\n        uint32_t index;\n        uint32_t generation;\n\n        bool operator==(const Handle&amp; other) const {\n            return index == other.index &amp;&amp; generation == other.generation;\n        }\n    };\n\n    Handle allocate() {\n        if (auto idx = free_indices.pop()) {\n            Slot&amp; slot = slots[*idx];\n            slot.allocated = true;\n            slot.generation++;\n            return Handle{*idx, slot.generation};\n        }\n        return Handle{INVALID_INDEX, 0};\n    }\n\n    void deallocate(Handle h) {\n        Slot&amp; slot = slots[h.index];\n        if (slot.generation == h.generation &amp;&amp; slot.allocated) {\n            slot.allocated = false;\n            free_indices.push(h.index);\n        }\n    }\n\n    T* get(Handle h) {\n        Slot&amp; slot = slots[h.index];\n        if (slot.generation == h.generation &amp;&amp; slot.allocated) {\n            return &amp;slot.object;\n        }\n        return nullptr;\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_object_pool/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"data_structures/shm_object_pool/#allocation-performance","title":"Allocation Performance","text":"Threads Throughput Latency CAS Retries 1 100M/sec 10ns 0 2 60M/sec 33ns 0.2 4 40M/sec 100ns 1.5 8 25M/sec 320ns 5.2"},{"location":"data_structures/shm_object_pool/#memory-efficiency","title":"Memory Efficiency","text":"<pre><code>// Memory overhead per object\nsize_t overhead = sizeof(uint32_t);  // Next pointer\nsize_t padding = alignof(T) - 1;     // Worst case alignment\nsize_t total_per_object = sizeof(T) + overhead + padding;\n\n// Pool efficiency\ndouble efficiency = sizeof(T) / double(total_per_object);\n// Typically 85-95% for reasonable object sizes\n</code></pre>"},{"location":"data_structures/shm_object_pool/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"data_structures/shm_object_pool/#1-handle-invalidation","title":"1. Handle Invalidation","text":"<pre><code>// WRONG: Using handle after deallocation\nHandle h = pool.allocate();\npool.deallocate(h);\nParticle&amp; p = pool.get(h);  // Undefined behavior!\n\n// CORRECT: Check validity\nif (pool.is_valid(h)) {\n    Particle&amp; p = pool.get(h);\n}\n</code></pre>"},{"location":"data_structures/shm_object_pool/#2-memory-leaks","title":"2. Memory Leaks","text":"<pre><code>// WRONG: Forgetting to deallocate\nfor (int i = 0; i &lt; 1000; ++i) {\n    Handle h = pool.allocate();\n    // Never deallocated!\n}\n\n// CORRECT: RAII wrapper\nclass PoolGuard {\n    Pool&amp; pool;\n    Handle h;\npublic:\n    PoolGuard(Pool&amp; p) : pool(p), h(p.allocate()) {}\n    ~PoolGuard() { if (h) pool.deallocate(h); }\n    Handle release() { Handle tmp = h; h = {}; return tmp; }\n};\n</code></pre>"},{"location":"data_structures/shm_object_pool/#3-aba-problem","title":"3. ABA Problem","text":"<pre><code>// WRONG: Simple pointer without generation\nstruct BadHandle {\n    uint32_t index;\n};\n// Object freed and reallocated - same index!\n\n// CORRECT: Generation counter\nstruct GoodHandle {\n    uint32_t index;\n    uint32_t generation;\n};\n// Generation changes on each allocation\n</code></pre>"},{"location":"data_structures/shm_object_pool/#testing-strategies","title":"Testing Strategies","text":"<pre><code>TEST_CASE(\"Pool handles allocation storms\") {\n    shm_object_pool&lt;TestObject&gt; pool(shm, \"test\", 10000);\n    std::atomic&lt;size_t&gt; total_allocations{0};\n\n    std::vector&lt;std::thread&gt; threads;\n    for (int t = 0; t &lt; 8; ++t) {\n        threads.emplace_back([&amp;]() {\n            std::vector&lt;Handle&gt; handles;\n            for (int iter = 0; iter &lt; 1000; ++iter) {\n                // Allocation phase\n                for (int i = 0; i &lt; 100; ++i) {\n                    if (auto h = pool.allocate()) {\n                        handles.push_back(h);\n                        total_allocations++;\n                    }\n                }\n\n                // Deallocation phase\n                for (auto h : handles) {\n                    pool.deallocate(h);\n                }\n                handles.clear();\n            }\n        });\n    }\n\n    for (auto&amp; t : threads) t.join();\n\n    REQUIRE(pool.free_count() == pool.capacity());\n    REQUIRE(pool.stats().allocations == total_allocations);\n    REQUIRE(pool.stats().deallocations == total_allocations);\n}\n</code></pre>"},{"location":"data_structures/shm_object_pool/#references","title":"References","text":"<ul> <li>The Slab Allocator: An Object-Caching Kernel Memory Allocator - Bonwick</li> <li>Lock-Free Memory Pool - Implementation reference</li> <li>Hazard Pointers: Safe Memory Reclamation - Michael</li> </ul>"},{"location":"data_structures/shm_queue/","title":"shm_queue: Lock-Free FIFO Queue for Shared Memory","text":""},{"location":"data_structures/shm_queue/#overview","title":"Overview","text":"<p><code>shm_queue&lt;T&gt;</code> implements a lock-free, bounded FIFO (First-In-First-Out) queue in POSIX shared memory. It enables efficient producer-consumer patterns across processes without kernel-level synchronization primitives.</p>"},{"location":"data_structures/shm_queue/#architecture","title":"Architecture","text":""},{"location":"data_structures/shm_queue/#memory-layout","title":"Memory Layout","text":"<pre><code>[shm_table metadata]\n[QueueHeader]\n  \u251c\u2500 atomic&lt;size_t&gt; head     // Next position to dequeue\n  \u251c\u2500 atomic&lt;size_t&gt; tail     // Next position to enqueue  \n  \u2514\u2500 size_t capacity         // Maximum elements + 1\n[T][T][T]...[T]              // Circular buffer of elements\n</code></pre>"},{"location":"data_structures/shm_queue/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li>Circular Buffer: Wraps around to reuse memory efficiently</li> <li>One Empty Slot: Distinguishes full from empty (when head == tail)</li> <li>Lock-Free Algorithm: Michael &amp; Scott's algorithm adapted for bounded queues</li> <li>Cache-Line Alignment: Head and tail in separate cache lines to prevent false sharing</li> </ol>"},{"location":"data_structures/shm_queue/#api-reference","title":"API Reference","text":""},{"location":"data_structures/shm_queue/#construction","title":"Construction","text":"<pre><code>// Create new queue\nposix_shm shm(\"/simulation\", 10 * 1024 * 1024);\nshm_queue&lt;Task&gt; task_queue(shm, \"tasks\", 1000);  // capacity = 1000\n\n// Open existing queue\nshm_queue&lt;Task&gt; existing(shm, \"tasks\", 0);  // capacity = 0 means open\n\n// Check if exists before opening\nif (shm_queue&lt;Task&gt;::exists(shm, \"tasks\")) {\n    shm_queue&lt;Task&gt; queue(shm, \"tasks\");\n}\n</code></pre>"},{"location":"data_structures/shm_queue/#basic-operations","title":"Basic Operations","text":"<pre><code>// Enqueue (returns false if full)\nTask task{...};\nif (!queue.enqueue(task)) {\n    // Queue is full, handle backpressure\n}\n\n// Dequeue (returns optional)\nauto task = queue.dequeue();\nif (task.has_value()) {\n    process(*task);\n}\n\n// Alternative dequeue with output parameter\nTask task;\nif (queue.dequeue(task)) {\n    process(task);\n}\n</code></pre>"},{"location":"data_structures/shm_queue/#queue-state","title":"Queue State","text":"<pre><code>size_t count = queue.size();      // Current number of elements\nsize_t cap = queue.capacity();    // Maximum capacity\nbool is_empty = queue.empty();    // True if no elements\nbool is_full = queue.full();      // True if at capacity\n</code></pre>"},{"location":"data_structures/shm_queue/#lock-free-algorithm","title":"Lock-Free Algorithm","text":""},{"location":"data_structures/shm_queue/#enqueue-operation","title":"Enqueue Operation","text":"<pre><code>bool enqueue(const T&amp; value) {\n    size_t current_tail = tail.load(memory_order_relaxed);\n    size_t next_tail = (current_tail + 1) % capacity;\n\n    // Check if queue is full\n    if (next_tail == head.load(memory_order_acquire)) {\n        return false;  // Queue full\n    }\n\n    // Write data\n    buffer[current_tail] = value;\n\n    // Update tail (publish)\n    tail.store(next_tail, memory_order_release);\n    return true;\n}\n</code></pre>"},{"location":"data_structures/shm_queue/#dequeue-operation","title":"Dequeue Operation","text":"<pre><code>optional&lt;T&gt; dequeue() {\n    size_t current_head = head.load(memory_order_relaxed);\n\n    // Check if queue is empty\n    if (current_head == tail.load(memory_order_acquire)) {\n        return nullopt;  // Queue empty\n    }\n\n    // Read data\n    T value = buffer[current_head];\n\n    // Update head\n    size_t next_head = (current_head + 1) % capacity;\n    head.store(next_head, memory_order_release);\n\n    return value;\n}\n</code></pre>"},{"location":"data_structures/shm_queue/#memory-ordering","title":"Memory Ordering","text":"<ul> <li>Acquire-Release: Ensures proper synchronization between producers and consumers</li> <li>Relaxed: Used for local operations that don't need synchronization</li> <li>No CAS needed for single-producer or single-consumer scenarios</li> </ul>"},{"location":"data_structures/shm_queue/#use-cases-in-n-body-simulation","title":"Use Cases in N-Body Simulation","text":""},{"location":"data_structures/shm_queue/#1-work-distribution","title":"1. Work Distribution","text":"<pre><code>// Coordinator process\nshm_queue&lt;WorkUnit&gt; work_queue(shm, \"work\", 10000);\n\nvoid distribute_work() {\n    for (size_t i = 0; i &lt; num_particles; i += batch_size) {\n        WorkUnit unit{\n            .start_idx = i,\n            .end_idx = min(i + batch_size, num_particles),\n            .timestep = current_time\n        };\n\n        while (!work_queue.enqueue(unit)) {\n            // Queue full, wait for workers\n            std::this_thread::yield();\n        }\n    }\n}\n\n// Worker processes\nvoid worker() {\n    shm_queue&lt;WorkUnit&gt; work_queue(shm, \"work\");\n\n    while (running) {\n        auto work = work_queue.dequeue();\n        if (work.has_value()) {\n            compute_forces(*work);\n        } else {\n            // No work available\n            std::this_thread::yield();\n        }\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_queue/#2-event-processing","title":"2. Event Processing","text":"<pre><code>struct CollisionEvent {\n    uint32_t particle1;\n    uint32_t particle2;\n    float time;\n    float3 position;\n};\n\nshm_queue&lt;CollisionEvent&gt; events(shm, \"collisions\", 1000);\n\n// Physics processes detect and enqueue collisions\nvoid detect_collisions() {\n    for (auto&amp; pair : particle_pairs) {\n        if (will_collide(pair)) {\n            CollisionEvent event = compute_collision(pair);\n            if (!events.enqueue(event)) {\n                // Handle queue overflow\n                stats.dropped_events++;\n            }\n        }\n    }\n}\n\n// Visualization process consumes events\nvoid visualize() {\n    while (auto event = events.dequeue()) {\n        render_collision_effect(*event);\n        update_statistics(*event);\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_queue/#3-command-pipeline","title":"3. Command Pipeline","text":"<pre><code>enum CommandType { START, STOP, PAUSE, STEP, RESET };\nstruct Command {\n    CommandType type;\n    uint64_t timestamp;\n    float parameters[4];\n};\n\nshm_queue&lt;Command&gt; cmd_queue(shm, \"commands\", 100);\n\n// Control interface\nvoid handle_user_input(CommandType cmd) {\n    Command command{\n        .type = cmd,\n        .timestamp = get_timestamp(),\n        .parameters = {0}\n    };\n\n    if (!cmd_queue.enqueue(command)) {\n        log_error(\"Command queue full\");\n    }\n}\n\n// Simulation engine\nvoid process_commands() {\n    while (auto cmd = cmd_queue.dequeue()) {\n        switch (cmd-&gt;type) {\n            case START: start_simulation(); break;\n            case STOP:  stop_simulation(); break;\n            case PAUSE: pause_simulation(); break;\n            case STEP:  step_simulation(); break;\n            case RESET: reset_simulation(); break;\n        }\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_queue/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"data_structures/shm_queue/#throughput","title":"Throughput","text":"<ul> <li>Single Producer/Consumer: ~50M ops/sec on modern x86</li> <li>Multiple Producers/Consumers: ~20M ops/sec with 4 threads</li> <li>Cross-Process: ~15M ops/sec (includes cache coherency overhead)</li> </ul>"},{"location":"data_structures/shm_queue/#latency","title":"Latency","text":"<ul> <li>Best Case: ~10ns (data in L1 cache)</li> <li>Typical: ~50ns (cross-core communication)</li> <li>Worst Case: ~200ns (cache miss + coherency)</li> </ul>"},{"location":"data_structures/shm_queue/#scalability","title":"Scalability","text":"Producers Consumers Throughput Notes 1 1 Optimal No contention N 1 Good Tail contention only 1 N Good Head contention only N M Moderate Both ends contend"},{"location":"data_structures/shm_queue/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"data_structures/shm_queue/#1-batched-operations","title":"1. Batched Operations","text":"<pre><code>template&lt;size_t BatchSize&gt;\nclass BatchedQueue {\n    shm_queue&lt;std::array&lt;T, BatchSize&gt;&gt; queue;\n    std::array&lt;T, BatchSize&gt; write_batch;\n    size_t write_idx = 0;\n\npublic:\n    void enqueue(const T&amp; item) {\n        write_batch[write_idx++] = item;\n        if (write_idx == BatchSize) {\n            queue.enqueue(write_batch);\n            write_idx = 0;\n        }\n    }\n\n    void flush() {\n        if (write_idx &gt; 0) {\n            // Partial batch\n            queue.enqueue(write_batch);  \n            write_idx = 0;\n        }\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_queue/#2-priority-queue","title":"2. Priority Queue","text":"<pre><code>template&lt;typename T, size_t NumPriorities&gt;\nclass PriorityQueue {\n    std::array&lt;shm_queue&lt;T&gt;, NumPriorities&gt; queues;\n\npublic:\n    bool enqueue(const T&amp; item, size_t priority) {\n        return queues[priority].enqueue(item);\n    }\n\n    std::optional&lt;T&gt; dequeue() {\n        // Check high priority first\n        for (auto&amp; q : queues) {\n            if (auto item = q.dequeue()) {\n                return item;\n            }\n        }\n        return std::nullopt;\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_queue/#3-blocking-wrapper","title":"3. Blocking Wrapper","text":"<pre><code>template&lt;typename T&gt;\nclass BlockingQueue {\n    shm_queue&lt;T&gt;&amp; queue;\n    shm_atomic&lt;uint32_t&gt; waiters;\n\npublic:\n    void enqueue_blocking(const T&amp; item) {\n        while (!queue.enqueue(item)) {\n            waiters.fetch_add(1);\n            // Use futex or condition variable\n            wait_on_space();\n            waiters.fetch_sub(1);\n        }\n    }\n\n    T dequeue_blocking() {\n        while (true) {\n            if (auto item = queue.dequeue()) {\n                if (waiters.load() &gt; 0) {\n                    notify_waiters();\n                }\n                return *item;\n            }\n            wait_on_data();\n        }\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_queue/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"data_structures/shm_queue/#1-size-vs-capacity-confusion","title":"1. Size vs Capacity Confusion","text":"<pre><code>// WRONG: size() returns current count, not max\nif (queue.size() &lt; 100) {  \n    queue.enqueue(item);  // May still fail!\n}\n\n// CORRECT: Check return value\nif (!queue.enqueue(item)) {\n    // Handle full queue\n}\n</code></pre>"},{"location":"data_structures/shm_queue/#2-race-between-check-and-operation","title":"2. Race Between Check and Operation","text":"<pre><code>// WRONG: TOCTOU race\nif (!queue.full()) {\n    queue.enqueue(item);  // May fail if filled between checks\n}\n\n// CORRECT: Atomic operation\nif (!queue.enqueue(item)) {\n    handle_full();\n}\n</code></pre>"},{"location":"data_structures/shm_queue/#3-assuming-ordering-across-queues","title":"3. Assuming Ordering Across Queues","text":"<pre><code>// WRONG: No ordering guarantee between queues\nqueue1.enqueue(A);\nqueue2.enqueue(B);\n// B might be dequeued before A!\n\n// CORRECT: Use single queue or explicit synchronization\nqueue.enqueue({A, B});  // Atomic pair\n</code></pre>"},{"location":"data_structures/shm_queue/#testing-strategies","title":"Testing Strategies","text":""},{"location":"data_structures/shm_queue/#correctness-tests","title":"Correctness Tests","text":"<pre><code>TEST_CASE(\"Queue maintains FIFO order\") {\n    shm_queue&lt;int&gt; queue(shm, \"test\", 100);\n\n    for (int i = 0; i &lt; 50; ++i) {\n        REQUIRE(queue.enqueue(i));\n    }\n\n    for (int i = 0; i &lt; 50; ++i) {\n        auto val = queue.dequeue();\n        REQUIRE(val.has_value());\n        REQUIRE(*val == i);  // FIFO order\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_queue/#stress-tests","title":"Stress Tests","text":"<pre><code>TEST_CASE(\"Queue handles concurrent operations\") {\n    shm_queue&lt;int&gt; queue(shm, \"stress\", 1000);\n    std::atomic&lt;int&gt; sum_in{0}, sum_out{0};\n\n    // Multiple producers\n    std::vector&lt;std::thread&gt; producers;\n    for (int t = 0; t &lt; 4; ++t) {\n        producers.emplace_back([&amp;, t]() {\n            for (int i = 0; i &lt; 10000; ++i) {\n                int val = t * 10000 + i;\n                while (!queue.enqueue(val)) {\n                    std::this_thread::yield();\n                }\n                sum_in += val;\n            }\n        });\n    }\n\n    // Multiple consumers  \n    std::vector&lt;std::thread&gt; consumers;\n    for (int t = 0; t &lt; 4; ++t) {\n        consumers.emplace_back([&amp;]() {\n            int count = 0;\n            while (count &lt; 10000) {\n                if (auto val = queue.dequeue()) {\n                    sum_out += *val;\n                    count++;\n                }\n            }\n        });\n    }\n\n    for (auto&amp; t : producers) t.join();\n    for (auto&amp; t : consumers) t.join();\n\n    REQUIRE(sum_in == sum_out);  // No lost items\n}\n</code></pre>"},{"location":"data_structures/shm_queue/#comparison-with-alternatives","title":"Comparison with Alternatives","text":"Feature shm_queue std::queue boost::lockfree::queue POSIX mqueue Lock-free \u2713 \u2717 \u2713 \u2717 Bounded \u2713 \u2717 Optional \u2713 IPC \u2713 \u2717 \u2717 \u2713 Zero-copy \u2713 \u2713 \u2713 \u2717 Custom types \u2713 \u2713 \u2713 \u2717 Performance High Medium High Low"},{"location":"data_structures/shm_queue/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Unbounded Queue: Dynamic growth using linked segments</li> <li>Multi-Queue: Multiple queues with work-stealing</li> <li>Persistent Queue: Survive process crashes</li> <li>Compression: Automatic compression for large items</li> <li>Metrics: Built-in performance counters</li> </ol>"},{"location":"data_structures/shm_queue/#references","title":"References","text":"<ul> <li>Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms - Michael &amp; Scott</li> <li>A Wait-free Queue as Fast as Fetch-and-Add - Yang &amp; Mellor-Crummey</li> <li>The Baskets Queue - Hoffman et al.</li> </ul>"},{"location":"data_structures/shm_ring_buffer/","title":"shm_ring_buffer: High-Performance Circular Buffer for Streaming","text":""},{"location":"data_structures/shm_ring_buffer/#overview","title":"Overview","text":"<p><code>shm_ring_buffer&lt;T&gt;</code> implements a lock-free circular buffer optimized for streaming data between processes. It provides single-producer/single-consumer (SPSC) guarantees with wait-free operations, making it ideal for real-time data pipelines, sensor streams, and continuous telemetry in simulations.</p>"},{"location":"data_structures/shm_ring_buffer/#architecture","title":"Architecture","text":""},{"location":"data_structures/shm_ring_buffer/#memory-layout","title":"Memory Layout","text":"<pre><code>[shm_table metadata]\n[RingBufferHeader]\n  \u251c\u2500 atomic&lt;size_t&gt; write_pos  // Producer's write position\n  \u251c\u2500 char padding[60]          // Cache line separation\n  \u251c\u2500 atomic&lt;size_t&gt; read_pos   // Consumer's read position  \n  \u251c\u2500 size_t capacity           // Buffer size (power of 2)\n  \u2514\u2500 char padding[48]          // Cache line alignment\n[T][T][T]...[T]                // Circular data buffer\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li>Cache Line Separation: Read and write positions in different cache lines</li> <li>Power-of-2 Size: Enables fast modulo with bitwise AND</li> <li>SPSC Optimization: No CAS needed, just atomic loads/stores</li> <li>Overflow Modes: Configurable behavior when full</li> </ol>"},{"location":"data_structures/shm_ring_buffer/#api-reference","title":"API Reference","text":""},{"location":"data_structures/shm_ring_buffer/#construction","title":"Construction","text":"<pre><code>// Create new ring buffer\nposix_shm shm(\"/streaming\", 100 * 1024 * 1024);\nshm_ring_buffer&lt;SensorData&gt; sensor_stream(shm, \"sensors\", 65536);\n\n// Open existing buffer\nshm_ring_buffer&lt;SensorData&gt; existing(shm, \"sensors\");\n\n// Check existence\nif (shm_ring_buffer&lt;SensorData&gt;::exists(shm, \"sensors\")) {\n    shm_ring_buffer&lt;SensorData&gt; buffer(shm, \"sensors\");\n}\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#producer-api","title":"Producer API","text":"<pre><code>// Write single element\nbool success = buffer.write(data);\n\n// Write multiple elements\nstd::vector&lt;T&gt; batch = {...};\nsize_t written = buffer.write_batch(batch.data(), batch.size());\n\n// Get contiguous write space\nauto [ptr, size] = buffer.write_ptr();\nif (size &gt; 0) {\n    // Direct write to buffer\n    memcpy(ptr, source, size * sizeof(T));\n    buffer.commit_write(size);\n}\n\n// Check available space\nsize_t space = buffer.write_available();\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#consumer-api","title":"Consumer API","text":"<pre><code>// Read single element\nif (auto data = buffer.read()) {\n    process(*data);\n}\n\n// Read multiple elements\nstd::vector&lt;T&gt; batch(100);\nsize_t read = buffer.read_batch(batch.data(), batch.size());\n\n// Get contiguous read space\nauto [ptr, size] = buffer.read_ptr();\nif (size &gt; 0) {\n    // Direct read from buffer\n    process_batch(ptr, size);\n    buffer.commit_read(size);\n}\n\n// Check available data\nsize_t available = buffer.read_available();\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#buffer-state","title":"Buffer State","text":"<pre><code>bool is_empty = buffer.empty();\nbool is_full = buffer.full();\nsize_t current_size = buffer.size();\nsize_t capacity = buffer.capacity();\n\n// Reset positions (not thread-safe!)\nbuffer.reset();\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#lock-free-algorithm-spsc","title":"Lock-Free Algorithm (SPSC)","text":""},{"location":"data_structures/shm_ring_buffer/#write-operation","title":"Write Operation","text":"<pre><code>bool write(const T&amp; value) {\n    size_t write = write_pos.load(memory_order_relaxed);\n    size_t next_write = (write + 1) &amp; (capacity - 1);\n\n    // Check if full\n    if (next_write == read_pos.load(memory_order_acquire)) {\n        return false;  // Buffer full\n    }\n\n    // Write data\n    buffer[write] = value;\n\n    // Update write position (publish)\n    write_pos.store(next_write, memory_order_release);\n    return true;\n}\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#read-operation","title":"Read Operation","text":"<pre><code>optional&lt;T&gt; read() {\n    size_t read = read_pos.load(memory_order_relaxed);\n\n    // Check if empty\n    if (read == write_pos.load(memory_order_acquire)) {\n        return nullopt;  // Buffer empty\n    }\n\n    // Read data\n    T value = buffer[read];\n\n    // Update read position\n    read_pos.store((read + 1) &amp; (capacity - 1), memory_order_release);\n    return value;\n}\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#use-cases-in-n-body-simulation","title":"Use Cases in N-Body Simulation","text":""},{"location":"data_structures/shm_ring_buffer/#1-telemetry-streaming","title":"1. Telemetry Streaming","text":"<pre><code>struct TelemetryPacket {\n    uint64_t timestamp;\n    float total_energy;\n    float momentum[3];\n    uint32_t active_particles;\n    float temperature;\n};\n\n// Producer: Simulation engine\nshm_ring_buffer&lt;TelemetryPacket&gt; telemetry(shm, \"telemetry\", 1024);\n\nvoid simulation_loop() {\n    while (running) {\n        simulate_timestep();\n\n        if (frame_count % TELEMETRY_INTERVAL == 0) {\n            TelemetryPacket packet{\n                .timestamp = get_timestamp(),\n                .total_energy = calculate_energy(),\n                .momentum = {px, py, pz},\n                .active_particles = count_active(),\n                .temperature = calculate_temperature()\n            };\n\n            if (!telemetry.write(packet)) {\n                stats.telemetry_drops++;\n            }\n        }\n    }\n}\n\n// Consumer: Monitoring dashboard\nvoid monitor_process() {\n    shm_ring_buffer&lt;TelemetryPacket&gt; telemetry(shm, \"telemetry\");\n\n    while (running) {\n        while (auto packet = telemetry.read()) {\n            update_dashboard(*packet);\n            check_invariants(*packet);\n\n            if (packet-&gt;total_energy &gt; THRESHOLD) {\n                alert_operator(\"Energy violation detected\");\n            }\n        }\n\n        std::this_thread::sleep_for(10ms);\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#2-frame-buffer-for-visualization","title":"2. Frame Buffer for Visualization","text":"<pre><code>struct Frame {\n    static constexpr size_t MAX_PARTICLES = 10000;\n    uint32_t particle_count;\n    float positions[MAX_PARTICLES * 3];\n    float colors[MAX_PARTICLES * 4];\n    uint64_t frame_number;\n};\n\n// Double buffering with ring buffer\nshm_ring_buffer&lt;Frame&gt; frames(shm, \"frames\", 4);  // Triple buffer + 1\n\n// Physics thread\nvoid physics_thread() {\n    Frame frame;\n    while (running) {\n        compute_physics();\n\n        // Prepare frame\n        frame.particle_count = active_particles;\n        copy_positions(frame.positions);\n        compute_colors(frame.colors);\n        frame.frame_number++;\n\n        // Non-blocking write\n        if (!frames.write(frame)) {\n            // Skip frame if buffer full (renderer is slow)\n            stats.dropped_frames++;\n        }\n    }\n}\n\n// Render thread\nvoid render_thread() {\n    while (running) {\n        if (auto frame = frames.read()) {\n            render_particles(*frame);\n            present_frame();\n        } else {\n            // No new frame, repeat last or interpolate\n            repeat_last_frame();\n        }\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#3-event-recording","title":"3. Event Recording","text":"<pre><code>struct Event {\n    enum Type { COLLISION, MERGE, SPLIT, ESCAPE };\n    Type type;\n    uint64_t timestamp;\n    uint32_t particles[2];\n    float data[4];\n};\n\nshm_ring_buffer&lt;Event&gt; events(shm, \"events\", 10000);\n\n// Multiple producers with synchronization\nclass EventRecorder {\n    shm_ring_buffer&lt;Event&gt;&amp; buffer;\n    shm_spinlock lock;  // For MPSC\n\npublic:\n    void record(const Event&amp; event) {\n        std::lock_guard&lt;shm_spinlock&gt; guard(lock);\n        if (!buffer.write(event)) {\n            // Buffer full - implement overflow policy\n            if (event.type == COLLISION) {\n                // High priority - force write\n                buffer.read();  // Drop oldest\n                buffer.write(event);\n            }\n        }\n    }\n};\n\n// Single consumer\nvoid event_processor() {\n    std::vector&lt;Event&gt; batch;\n    batch.reserve(100);\n\n    while (running) {\n        // Batch read for efficiency\n        size_t count = events.read_batch(batch.data(), batch.capacity());\n\n        for (size_t i = 0; i &lt; count; ++i) {\n            switch (batch[i].type) {\n                case Event::COLLISION:\n                    handle_collision(batch[i]);\n                    break;\n                case Event::MERGE:\n                    handle_merge(batch[i]);\n                    break;\n                // ...\n            }\n        }\n\n        if (count == 0) {\n            std::this_thread::sleep_for(1ms);\n        }\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#4-command-buffer","title":"4. Command Buffer","text":"<pre><code>struct Command {\n    char name[32];\n    float args[8];\n    std::function&lt;void(float*)&gt; execute;\n};\n\nclass CommandBuffer {\n    shm_ring_buffer&lt;Command&gt; commands;\n\npublic:\n    // Batched command execution\n    void flush() {\n        std::vector&lt;Command&gt; batch(commands.size());\n        size_t count = commands.read_batch(batch.data(), batch.size());\n\n        // Sort by command type for better cache usage\n        std::sort(batch.begin(), batch.begin() + count,\n                  [](const Command&amp; a, const Command&amp; b) {\n                      return strcmp(a.name, b.name) &lt; 0;\n                  });\n\n        // Execute sorted commands\n        for (size_t i = 0; i &lt; count; ++i) {\n            batch[i].execute(batch[i].args);\n        }\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"data_structures/shm_ring_buffer/#1-multi-producer-ring-buffer-mpsc","title":"1. Multi-Producer Ring Buffer (MPSC)","text":"<pre><code>template&lt;typename T&gt;\nclass MPSCRingBuffer {\n    shm_ring_buffer&lt;T&gt; buffer;\n    shm_atomic&lt;size_t&gt; write_claim{0};\n\npublic:\n    bool write(const T&amp; value) {\n        // Claim a slot\n        size_t slot = write_claim.fetch_add(1);\n        size_t index = slot &amp; (buffer.capacity() - 1);\n\n        // Wait for our turn (sequence consistency)\n        while (buffer.write_pos.load() != slot) {\n            std::this_thread::yield();\n        }\n\n        // Write and publish\n        buffer.data[index] = value;\n        buffer.write_pos.store(slot + 1);\n        return true;\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#2-watermark-based-flow-control","title":"2. Watermark-Based Flow Control","text":"<pre><code>template&lt;typename T&gt;\nclass WatermarkBuffer {\n    shm_ring_buffer&lt;T&gt; buffer;\n    static constexpr size_t LOW_WATER = 25;   // 25% full\n    static constexpr size_t HIGH_WATER = 75;  // 75% full\n    shm_atomic&lt;bool&gt; backpressure{false};\n\npublic:\n    bool write(const T&amp; value) {\n        size_t percent = (buffer.size() * 100) / buffer.capacity();\n\n        if (percent &gt; HIGH_WATER) {\n            backpressure.store(true);\n            // Signal producer to slow down\n            return false;\n        } else if (percent &lt; LOW_WATER) {\n            backpressure.store(false);\n            // Signal producer to speed up\n        }\n\n        return buffer.write(value);\n    }\n\n    bool should_throttle() const {\n        return backpressure.load();\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#3-zero-copy-streaming","title":"3. Zero-Copy Streaming","text":"<pre><code>template&lt;typename T&gt;\nclass ZeroCopyStream {\n    shm_ring_buffer&lt;T&gt; buffer;\n\npublic:\n    // Producer gets direct buffer access\n    template&lt;typename Producer&gt;\n    size_t produce(Producer&amp;&amp; prod) {\n        auto [ptr, size] = buffer.write_ptr();\n        if (size == 0) return 0;\n\n        // Producer writes directly to buffer\n        size_t produced = prod(ptr, size);\n\n        buffer.commit_write(produced);\n        return produced;\n    }\n\n    // Consumer gets direct buffer access\n    template&lt;typename Consumer&gt;\n    size_t consume(Consumer&amp;&amp; cons) {\n        auto [ptr, size] = buffer.read_ptr();\n        if (size == 0) return 0;\n\n        // Consumer reads directly from buffer\n        size_t consumed = cons(ptr, size);\n\n        buffer.commit_read(consumed);\n        return consumed;\n    }\n};\n\n// Usage with file I/O\nvoid stream_file_to_shm(int fd) {\n    ZeroCopyStream&lt;char&gt; stream(shm, \"file_stream\");\n\n    stream.produce([fd](char* buffer, size_t size) {\n        return read(fd, buffer, size);\n    });\n}\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"data_structures/shm_ring_buffer/#throughput","title":"Throughput","text":"Configuration Throughput Latency SPSC 100M ops/sec ~10ns MPSC (2 producers) 50M ops/sec ~20ns MPSC (4 producers) 25M ops/sec ~40ns MPMC 10M ops/sec ~100ns"},{"location":"data_structures/shm_ring_buffer/#memory-bandwidth","title":"Memory Bandwidth","text":"<ul> <li>Sequential Write: ~30 GB/s (memory bandwidth limited)</li> <li>Sequential Read: ~50 GB/s (memory bandwidth limited)</li> <li>Random Access: Not applicable (sequential only)</li> </ul>"},{"location":"data_structures/shm_ring_buffer/#cache-effects","title":"Cache Effects","text":"<pre><code>// Optimal: Process in batches\nconst size_t BATCH_SIZE = 64;  // Full cache line\nT batch[BATCH_SIZE];\n\nwhile (running) {\n    size_t count = buffer.read_batch(batch, BATCH_SIZE);\n\n    // Process entire batch (cache friendly)\n    for (size_t i = 0; i &lt; count; ++i) {\n        process(batch[i]);\n    }\n}\n\n// Poor: One at a time\nwhile (running) {\n    if (auto item = buffer.read()) {\n        process(*item);  // Cache miss likely\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#comparison-with-alternatives","title":"Comparison with Alternatives","text":"Feature Ring Buffer Queue Pipe Shared Memory Bounded Yes Yes No* No Zero-copy Yes No No Yes SPSC Optimal Good Good Manual MPSC Good Good Poor Manual Ordering FIFO FIFO FIFO None Overflow Drop/Block Block Block N/A"},{"location":"data_structures/shm_ring_buffer/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"data_structures/shm_ring_buffer/#1-not-power-of-2-size","title":"1. Not Power-of-2 Size","text":"<pre><code>// WRONG: Expensive modulo operation\nsize_t next = (current + 1) % 1000;  // Slow division\n\n// CORRECT: Fast bitwise AND\nsize_t next = (current + 1) &amp; (1024 - 1);  // Single instruction\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#2-false-sharing","title":"2. False Sharing","text":"<pre><code>// WRONG: Read and write positions adjacent\nstruct BadHeader {\n    std::atomic&lt;size_t&gt; write_pos;\n    std::atomic&lt;size_t&gt; read_pos;  // Same cache line!\n};\n\n// CORRECT: Cache line separation\nstruct GoodHeader {\n    std::atomic&lt;size_t&gt; write_pos;\n    char padding[56];  // Separate cache lines\n    std::atomic&lt;size_t&gt; read_pos;\n};\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#3-inconsistent-memory-ordering","title":"3. Inconsistent Memory Ordering","text":"<pre><code>// WRONG: Relaxed ordering loses updates\nwrite_pos.store(new_pos, std::memory_order_relaxed);\n// Consumer might not see the write!\n\n// CORRECT: Acquire-release semantics\nwrite_pos.store(new_pos, std::memory_order_release);\n// Guarantees consumer sees the write\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#testing-strategies","title":"Testing Strategies","text":"<pre><code>TEST_CASE(\"Ring buffer maintains order\") {\n    shm_ring_buffer&lt;int&gt; buffer(shm, \"test\", 1024);\n\n    // Producer thread\n    std::thread producer([&amp;]() {\n        for (int i = 0; i &lt; 10000; ++i) {\n            while (!buffer.write(i)) {\n                std::this_thread::yield();\n            }\n        }\n    });\n\n    // Consumer thread\n    std::thread consumer([&amp;]() {\n        for (int expected = 0; expected &lt; 10000; ++expected) {\n            int value;\n            while (!buffer.read(value)) {\n                std::this_thread::yield();\n            }\n            REQUIRE(value == expected);  // Order preserved\n        }\n    });\n\n    producer.join();\n    consumer.join();\n}\n</code></pre>"},{"location":"data_structures/shm_ring_buffer/#references","title":"References","text":"<ul> <li>Disruptor: High Performance Alternative to Bounded Queues</li> <li>A Lock-Free Ring Buffer</li> <li>The Virtues of Lamport's Bakery Algorithm</li> </ul>"},{"location":"data_structures/shm_stack/","title":"shm_stack: Lock-Free LIFO Stack for Shared Memory","text":""},{"location":"data_structures/shm_stack/#overview","title":"Overview","text":"<p><code>shm_stack&lt;T&gt;</code> implements a lock-free, bounded LIFO (Last-In-First-Out) stack in POSIX shared memory. It provides efficient push/pop operations using atomic compare-and-swap (CAS), making it ideal for undo/redo systems, recursion management, and work-stealing algorithms in multi-process environments.</p>"},{"location":"data_structures/shm_stack/#architecture","title":"Architecture","text":""},{"location":"data_structures/shm_stack/#memory-layout","title":"Memory Layout","text":"<pre><code>[shm_table metadata]\n[StackHeader]\n  \u251c\u2500 atomic&lt;size_t&gt; top      // Index of next free slot\n  \u251c\u2500 size_t capacity         // Maximum elements\n  \u2514\u2500 padding[48]             // Cache line alignment (64 bytes)\n[T][T][T]...[T]              // Contiguous array of elements\n</code></pre>"},{"location":"data_structures/shm_stack/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li>Array-Based: Bounded size with O(1) operations</li> <li>Lock-Free Push/Pop: CAS-based Treiber stack algorithm</li> <li>Cache-Line Aligned: Header in single cache line to minimize contention</li> <li>ABA Prevention: Uses array indices instead of pointers</li> </ol>"},{"location":"data_structures/shm_stack/#api-reference","title":"API Reference","text":""},{"location":"data_structures/shm_stack/#construction","title":"Construction","text":"<pre><code>// Create new stack\nposix_shm shm(\"/simulation\", 10 * 1024 * 1024);\nshm_stack&lt;State&gt; undo_stack(shm, \"undo\", 1000);  // capacity = 1000\n\n// Open existing stack\nshm_stack&lt;State&gt; existing(shm, \"undo\", 0);  // capacity = 0 means open\n\n// Check existence before opening\nif (shm_stack&lt;State&gt;::exists(shm, \"undo\")) {\n    shm_stack&lt;State&gt; stack(shm, \"undo\");\n}\n</code></pre>"},{"location":"data_structures/shm_stack/#basic-operations","title":"Basic Operations","text":"<pre><code>// Push (returns false if full)\nState state{...};\nif (!stack.push(state)) {\n    // Stack is full\n    handle_overflow();\n}\n\n// Pop (returns optional)\nauto state = stack.pop();\nif (state.has_value()) {\n    restore(*state);\n}\n\n// Peek at top without removing\nauto top_state = stack.top();\nif (top_state.has_value()) {\n    preview(*top_state);\n}\n</code></pre>"},{"location":"data_structures/shm_stack/#stack-state","title":"Stack State","text":"<pre><code>size_t count = stack.size();      // Current number of elements\nsize_t cap = stack.capacity();    // Maximum capacity  \nbool is_empty = stack.empty();    // True if no elements\nbool is_full = stack.full();      // True if at capacity\n\n// Clear all elements (NOT thread-safe!)\nstack.clear();\n</code></pre>"},{"location":"data_structures/shm_stack/#lock-free-algorithm","title":"Lock-Free Algorithm","text":""},{"location":"data_structures/shm_stack/#push-operation","title":"Push Operation","text":"<pre><code>bool push(const T&amp; value) {\n    size_t current_top = top.load(memory_order_acquire);\n\n    do {\n        if (current_top &gt;= capacity) {\n            return false;  // Stack is full\n        }\n\n        // Try to claim the slot\n        if (top.compare_exchange_weak(current_top, current_top + 1,\n                                      memory_order_release,\n                                      memory_order_acquire)) {\n            // Successfully claimed slot, write the value\n            data[current_top] = value;\n            return true;\n        }\n        // CAS failed, current_top was updated, retry\n    } while (true);\n}\n</code></pre>"},{"location":"data_structures/shm_stack/#pop-operation","title":"Pop Operation","text":"<pre><code>optional&lt;T&gt; pop() {\n    size_t current_top = top.load(memory_order_acquire);\n\n    do {\n        if (current_top == 0) {\n            return nullopt;  // Stack is empty\n        }\n\n        // Read the value before updating top\n        T value = data[current_top - 1];\n\n        // Try to update top\n        if (top.compare_exchange_weak(current_top, current_top - 1,\n                                      memory_order_release,\n                                      memory_order_acquire)) {\n            return value;\n        }\n        // CAS failed, current_top was updated, retry\n    } while (true);\n}\n</code></pre>"},{"location":"data_structures/shm_stack/#memory-ordering","title":"Memory Ordering","text":"<ul> <li>Acquire: Ensures we see all writes before the release</li> <li>Release: Ensures our writes are visible before the release</li> <li>CAS Loop: Provides lock-free progress guarantee</li> </ul>"},{"location":"data_structures/shm_stack/#use-cases-in-n-body-simulation","title":"Use Cases in N-Body Simulation","text":""},{"location":"data_structures/shm_stack/#1-undoredo-system","title":"1. Undo/Redo System","text":"<pre><code>struct SimulationState {\n    uint64_t timestep;\n    uint64_t seed;\n    float dt;\n    float total_energy;\n};\n\nshm_stack&lt;SimulationState&gt; undo_stack(shm, \"undo\", 100);\nshm_stack&lt;SimulationState&gt; redo_stack(shm, \"redo\", 100);\n\nvoid save_state() {\n    SimulationState state{\n        .timestep = current_timestep,\n        .seed = random_seed,\n        .dt = delta_time,\n        .total_energy = calculate_energy()\n    };\n\n    if (!undo_stack.push(state)) {\n        // Remove oldest state\n        rotate_undo_buffer();\n        undo_stack.push(state);\n    }\n\n    // Clear redo stack on new action\n    redo_stack.clear();\n}\n\nvoid undo() {\n    if (auto state = undo_stack.pop()) {\n        // Save current state for redo\n        save_current_for_redo();\n\n        // Restore previous state\n        restore_simulation(*state);\n    }\n}\n\nvoid redo() {\n    if (auto state = redo_stack.pop()) {\n        save_state();  // Save for undo\n        restore_simulation(*state);\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_stack/#2-work-stealing-deque","title":"2. Work-Stealing Deque","text":"<pre><code>template&lt;typename Task&gt;\nclass WorkStealingDeque {\n    shm_stack&lt;Task&gt; local_stack;\n    shm_atomic&lt;bool&gt; stealing;\n\npublic:\n    // Owner pushes/pops from top\n    void push(const Task&amp; task) {\n        if (!local_stack.push(task)) {\n            // Handle overflow\n            execute_immediately(task);\n        }\n    }\n\n    std::optional&lt;Task&gt; pop() {\n        return local_stack.pop();\n    }\n\n    // Thieves steal from bottom (simulated)\n    std::optional&lt;Task&gt; steal() {\n        if (stealing.exchange(true)) {\n            return std::nullopt;  // Another thief is stealing\n        }\n\n        // In real implementation, would need double-ended structure\n        auto task = local_stack.pop();  \n        stealing.store(false);\n        return task;\n    }\n};\n\n// Worker threads\nvoid worker(int id) {\n    WorkStealingDeque&lt;Task&gt; my_deque(shm, \"worker_\" + std::to_string(id));\n\n    while (running) {\n        // Try local work first\n        if (auto task = my_deque.pop()) {\n            execute(*task);\n        } else {\n            // Steal from others\n            for (int i = 0; i &lt; num_workers; ++i) {\n                if (i != id) {\n                    WorkStealingDeque&lt;Task&gt; other(shm, \"worker_\" + std::to_string(i));\n                    if (auto task = other.steal()) {\n                        execute(*task);\n                        break;\n                    }\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_stack/#3-recursion-stack","title":"3. Recursion Stack","text":"<pre><code>struct TreeNode {\n    uint32_t particle_idx;\n    float3 center;\n    float radius;\n    uint32_t children[8];  // Octree\n};\n\nshm_stack&lt;TreeNode&gt; traversal(shm, \"octree_traversal\", 1000);\n\nvoid find_neighbors(uint32_t particle_id, float search_radius) {\n    // Initialize with root\n    traversal.push(get_root_node());\n\n    std::vector&lt;uint32_t&gt; neighbors;\n\n    while (!traversal.empty()) {\n        auto node = traversal.pop();\n        if (!node) break;\n\n        // Check if particle could be in this node\n        if (distance(particles[particle_id].pos, node-&gt;center) \n            &lt; search_radius + node-&gt;radius) {\n\n            if (is_leaf(*node)) {\n                // Check actual particle\n                if (distance(particles[particle_id].pos, \n                           particles[node-&gt;particle_idx].pos) &lt; search_radius) {\n                    neighbors.push_back(node-&gt;particle_idx);\n                }\n            } else {\n                // Push children for traversal\n                for (int i = 0; i &lt; 8; ++i) {\n                    if (node-&gt;children[i] != INVALID) {\n                        traversal.push(get_node(node-&gt;children[i]));\n                    }\n                }\n            }\n        }\n    }\n\n    process_neighbors(particle_id, neighbors);\n}\n</code></pre>"},{"location":"data_structures/shm_stack/#4-memory-pool-free-list","title":"4. Memory Pool Free List","text":"<pre><code>template&lt;typename T&gt;\nclass StackAllocator {\n    shm_stack&lt;uint32_t&gt; free_indices;\n    shm_array&lt;T&gt; pool;\n\npublic:\n    StackAllocator(posix_shm&amp; shm, size_t capacity)\n        : free_indices(shm, \"free_list\", capacity),\n          pool(shm, \"pool\", capacity) {\n\n        // Initialize free list with all indices\n        for (uint32_t i = capacity; i &gt; 0; --i) {\n            free_indices.push(i - 1);\n        }\n    }\n\n    std::optional&lt;uint32_t&gt; allocate() {\n        return free_indices.pop();\n    }\n\n    void deallocate(uint32_t idx) {\n        // Clear the object\n        pool[idx] = T{};\n\n        // Return to free list\n        if (!free_indices.push(idx)) {\n            // Stack overflow - memory leak!\n            log_error(\"Free list overflow\");\n        }\n    }\n\n    T&amp; operator[](uint32_t idx) {\n        return pool[idx];\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_stack/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"data_structures/shm_stack/#throughput","title":"Throughput","text":"<ul> <li>Uncontended: ~60M ops/sec on modern x86</li> <li>Light Contention (2-4 threads): ~30M ops/sec</li> <li>Heavy Contention (8+ threads): ~10M ops/sec</li> </ul>"},{"location":"data_structures/shm_stack/#latency","title":"Latency","text":"<ul> <li>Best Case: ~8ns (no contention, data in L1)</li> <li>Average: ~20ns (some CAS retries)</li> <li>Worst Case: ~100ns (high contention, many retries)</li> </ul>"},{"location":"data_structures/shm_stack/#cas-retry-statistics","title":"CAS Retry Statistics","text":"Threads Avg Retries Max Retries Success Rate 1 0 0 100% 2 0.1 3 99% 4 0.5 8 95% 8 2.3 20 85%"},{"location":"data_structures/shm_stack/#comparison-with-queue","title":"Comparison with Queue","text":"Aspect Stack (LIFO) Queue (FIFO) Ordering Last-In-First-Out First-In-First-Out Cache Locality Better (hot data) Worse (cold data) Fairness Unfair Fair Use Cases Undo, recursion, allocation Tasks, events, messages Implementation Single atomic index Two atomic indices Memory Contiguous array Circular buffer"},{"location":"data_structures/shm_stack/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"data_structures/shm_stack/#1-elimination-stack","title":"1. Elimination Stack","text":"<pre><code>template&lt;typename T&gt;\nclass EliminationStack {\n    shm_stack&lt;T&gt; stack;\n    shm_array&lt;std::optional&lt;T&gt;&gt; elimination;\n    shm_atomic&lt;uint32_t&gt; collision_counter;\n\npublic:\n    bool push(const T&amp; value) {\n        // Try elimination first\n        uint32_t slot = hash(std::this_thread::get_id()) % elimination.size();\n\n        if (!elimination[slot].has_value()) {\n            elimination[slot] = value;\n\n            // Wait briefly for consumer\n            std::this_thread::sleep_for(std::chrono::microseconds(1));\n\n            if (!elimination[slot].has_value()) {\n                // Value was consumed via elimination\n                collision_counter.fetch_add(1);\n                return true;\n            }\n\n            // No elimination, clear slot\n            elimination[slot].reset();\n        }\n\n        // Fall back to stack\n        return stack.push(value);\n    }\n\n    std::optional&lt;T&gt; pop() {\n        // Check elimination array first\n        uint32_t slot = hash(std::this_thread::get_id()) % elimination.size();\n\n        if (auto value = elimination[slot].exchange(std::nullopt)) {\n            collision_counter.fetch_add(1);\n            return value;\n        }\n\n        // Fall back to stack\n        return stack.pop();\n    }\n};\n</code></pre>"},{"location":"data_structures/shm_stack/#2-bounded-recursion","title":"2. Bounded Recursion","text":"<pre><code>class RecursionGuard {\n    shm_stack&lt;std::string&gt; call_stack;\n    static constexpr size_t MAX_DEPTH = 100;\n\npublic:\n    RecursionGuard(posix_shm&amp; shm) \n        : call_stack(shm, \"recursion\", MAX_DEPTH) {}\n\n    [[nodiscard]] bool enter(const std::string&amp; function) {\n        if (call_stack.size() &gt;= MAX_DEPTH) {\n            dump_stack_trace();\n            return false;  // Stack overflow\n        }\n        return call_stack.push(function);\n    }\n\n    void exit() {\n        call_stack.pop();\n    }\n\n    void dump_stack_trace() {\n        std::cerr &lt;&lt; \"Stack trace:\\n\";\n        // Note: This modifies the stack!\n        while (auto frame = call_stack.pop()) {\n            std::cerr &lt;&lt; \"  at \" &lt;&lt; *frame &lt;&lt; \"\\n\";\n        }\n    }\n};\n\n// RAII wrapper\nclass ScopedRecursion {\n    RecursionGuard&amp; guard;\n    bool entered;\n\npublic:\n    ScopedRecursion(RecursionGuard&amp; g, const std::string&amp; func)\n        : guard(g), entered(g.enter(func)) {}\n\n    ~ScopedRecursion() {\n        if (entered) guard.exit();\n    }\n\n    operator bool() const { return entered; }\n};\n\n// Usage\nvoid recursive_function(RecursionGuard&amp; guard) {\n    ScopedRecursion scope(guard, __FUNCTION__);\n    if (!scope) {\n        throw std::runtime_error(\"Stack overflow\");\n    }\n\n    // Recursive logic...\n}\n</code></pre>"},{"location":"data_structures/shm_stack/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"data_structures/shm_stack/#1-aba-problem-in-custom-implementations","title":"1. ABA Problem in Custom Implementations","text":"<pre><code>// WRONG: Susceptible to ABA\nstruct Node {\n    T data;\n    Node* next;\n};\n\n// Thread 1 reads A\nNode* old_top = top.load();\n// Thread 2: pops A, pops B, pushes A (same address!)\n// Thread 1: CAS succeeds but next pointer is wrong!\n\n// CORRECT: Use hazard pointers or array indices\nsize_t old_top = top.load();  // Index can't be reused while in use\n</code></pre>"},{"location":"data_structures/shm_stack/#2-memory-ordering-issues","title":"2. Memory Ordering Issues","text":"<pre><code>// WRONG: Data race\ndata[top] = value;\ntop.fetch_add(1, std::memory_order_relaxed);\n\n// CORRECT: Proper synchronization\nsize_t idx = top.fetch_add(1, std::memory_order_acq_rel);\ndata[idx] = value;\n</code></pre>"},{"location":"data_structures/shm_stack/#3-clear-safety","title":"3. Clear() Safety","text":"<pre><code>// WRONG: clear() while others are pushing/popping\nvoid unsafe_reset() {\n    stack.clear();  // NOT thread-safe!\n}\n\n// CORRECT: Drain the stack\nvoid safe_drain() {\n    while (stack.pop().has_value()) {\n        // Keep popping\n    }\n}\n</code></pre>"},{"location":"data_structures/shm_stack/#testing-strategies","title":"Testing Strategies","text":""},{"location":"data_structures/shm_stack/#unit-tests","title":"Unit Tests","text":"<pre><code>TEST_CASE(\"Stack maintains LIFO order\") {\n    shm_stack&lt;int&gt; stack(shm, \"test\", 100);\n\n    // Push sequence\n    for (int i = 0; i &lt; 10; ++i) {\n        REQUIRE(stack.push(i));\n    }\n\n    // Pop in reverse order\n    for (int i = 9; i &gt;= 0; --i) {\n        auto val = stack.pop();\n        REQUIRE(val.has_value());\n        REQUIRE(*val == i);\n    }\n\n    REQUIRE(stack.empty());\n}\n</code></pre>"},{"location":"data_structures/shm_stack/#stress-tests","title":"Stress Tests","text":"<pre><code>TEST_CASE(\"Stack handles concurrent operations\") {\n    shm_stack&lt;int&gt; stack(shm, \"concurrent\", 10000);\n    std::atomic&lt;int&gt; sum{0};\n    const int ops_per_thread = 1000;\n\n    auto worker = [&amp;](int id) {\n        int local_sum = 0;\n\n        // Push phase\n        for (int i = 0; i &lt; ops_per_thread; ++i) {\n            int val = id * ops_per_thread + i;\n            while (!stack.push(val)) {\n                std::this_thread::yield();\n            }\n            local_sum += val;\n        }\n\n        // Pop phase\n        for (int i = 0; i &lt; ops_per_thread; ++i) {\n            while (true) {\n                if (auto val = stack.pop()) {\n                    local_sum -= *val;\n                    break;\n                }\n                std::this_thread::yield();\n            }\n        }\n\n        sum += local_sum;\n    };\n\n    std::vector&lt;std::thread&gt; threads;\n    for (int i = 0; i &lt; 8; ++i) {\n        threads.emplace_back(worker, i);\n    }\n\n    for (auto&amp; t : threads) t.join();\n\n    REQUIRE(sum == 0);  // Conservation of values\n    REQUIRE(stack.empty());\n}\n</code></pre>"},{"location":"data_structures/shm_stack/#references","title":"References","text":"<ul> <li>Systems Programming: Coping with Parallelism - Treiber's Stack</li> <li>The Elimination Back-off Stack - Hendler et al.</li> <li>A Scalable Lock-free Stack Algorithm - Hendler et al.</li> <li>Hazard Pointers - Safe memory reclamation</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Real-world examples demonstrating ZeroIPC usage patterns.</p>"},{"location":"examples/#example-categories","title":"Example Categories","text":""},{"location":"examples/#cross-language-communication","title":"Cross-Language Communication","text":"<p>Examples of C++, Python, and C processes communicating:</p> <ul> <li>C++ producer, Python consumer</li> <li>Python producer, C++ consumer</li> <li>Multi-language pipeline</li> <li>Bidirectional messaging</li> </ul>"},{"location":"examples/#producer-consumer","title":"Producer-Consumer","text":"<p>Classic producer-consumer patterns:</p> <ul> <li>Single producer, single consumer</li> <li>Multiple producers, single consumer</li> <li>Single producer, multiple consumers</li> <li>Work stealing queue</li> </ul>"},{"location":"examples/#sensor-data-sharing","title":"Sensor Data Sharing","text":"<p>IoT and sensor data examples:</p> <ul> <li>Temperature monitoring</li> <li>Multi-sensor aggregation</li> <li>Real-time data visualization</li> <li>Historical data buffering</li> </ul>"},{"location":"examples/#reactive-processing","title":"Reactive Processing","text":"<p>Event-driven programming with streams:</p> <ul> <li>Stream transformations (map, filter)</li> <li>Event aggregation</li> <li>Real-time analytics</li> <li>Backpressure handling</li> </ul>"},{"location":"examples/#real-time-analytics","title":"Real-Time Analytics","text":"<p>High-performance data processing:</p> <ul> <li>Sliding window calculations</li> <li>Real-time aggregations</li> <li>Pattern detection</li> <li>Alert generation</li> </ul>"},{"location":"examples/#quick-examples","title":"Quick Examples","text":""},{"location":"examples/#1-simple-data-sharing","title":"1. Simple Data Sharing","text":"<p>Share an array between C++ and Python.</p> C++ (writer.cpp)Python (reader.py) <pre><code>#include &lt;zeroipc/memory.h&gt;\n#include &lt;zeroipc/array.h&gt;\n\nint main() {\n    zeroipc::Memory mem(\"/demo\", 1024*1024);\n    zeroipc::Array&lt;double&gt; data(mem, \"values\", 100);\n\n    for (int i = 0; i &lt; 100; ++i) {\n        data[i] = i * 1.5;\n    }\n\n    std::cout &lt;&lt; \"Data ready. Press Ctrl+C to exit.\\n\";\n    std::this_thread::sleep_for(std::chrono::hours(1));\n}\n</code></pre> <pre><code>from zeroipc import Memory, Array\nimport numpy as np\n\nmem = Memory(\"/demo\")\ndata = Array(mem, \"values\", dtype=np.float64)\n\nprint(f\"First value: {data[0]}\")\nprint(f\"Sum: {np.sum(data[:])}\")\nprint(f\"Mean: {np.mean(data[:])}\")\n</code></pre>"},{"location":"examples/#2-work-queue","title":"2. Work Queue","text":"<p>Distribute tasks across worker processes.</p> Manager (C++)Worker (Python) <pre><code>#include &lt;zeroipc/memory.h&gt;\n#include &lt;zeroipc/queue.h&gt;\n\nstruct Task {\n    int id;\n    char data[64];\n};\n\nint main() {\n    zeroipc::Memory mem(\"/tasks\", 10*1024*1024);\n    zeroipc::Queue&lt;Task&gt; queue(mem, \"work\", 1000);\n\n    // Add tasks\n    for (int i = 0; i &lt; 1000; ++i) {\n        Task t{i, \"process_image\"};\n        queue.enqueue(t);\n    }\n\n    std::cout &lt;&lt; \"Added 1000 tasks\\n\";\n}\n</code></pre> <pre><code>from zeroipc import Memory, Queue\nimport numpy as np\n\n# Define matching dtype\ntask_dtype = np.dtype([\n    ('id', np.int32),\n    ('data', 'S64')\n])\n\nmem = Memory(\"/tasks\")\nqueue = Queue(mem, \"work\", dtype=task_dtype)\n\nwhile True:\n    task = queue.dequeue()\n    if task is not None:\n        process_task(task)\n    else:\n        break  # No more tasks\n</code></pre>"},{"location":"examples/#3-real-time-stream-processing","title":"3. Real-Time Stream Processing","text":"<p>Process sensor data with reactive streams.</p> Sensor (C++)Processor (C++) <pre><code>#include &lt;zeroipc/memory.h&gt;\n#include &lt;zeroipc/stream.h&gt;\n\nstruct Reading {\n    double temperature;\n    double pressure;\n    uint64_t timestamp;\n};\n\nint main() {\n    zeroipc::Memory mem(\"/sensors\", 10*1024*1024);\n    zeroipc::Stream&lt;Reading&gt; stream(mem, \"raw\", 1000);\n\n    while (running) {\n        Reading r = read_sensor();\n        stream.emit(r);\n        std::this_thread::sleep_for(100ms);\n    }\n}\n</code></pre> <pre><code>zeroipc::Memory mem(\"/sensors\");\nzeroipc::Stream&lt;Reading&gt; raw(mem, \"raw\");\n\n// Create derived streams\nauto high_temp = raw.filter(mem, \"alerts\", \n    [](Reading&amp; r) { return r.temperature &gt; 30.0; });\n\nauto celsius_to_f = raw.map(mem, \"fahrenheit\",\n    [](Reading&amp; r) { \n        Reading f = r;\n        f.temperature = r.temperature * 9/5 + 32;\n        return f;\n    });\n\n// Subscribe to alerts\nhigh_temp.subscribe([](Reading&amp; r) {\n    send_alert(\"High temperature: \" + std::to_string(r.temperature));\n});\n</code></pre>"},{"location":"examples/#example-repository","title":"Example Repository","text":"<p>All examples with build files and instructions:</p> <pre><code>git clone https://github.com/yourusername/zeroipc.git\ncd zeroipc/examples\n\n# Each example has its own directory\nls -la\n# basic/\n# cross_language/\n# producer_consumer/\n# sensors/\n# streams/\n# analytics/\n</code></pre> <p>Each example includes: - README.md - What it demonstrates - Makefile or CMakeLists.txt - Build instructions - C++ source - Complete working code - Python source - Complete working code - run.sh - Helper script to run the example</p>"},{"location":"examples/#building-examples","title":"Building Examples","text":""},{"location":"examples/#c-examples","title":"C++ Examples","text":"<pre><code>cd examples/cross_language/cpp\nmkdir build &amp;&amp; cd build\ncmake ..\nmake\n./producer\n</code></pre>"},{"location":"examples/#python-examples","title":"Python Examples","text":"<pre><code>cd examples/cross_language/python\npip install -r requirements.txt\npython consumer.py\n</code></pre>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<p>Explore specific examples:</p> <ul> <li>Cross-Language - Language interop</li> <li>Producer-Consumer - Work distribution</li> <li>Sensor Data - IoT patterns</li> <li>Reactive Processing - Event-driven</li> <li>Analytics - Real-time processing</li> </ul>"},{"location":"getting-started/","title":"Getting Started with ZeroIPC","text":"<p>Welcome to ZeroIPC! This guide will help you get up and running with cross-language shared memory IPC in minutes.</p>"},{"location":"getting-started/#what-youll-learn","title":"What You'll Learn","text":"<p>This section covers everything you need to start using ZeroIPC:</p> <ol> <li>Installation - Install ZeroIPC for C++, C, or Python</li> <li>Quick Start - Write your first shared memory program</li> <li>Basic Concepts - Understanding shared memory, metadata tables, and data structures</li> </ol>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li>Operating System: Linux, macOS, or any POSIX-compliant system</li> <li>For C++: C++23-compatible compiler (GCC 12+, Clang 15+)</li> <li>For C: C99-compatible compiler</li> <li>For Python: Python 3.8+ with NumPy</li> </ul> <p>Platform Support</p> <p>ZeroIPC uses POSIX shared memory (<code>shm_open</code>, <code>mmap</code>), which is available on all Unix-like systems. Windows support via Cygwin or WSL.</p>"},{"location":"getting-started/#typical-workflow","title":"Typical Workflow","text":"<p>Here's the typical development workflow with ZeroIPC:</p> <pre><code>graph LR\n    A[Install ZeroIPC] --&gt; B[Create Shared Memory]\n    B --&gt; C[Add Data Structures]\n    C --&gt; D[Share Between Processes]\n    D --&gt; E[Inspect with CLI]\n    E --&gt; F[Production Use]</code></pre>"},{"location":"getting-started/#quick-decision-guide","title":"Quick Decision Guide","text":""},{"location":"getting-started/#which-language-should-i-use","title":"Which Language Should I Use?","text":"<p>Choose based on your needs:</p> Use Case Recommended Language Why Maximum performance C++ Header-only templates, zero overhead Legacy integration C Pure C99, minimal dependencies Data science/ML Python NumPy integration, REPL convenience Mixed environment All three Language independence is the point!"},{"location":"getting-started/#which-data-structure-should-i-use","title":"Which Data Structure Should I Use?","text":"Need Structure Best For Random access <code>Array</code> Sensor readings, bulk data FIFO queue <code>Queue</code> Task queues, event streams LIFO stack <code>Stack</code> Undo buffers, call stacks Key-value lookup <code>Map</code> Caching, configuration Unique elements <code>Set</code> Deduplication, membership tests Object recycling <code>Pool</code> Memory-efficient object reuse Streaming <code>Stream</code> Reactive event processing Async results <code>Future</code> Cross-process async/await Message passing <code>Channel</code> CSP-style communication"},{"location":"getting-started/#example-temperature-monitoring","title":"Example: Temperature Monitoring","text":"<p>Let's look at a complete example of a C++ producer and Python consumer:</p> C++ Producer (producer.cpp)Python Consumer (consumer.py) <pre><code>#include &lt;zeroipc/memory.h&gt;\n#include &lt;zeroipc/array.h&gt;\n#include &lt;chrono&gt;\n#include &lt;thread&gt;\n#include &lt;random&gt;\n\nint main() {\n    // Create 10MB shared memory\n    zeroipc::Memory mem(\"/sensors\", 10*1024*1024);\n\n    // Create array for 1000 temperature readings\n    zeroipc::Array&lt;float&gt; temps(mem, \"temperatures\", 1000);\n\n    // Simulate sensor readings\n    std::random_device rd;\n    std::mt19937 gen(rd());\n    std::uniform_real_distribution&lt;float&gt; dis(20.0f, 30.0f);\n\n    for (int i = 0; i &lt; 1000; ++i) {\n        temps[i] = dis(gen);\n        std::this_thread::sleep_for(std::chrono::milliseconds(10));\n    }\n\n    return 0;\n}\n</code></pre> <pre><code>from zeroipc import Memory, Array\nimport numpy as np\nimport time\n\n# Open existing shared memory\nmem = Memory(\"/sensors\")\n\n# Access temperature array\ntemps = Array(mem, \"temperatures\", dtype=np.float32)\n\n# Monitor temperatures\nwhile True:\n    current_temps = np.array(temps[:100])  # Read first 100\n    avg_temp = np.mean(current_temps)\n    max_temp = np.max(current_temps)\n\n    print(f\"Average: {avg_temp:.2f}\u00b0C, Max: {max_temp:.2f}\u00b0C\")\n    time.sleep(1)\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Ready to install ZeroIPC? Head to the Installation Guide to get started!</p> <p>Already installed? Jump to the Quick Start to write your first program.</p> <p>Want to understand the concepts first? Read Basic Concepts.</p>"},{"location":"getting-started/concepts/","title":"Basic Concepts","text":"<p>Understanding these core concepts will help you use ZeroIPC effectively and avoid common pitfalls.</p>"},{"location":"getting-started/concepts/#shared-memory-fundamentals","title":"Shared Memory Fundamentals","text":""},{"location":"getting-started/concepts/#what-is-shared-memory","title":"What is Shared Memory?","text":"<p>Shared memory is a region of RAM that multiple processes can access simultaneously. Unlike message passing (pipes, sockets), shared memory provides:</p> <ul> <li>Zero-copy: Data stays in one place, no copying between processes</li> <li>Direct access: Read/write like normal memory</li> <li>High performance: Nanosecond latency vs microseconds for IPC</li> </ul> <pre><code>graph LR\n    A[Process A] --&gt;|mmap| C[Shared Memory]\n    B[Process B] --&gt;|mmap| C\n    D[Process C] --&gt;|mmap| C\n\n    style C fill:#f9f,stroke:#333,stroke-width:4px</code></pre>"},{"location":"getting-started/concepts/#posix-shared-memory","title":"POSIX Shared Memory","text":"<p>ZeroIPC uses POSIX shared memory (<code>shm_open</code>, <code>mmap</code>):</p> <ul> <li>Named segments: Global names like \"/sensor_data\"</li> <li>File-backed: Lives in <code>/dev/shm</code> on Linux</li> <li>Persistent: Survives process crashes</li> <li>Permissions: Standard Unix file permissions</li> </ul> <p>Listing Shared Memory</p> <p>On Linux, shared memory appears as files: <pre><code>ls -lh /dev/shm\n</code></pre></p>"},{"location":"getting-started/concepts/#zeroipc-architecture","title":"ZeroIPC Architecture","text":""},{"location":"getting-started/concepts/#memory-layout","title":"Memory Layout","text":"<p>Every ZeroIPC shared memory segment has this structure:</p> <pre><code>+------------------+  \u2190 Offset 0\n| Table Header     |  16 bytes: magic, version, entry_count, next_offset\n+------------------+\n| Table Entry 0    |  40 bytes: name[32], offset, size\n+------------------+\n| Table Entry 1    |  40 bytes\n+------------------+\n| ...              |\n+------------------+  \u2190 First structure offset\n| Structure 1      |  Variable size\n| (e.g., Array)    |\n+------------------+\n| Structure 2      |  Variable size\n| (e.g., Queue)    |\n+------------------+\n| ...              |\n+------------------+\n</code></pre>"},{"location":"getting-started/concepts/#metadata-table","title":"Metadata Table","text":"<p>The metadata table is the registry of all structures in shared memory:</p>"},{"location":"getting-started/concepts/#table-header","title":"Table Header","text":"<pre><code>struct TableHeader {\n    uint32_t magic;         // 0x5A49504D ('ZIPM')\n    uint32_t version;       // Format version (currently 1)\n    uint32_t entry_count;   // Number of registered structures\n    uint32_t next_offset;   // Where to allocate next structure\n};\n</code></pre>"},{"location":"getting-started/concepts/#table-entry","title":"Table Entry","text":"<pre><code>struct TableEntry {\n    char     name[32];      // Structure name (null-terminated)\n    uint32_t offset;        // Offset from memory start\n    uint32_t size;          // Total allocated size\n};\n</code></pre> <p>Minimal Metadata</p> <p>Notice what's NOT stored: no type information! This enables language independence\u2014each language interprets data using its own type system.</p>"},{"location":"getting-started/concepts/#table-size-configuration","title":"Table Size Configuration","text":"<p>Tables have a fixed maximum number of entries, configured at creation:</p> <pre><code>// C++ offers predefined sizes\nzeroipc::memory&lt;table64&gt; mem1(...);    // 64 entries (default)\nzeroipc::memory&lt;table256&gt; mem2(...);   // 256 entries\nzeroipc::memory&lt;table1024&gt; mem3(...);  // 1024 entries\n</code></pre> <p>Available sizes: <code>table1</code>, <code>table4</code>, <code>table8</code>, <code>table16</code>, <code>table32</code>, <code>table64</code>, <code>table128</code>, <code>table256</code>, <code>table512</code>, <code>table1024</code>, <code>table2048</code>, <code>table4096</code>.</p> <p>Entry Limit</p> <p>Once the table is full, you cannot create more structures! Choose table size based on expected structure count.</p>"},{"location":"getting-started/concepts/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/concepts/#1-memory-segments","title":"1. Memory Segments","text":"<p>A memory segment is a named region of shared memory:</p> <pre><code>// Create or open \"/sensor_data\" with 10MB\nzeroipc::Memory mem(\"/sensor_data\", 10*1024*1024);\n</code></pre> <p>Key points: - Name must start with \"/\" - Size is rounded up to page size (typically 4KB) - First process creates, others open existing - Persists until explicitly deleted or system reboot</p>"},{"location":"getting-started/concepts/#2-data-structures","title":"2. Data Structures","text":"<p>Structures are allocated within a memory segment:</p> <pre><code>zeroipc::Memory mem(\"/data\", 1*1024*1024);\nzeroipc::Array&lt;float&gt; temps(mem, \"temperatures\", 1000);\nzeroipc::Queue&lt;int&gt; tasks(mem, \"task_queue\", 100);\n</code></pre> <p>Key points: - Each structure has a unique name (max 31 characters) - Allocated from the memory segment - Registered in the metadata table - Discoverable by other processes</p>"},{"location":"getting-started/concepts/#3-duck-typing","title":"3. Duck Typing","text":"<p>ZeroIPC uses duck typing\u2014no type information is stored:</p> <pre><code>// C++ creates as int array\nzeroipc::Array&lt;int&gt; numbers(mem, \"data\", 100);\n</code></pre> <pre><code># Python must specify correct type\nnumbers = Array(mem, \"data\", dtype=np.int32)  # Correct!\nnumbers = Array(mem, \"data\", dtype=np.float32)  # Wrong type!\n</code></pre> <p>Your responsibility: - Ensure type consistency across languages - Match type sizes (C++ <code>int</code> \u2248 Python <code>np.int32</code>) - Document shared data types</p>"},{"location":"getting-started/concepts/#4-structure-discovery","title":"4. Structure Discovery","text":"<p>Processes can discover existing structures:</p> <pre><code>// Process A creates\nzeroipc::Array&lt;float&gt; temps(mem, \"temps\", 1000);\n\n// Process B discovers and accesses\n// (if it knows the type and capacity)\nzeroipc::Array&lt;float&gt; temps(mem, \"temps\", 1000);\n</code></pre> <pre><code># Python process discovers\ntemps = Array(mem, \"temps\", dtype=np.float32)\n</code></pre>"},{"location":"getting-started/concepts/#5-lock-free-operations","title":"5. Lock-Free Operations","text":"<p>Most structures use lock-free algorithms:</p> <pre><code>// Queue uses CAS (Compare-And-Swap)\nqueue.enqueue(value);  // Lock-free, safe from multiple threads\n</code></pre> <p>Benefits: - No deadlocks - No priority inversion - Scalable across cores - Progress guarantees</p> <p>Trade-offs: - More complex implementation - May retry on contention - Requires atomic operations</p>"},{"location":"getting-started/concepts/#type-system","title":"Type System","text":""},{"location":"getting-started/concepts/#c-types","title":"C++ Types","text":"<p>C++ uses templates for compile-time type safety:</p> <pre><code>zeroipc::Array&lt;int&gt; ints(mem, \"ints\", 100);\nzeroipc::Array&lt;double&gt; doubles(mem, \"doubles\", 100);\nzeroipc::Queue&lt;MyStruct&gt; queue(mem, \"queue\", 50);\n</code></pre> <p>Requirements: - Types must be trivially copyable (no pointers, virtual functions, etc.) - Use POD (Plain Old Data) types - Padding may affect cross-language use</p>"},{"location":"getting-started/concepts/#python-types","title":"Python Types","text":"<p>Python uses NumPy dtypes for type specification:</p> <pre><code># Basic types\nints = Array(mem, \"ints\", dtype=np.int32)\nfloats = Array(mem, \"floats\", dtype=np.float64)\n\n# Structured arrays\ndtype = np.dtype([('x', np.float32), ('y', np.float32), ('z', np.float32)])\npoints = Array(mem, \"points\", dtype=dtype)\n</code></pre>"},{"location":"getting-started/concepts/#type-mapping","title":"Type Mapping","text":"C++ Python (NumPy) Size <code>int8_t</code> <code>np.int8</code> 1 byte <code>uint8_t</code> <code>np.uint8</code> 1 byte <code>int16_t</code> <code>np.int16</code> 2 bytes <code>uint16_t</code> <code>np.uint16</code> 2 bytes <code>int32_t</code> <code>np.int32</code> 4 bytes <code>uint32_t</code> <code>np.uint32</code> 4 bytes <code>int64_t</code> <code>np.int64</code> 8 bytes <code>uint64_t</code> <code>np.uint64</code> 8 bytes <code>float</code> <code>np.float32</code> 4 bytes <code>double</code> <code>np.float64</code> 8 bytes <p>Platform-Dependent Types</p> <p>Avoid <code>int</code>, <code>long</code>, <code>size_t</code> in cross-language scenarios\u2014use fixed-width types like <code>int32_t</code>, <code>int64_t</code>.</p>"},{"location":"getting-started/concepts/#synchronization","title":"Synchronization","text":""},{"location":"getting-started/concepts/#atomic-operations","title":"Atomic Operations","text":"<p>Arrays support atomic read-modify-write:</p> <pre><code>zeroipc::Array&lt;int&gt; counter(mem, \"counter\", 1);\n\n// Atomic increment\nint old = counter.fetch_add(0, 1);  // Atomically add 1 to counter[0]\n</code></pre>"},{"location":"getting-started/concepts/#synchronization-primitives","title":"Synchronization Primitives","text":"<p>ZeroIPC provides cross-process synchronization:</p> <pre><code>// Semaphore for mutual exclusion\nzeroipc::Semaphore mutex(mem, \"mutex\", 1);\nmutex.acquire();  // Lock\n// Critical section\nmutex.release();  // Unlock\n\n// Barrier for synchronization points\nzeroipc::Barrier barrier(mem, \"sync\", 4);  // 4 participants\nbarrier.wait();  // All 4 must reach this point\n\n// Latch for one-time countdown\nzeroipc::Latch ready(mem, \"ready\", 4);\nready.count_down();  // Each worker counts down\nready.wait();  // Main thread waits for all\n</code></pre>"},{"location":"getting-started/concepts/#memory-management","title":"Memory Management","text":""},{"location":"getting-started/concepts/#allocation","title":"Allocation","text":"<p>Memory is allocated using bump allocation:</p> <pre><code>zeroipc::Memory mem(\"/data\", 1*1024*1024);  // 1MB total\n\n// Each allocation moves next_offset forward\nzeroipc::Array&lt;int&gt; a1(mem, \"a1\", 100);    // Uses 400 bytes\nzeroipc::Array&lt;int&gt; a2(mem, \"a2\", 200);    // Uses 800 bytes\n// ... until memory is exhausted\n</code></pre> <p>Characteristics: - O(1) allocation time - No fragmentation during allocation - No deallocation\u2014structures persist</p> <p>No Deallocation</p> <p>ZeroIPC does not support deleting individual structures. To reclaim space, delete the entire segment.</p>"},{"location":"getting-started/concepts/#cleanup","title":"Cleanup","text":"<p>Shared memory persists until explicitly removed:</p> <pre><code># List segments\nls /dev/shm\n\n# Remove segment\nrm /dev/shm/sensor_data\n</code></pre> <p>Or programmatically:</p> <pre><code>// C++\nzeroipc::Memory::unlink(\"/sensor_data\");\n</code></pre> <pre><code># Python\nMemory.unlink(\"/sensor_data\")\n</code></pre>"},{"location":"getting-started/concepts/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/concepts/#1-name-your-segments-clearly","title":"1. Name Your Segments Clearly","text":"<pre><code>// Good\nzeroipc::Memory sensor_data(\"/sensor_data\", size);\nzeroipc::Memory user_cache(\"/app_user_cache\", size);\n\n// Bad\nzeroipc::Memory m(\"/x\", size);\nzeroipc::Memory data(\"/data\", size);  // Too generic\n</code></pre>"},{"location":"getting-started/concepts/#2-document-your-types","title":"2. Document Your Types","text":"<p>Create a header shared between languages:</p> <pre><code>// shared_types.h\nstruct SensorReading {\n    float temperature;\n    float humidity;\n    uint64_t timestamp;\n};\n</code></pre> <pre><code># shared_types.py\nSENSOR_READING_DTYPE = np.dtype([\n    ('temperature', np.float32),\n    ('humidity', np.float32),\n    ('timestamp', np.uint64),\n])\n</code></pre>"},{"location":"getting-started/concepts/#3-choose-appropriate-table-sizes","title":"3. Choose Appropriate Table Sizes","text":"<pre><code>// Few structures (&lt; 10)\nzeroipc::memory&lt;table16&gt; mem(...);\n\n// Moderate (10-50)\nzeroipc::memory&lt;table64&gt; mem(...);  // Default\n\n// Many (100+)\nzeroipc::memory&lt;table256&gt; mem(...);\n</code></pre>"},{"location":"getting-started/concepts/#4-handle-errors","title":"4. Handle Errors","text":"<pre><code>// Check if structure exists\nif (auto arr = zeroipc::Array&lt;int&gt;::open(mem, \"data\")) {\n    // Use arr\n} else {\n    // Doesn't exist, create it\n    zeroipc::Array&lt;int&gt; arr(mem, \"data\", 100);\n}\n</code></pre>"},{"location":"getting-started/concepts/#5-clean-up-test-memory","title":"5. Clean Up Test Memory","text":"<pre><code># In tests, use unique names\nstd::string name = \"/test_\" + std::to_string(getpid());\n</code></pre>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<p>Now that you understand the fundamentals:</p> <ul> <li>Tutorial - Hands-on guide to each structure</li> <li>Architecture - Deep dive into internals</li> <li>Best Practices - Tips and pitfalls</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide covers installing ZeroIPC for C++, C, and Python on various platforms.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#operating-system","title":"Operating System","text":"<ul> <li>Linux (Ubuntu 20.04+, Debian 10+, Fedora 33+, etc.)</li> <li>macOS 10.15+</li> <li>Any POSIX-compliant Unix system</li> <li>Windows via WSL2 or Cygwin</li> </ul>"},{"location":"getting-started/installation/#compiler-requirements","title":"Compiler Requirements","text":"Language Minimum Version Recommended C++ GCC 12+ or Clang 15+ GCC 13+ or Clang 17+ C GCC 9+ or Clang 10+ GCC 11+ or Clang 14+ Python Python 3.8+ Python 3.10+ <p>C++23 Required</p> <p>The C++ implementation requires C++23 support. Ensure your compiler is up-to-date.</p>"},{"location":"getting-started/installation/#quick-install","title":"Quick Install","text":"C++PythonC <pre><code># Clone repository\ngit clone https://github.com/yourusername/zeroipc.git\ncd zeroipc/cpp\n\n# Configure and build\ncmake -B build -DCMAKE_BUILD_TYPE=Release .\ncmake --build build\n\n# Install (optional)\nsudo cmake --install build\n</code></pre> <pre><code># Install from PyPI (when available)\npip install zeroipc\n\n# Or install from source\ngit clone https://github.com/yourusername/zeroipc.git\ncd zeroipc/python\npip install -e .\n</code></pre> <pre><code># Clone repository\ngit clone https://github.com/yourusername/zeroipc.git\ncd zeroipc/c\n\n# Build static library\nmake\n\n# Install (optional)\nsudo make install\n</code></pre>"},{"location":"getting-started/installation/#detailed-installation-instructions","title":"Detailed Installation Instructions","text":""},{"location":"getting-started/installation/#c-installation","title":"C++ Installation","text":""},{"location":"getting-started/installation/#1-install-dependencies","title":"1. Install Dependencies","text":"Ubuntu/DebianFedora/RHELmacOS <pre><code>sudo apt update\nsudo apt install -y \\\n    build-essential \\\n    cmake \\\n    libgtest-dev \\\n    g++-13\n\n# Set GCC 13 as default\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-13 100\nsudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-13 100\n</code></pre> <pre><code>sudo dnf install -y \\\n    gcc-c++ \\\n    cmake \\\n    gtest-devel\n</code></pre> <pre><code># Install Homebrew if not already installed\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install dependencies\nbrew install cmake googletest llvm\n\n# Use LLVM's clang (for C++23 support)\nexport CC=/usr/local/opt/llvm/bin/clang\nexport CXX=/usr/local/opt/llvm/bin/clang++\n</code></pre>"},{"location":"getting-started/installation/#2-build-zeroipc","title":"2. Build ZeroIPC","text":"<pre><code># Clone the repository\ngit clone https://github.com/yourusername/zeroipc.git\ncd zeroipc/cpp\n\n# Configure CMake\ncmake -B build \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_CXX_STANDARD=23 \\\n    -DBUILD_TESTS=ON \\\n    .\n\n# Build\ncmake --build build -j$(nproc)\n\n# Run tests to verify installation\ncd build &amp;&amp; ctest --output-on-failure\n</code></pre>"},{"location":"getting-started/installation/#3-install-optional","title":"3. Install (Optional)","text":"<pre><code># Install to /usr/local\nsudo cmake --install build\n\n# Or install to custom location\ncmake --install build --prefix /opt/zeroipc\n</code></pre>"},{"location":"getting-started/installation/#4-use-in-your-project","title":"4. Use in Your Project","text":"<p>ZeroIPC is header-only, so you can:</p> <p>Option 1: Direct include <pre><code># Copy headers to your project\ncp -r zeroipc/cpp/include/zeroipc /path/to/your/project/include/\n</code></pre></p> <p>Option 2: CMake find_package <pre><code># In your CMakeLists.txt\nfind_package(zeroipc REQUIRED)\ntarget_link_libraries(your_target PRIVATE zeroipc::zeroipc)\n</code></pre></p> <p>Option 3: CMake add_subdirectory <pre><code># In your CMakeLists.txt\nadd_subdirectory(external/zeroipc/cpp)\ntarget_link_libraries(your_target PRIVATE zeroipc::zeroipc)\n</code></pre></p>"},{"location":"getting-started/installation/#python-installation","title":"Python Installation","text":""},{"location":"getting-started/installation/#1-install-python-dependencies","title":"1. Install Python Dependencies","text":"<pre><code># Install NumPy\npip install numpy\n\n# For development\npip install numpy pytest pytest-cov black mypy ruff\n</code></pre>"},{"location":"getting-started/installation/#2-install-zeroipc","title":"2. Install ZeroIPC","text":"From PyPI (Recommended)From Source <pre><code>pip install zeroipc\n</code></pre> <pre><code>git clone https://github.com/yourusername/zeroipc.git\ncd zeroipc/python\n\n# Development install (editable)\npip install -e .\n\n# Or regular install\npip install .\n</code></pre>"},{"location":"getting-started/installation/#3-verify-installation","title":"3. Verify Installation","text":"<pre><code>import zeroipc\nprint(zeroipc.__version__)\n\nfrom zeroipc import Memory, Array\n# If no errors, installation successful!\n</code></pre>"},{"location":"getting-started/installation/#c-installation_1","title":"C Installation","text":""},{"location":"getting-started/installation/#1-build-static-library","title":"1. Build Static Library","text":"<pre><code>git clone https://github.com/yourusername/zeroipc.git\ncd zeroipc/c\n\n# Build\nmake\n\n# Run tests\nmake test\n</code></pre>"},{"location":"getting-started/installation/#2-install","title":"2. Install","text":"<pre><code># Install to /usr/local\nsudo make install\n\n# Or specify prefix\nmake install PREFIX=/opt/zeroipc\n</code></pre> <p>This installs: - <code>libzeroipc.a</code> to <code>$PREFIX/lib/</code> - Headers to <code>$PREFIX/include/zeroipc/</code></p>"},{"location":"getting-started/installation/#3-link-in-your-project","title":"3. Link in Your Project","text":"<pre><code>gcc -o myapp myapp.c -lzeroipc -lrt -lpthread\n</code></pre> <p>Or in a Makefile: <pre><code>CFLAGS = -std=c99 -Wall\nLDFLAGS = -lzeroipc -lrt -lpthread\n\nmyapp: myapp.c\n    $(CC) $(CFLAGS) -o $@ $&lt; $(LDFLAGS)\n</code></pre></p>"},{"location":"getting-started/installation/#building-cli-tools","title":"Building CLI Tools","text":"<p>The <code>zeroipc</code> CLI tool is built automatically with the C++ library:</p> <pre><code>cd cpp\ncmake -B build .\ncmake --build build\n\n# The tool will be at build/tools/zeroipc\n./build/tools/zeroipc --help\n</code></pre> <p>To install globally: <pre><code>sudo cp build/tools/zeroipc /usr/local/bin/\n</code></pre></p>"},{"location":"getting-started/installation/#verification","title":"Verification","text":""},{"location":"getting-started/installation/#c-verification","title":"C++ Verification","text":"<p>Create a test file <code>test_install.cpp</code>:</p> <pre><code>#include &lt;zeroipc/memory.h&gt;\n#include &lt;zeroipc/array.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    zeroipc::Memory mem(\"/test\", 1024*1024);\n    zeroipc::Array&lt;int&gt; arr(mem, \"numbers\", 10);\n    arr[0] = 42;\n    std::cout &lt;&lt; \"ZeroIPC works! Value: \" &lt;&lt; arr[0] &lt;&lt; std::endl;\n    return 0;\n}\n</code></pre> <p>Compile and run: <pre><code>g++ -std=c++23 -o test_install test_install.cpp -lrt -lpthread\n./test_install\n# Output: ZeroIPC works! Value: 42\n</code></pre></p>"},{"location":"getting-started/installation/#python-verification","title":"Python Verification","text":"<pre><code>from zeroipc import Memory, Array\nimport numpy as np\n\nmem = Memory(\"/test\", 1024*1024)\narr = Array(mem, \"numbers\", dtype=np.int32, capacity=10)\narr[0] = 42\nprint(f\"ZeroIPC works! Value: {arr[0]}\")\n# Output: ZeroIPC works! Value: 42\n</code></pre>"},{"location":"getting-started/installation/#c-verification_1","title":"C Verification","text":"<p>Create <code>test_install.c</code>:</p> <pre><code>#include &lt;zeroipc/memory.h&gt;\n#include &lt;zeroipc/array.h&gt;\n#include &lt;stdio.h&gt;\n\nint main() {\n    zipc_memory_t* mem = zipc_memory_create(\"/test\", 1024*1024);\n    zipc_array_t* arr = zipc_array_create(mem, \"numbers\", sizeof(int), 10);\n\n    int value = 42;\n    zipc_array_set(arr, 0, &amp;value);\n\n    int result;\n    zipc_array_get(arr, 0, &amp;result);\n\n    printf(\"ZeroIPC works! Value: %d\\n\", result);\n\n    zipc_array_destroy(arr);\n    zipc_memory_destroy(mem);\n    return 0;\n}\n</code></pre> <p>Compile and run: <pre><code>gcc -std=c99 -o test_install test_install.c -lzeroipc -lrt -lpthread\n./test_install\n# Output: ZeroIPC works! Value: 42\n</code></pre></p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#issue-c23-features-not-available","title":"Issue: \"C++23 features not available\"","text":"<p>Solution: Update your compiler: <pre><code># Ubuntu\nsudo apt install g++-13\n\n# macOS\nbrew install llvm\nexport CXX=/usr/local/opt/llvm/bin/clang++\n</code></pre></p>"},{"location":"getting-started/installation/#issue-cannot-find-lrt","title":"Issue: \"Cannot find -lrt\"","text":"<p>Solution: The realtime library is not available. On most Linux systems: <pre><code># Usually part of glibc, but ensure it's installed\nsudo apt install libc6-dev\n</code></pre></p> <p>On macOS, <code>-lrt</code> is not needed (POSIX shared memory is in libSystem).</p>"},{"location":"getting-started/installation/#issue-permission-denied-when-creating-shared-memory","title":"Issue: \"Permission denied when creating shared memory\"","text":"<p>Solution: Check <code>/dev/shm</code> permissions: <pre><code>ls -la /dev/shm\n# Should be drwxrwxrwt\n\n# If not, fix permissions\nsudo chmod 1777 /dev/shm\n</code></pre></p>"},{"location":"getting-started/installation/#issue-python-import-fails","title":"Issue: \"Python import fails\"","text":"<p>Solution: Ensure NumPy is installed: <pre><code>pip install numpy\n</code></pre></p>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the FAQ</li> <li>Search GitHub Issues</li> <li>Create a new issue with:</li> <li>OS and version</li> <li>Compiler version</li> <li>Error message</li> <li>Minimal reproduction code</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Installation complete! Now:</p> <ol> <li>Quick Start - Write your first program</li> <li>Tutorial - Learn all the features</li> <li>Examples - See real-world usage</li> </ol>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get up and running with ZeroIPC in 5 minutes! This guide shows you the fastest path from installation to your first working program.</p>"},{"location":"getting-started/quick-start/#your-first-shared-memory-program","title":"Your First Shared Memory Program","text":"<p>Let's create a simple producer-consumer example where a C++ program writes data and a Python program reads it.</p>"},{"location":"getting-started/quick-start/#step-1-create-the-producer-c","title":"Step 1: Create the Producer (C++)","text":"<p>Create a file called <code>producer.cpp</code>:</p> <pre><code>#include &lt;zeroipc/memory.h&gt;\n#include &lt;zeroipc/array.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n    // Create 1MB of shared memory named \"/demo\"\n    zeroipc::Memory mem(\"/demo\", 1024 * 1024);\n\n    // Create an array of 100 integers named \"numbers\"\n    zeroipc::Array&lt;int&gt; numbers(mem, \"numbers\", 100);\n\n    // Fill the array with values\n    for (int i = 0; i &lt; 100; ++i) {\n        numbers[i] = i * 2;  // Even numbers\n    }\n\n    std::cout &lt;&lt; \"Producer: Created array with 100 numbers\\n\";\n    std::cout &lt;&lt; \"Producer: First value = \" &lt;&lt; numbers[0] &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; \"Producer: Last value = \" &lt;&lt; numbers[99] &lt;&lt; \"\\n\";\n    std::cout &lt;&lt; \"Producer: Keeping shared memory alive. Press Ctrl+C to exit.\\n\";\n\n    // Keep running so consumer can read\n    std::this_thread::sleep_for(std::chrono::seconds(60));\n\n    return 0;\n}\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-compile-the-producer","title":"Step 2: Compile the Producer","text":"<pre><code>g++ -std=c++23 -o producer producer.cpp -lrt -lpthread\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-create-the-consumer-python","title":"Step 3: Create the Consumer (Python)","text":"<p>Create a file called <code>consumer.py</code>:</p> <pre><code>from zeroipc import Memory, Array\nimport numpy as np\n\n# Open the existing shared memory\nmem = Memory(\"/demo\")\n\n# Access the array (specify type as int32)\nnumbers = Array(mem, \"numbers\", dtype=np.int32)\n\nprint(f\"Consumer: Found array with {len(numbers)} numbers\")\nprint(f\"Consumer: First value = {numbers[0]}\")\nprint(f\"Consumer: Last value = {numbers[99]}\")\nprint(f\"Consumer: Sum of all values = {np.sum(numbers[:])}\")\n\n# Verify the values\nassert numbers[0] == 0\nassert numbers[50] == 100\nassert numbers[99] == 198\n\nprint(\"Consumer: All values correct!\")\n</code></pre>"},{"location":"getting-started/quick-start/#step-4-run-it","title":"Step 4: Run It!","text":"<p>In one terminal: <pre><code>./producer\n</code></pre></p> <p>In another terminal (while producer is running): <pre><code>python consumer.py\n</code></pre></p> <p>You should see:</p> <pre><code># Producer terminal\nProducer: Created array with 100 numbers\nProducer: First value = 0\nProducer: Last value = 198\nProducer: Keeping shared memory alive. Press Ctrl+C to exit.\n\n# Consumer terminal\nConsumer: Found array with 100 numbers\nConsumer: First value = 0\nConsumer: Last value = 198\nConsumer: Sum of all values = 9900\nConsumer: All values correct!\n</code></pre> <p>Congratulations! You just created your first cross-language shared memory communication!</p>"},{"location":"getting-started/quick-start/#understanding-what-happened","title":"Understanding What Happened","text":"<p>Let's break down what just occurred:</p> <ol> <li>Memory Creation: The C++ producer created a shared memory segment named \"/demo\"</li> <li>Structure Registration: An array called \"numbers\" was registered in the metadata table</li> <li>Data Writing: The producer filled the array with even numbers</li> <li>Cross-Language Access: Python opened the same memory and read the data</li> <li>Type Specification: Python used NumPy's <code>int32</code> to match C++'s <code>int</code></li> </ol>"},{"location":"getting-started/quick-start/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/quick-start/#shared-memory-name","title":"Shared Memory Name","text":"<p><pre><code>zeroipc::Memory mem(\"/demo\", 1024 * 1024);\n</code></pre> The name <code>/demo</code> is global on the system. Any process can access it.</p>"},{"location":"getting-started/quick-start/#structure-name","title":"Structure Name","text":"<p><pre><code>zeroipc::Array&lt;int&gt; numbers(mem, \"numbers\", 100);\n</code></pre> Structures are registered with names in the metadata table for discovery.</p>"},{"location":"getting-started/quick-start/#type-consistency","title":"Type Consistency","text":"<p><pre><code>// C++: int (typically 32-bit)\nzeroipc::Array&lt;int&gt; numbers(mem, \"numbers\", 100);\n</code></pre> <pre><code># Python: must match with np.int32\nnumbers = Array(mem, \"numbers\", dtype=np.int32)\n</code></pre></p> <p>Type Matching</p> <p>You must ensure type sizes match between languages! C++ <code>int</code> = Python <code>np.int32</code>, C++ <code>double</code> = Python <code>np.float64</code>.</p>"},{"location":"getting-started/quick-start/#try-it-bidirectional-communication","title":"Try It: Bidirectional Communication","text":"<p>Now let's make it bidirectional\u2014both processes can read and write!</p>"},{"location":"getting-started/quick-start/#modified-producer","title":"Modified Producer","text":"<pre><code>#include &lt;zeroipc/memory.h&gt;\n#include &lt;zeroipc/queue.h&gt;\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n#include &lt;chrono&gt;\n\nint main() {\n    zeroipc::Memory mem(\"/chat\", 1024 * 1024);\n\n    // Create two queues for bidirectional messaging\n    zeroipc::Queue&lt;int&gt; to_python(mem, \"to_python\", 10);\n    zeroipc::Queue&lt;int&gt; from_python(mem, \"from_python\", 10);\n\n    // Send messages to Python\n    for (int i = 1; i &lt;= 5; ++i) {\n        to_python.enqueue(i * 10);\n        std::cout &lt;&lt; \"C++: Sent \" &lt;&lt; i * 10 &lt;&lt; std::endl;\n        std::this_thread::sleep_for(std::chrono::milliseconds(500));\n    }\n\n    // Receive messages from Python\n    for (int i = 0; i &lt; 5; ++i) {\n        auto msg = from_python.dequeue();\n        if (msg) {\n            std::cout &lt;&lt; \"C++: Received \" &lt;&lt; *msg &lt;&lt; std::endl;\n        }\n        std::this_thread::sleep_for(std::chrono::milliseconds(500));\n    }\n\n    return 0;\n}\n</code></pre>"},{"location":"getting-started/quick-start/#modified-consumer","title":"Modified Consumer","text":"<pre><code>from zeroipc import Memory, Queue\nimport numpy as np\nimport time\n\nmem = Memory(\"/chat\")\n\nto_python = Queue(mem, \"to_python\", dtype=np.int32)\nfrom_python = Queue(mem, \"from_python\", dtype=np.int32)\n\n# Receive from C++\nfor _ in range(5):\n    msg = to_python.dequeue()\n    if msg is not None:\n        print(f\"Python: Received {msg}\")\n    time.sleep(0.5)\n\n# Send to C++\nfor i in range(1, 6):\n    from_python.enqueue(i * 100)\n    print(f\"Python: Sent {i * 100}\")\n    time.sleep(0.5)\n</code></pre> <p>Compile and run: <pre><code># Terminal 1\ng++ -std=c++23 -o chat producer_chat.cpp -lrt -lpthread\n./chat\n\n# Terminal 2\npython consumer_chat.py\n</code></pre></p>"},{"location":"getting-started/quick-start/#inspecting-with-the-cli-tool","title":"Inspecting with the CLI Tool","text":"<p>ZeroIPC comes with a powerful CLI tool for inspection:</p> <pre><code># List all shared memory segments\nzeroipc list\n\n# Show structures in /demo\nzeroipc show /demo\n\n# Inspect the array\nzeroipc array /demo numbers\n\n# Interactive REPL mode\nzeroipc -r\n</code></pre> <p>In REPL mode: <pre><code>zeroipc&gt; ls /\ndemo/        1.0 MB      1 structures\nchat/        1.0 MB      2 structures\n\nzeroipc&gt; cd /demo\n/demo&gt; ls\nnumbers      array&lt;int&gt;[100]      400 bytes\n\n/demo&gt; cd /\nzeroipc&gt; quit\n</code></pre></p>"},{"location":"getting-started/quick-start/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quick-start/#pattern-1-single-producer-multiple-consumers","title":"Pattern 1: Single Producer, Multiple Consumers","text":"<p>One process writes, many processes read:</p> <pre><code>// Producer\nzeroipc::Memory mem(\"/data\", 10*1024*1024);\nzeroipc::Array&lt;float&gt; sensors(mem, \"temp\", 1000);\n\n// Update continuously\nwhile (running) {\n    sensors[0] = read_sensor();\n}\n</code></pre> <pre><code># Consumer 1: Logger\nmem = Memory(\"/data\")\ntemps = Array(mem, \"temp\", dtype=np.float32)\nwhile True:\n    log_temperature(temps[0])\n\n# Consumer 2: Monitor\nmem = Memory(\"/data\")\ntemps = Array(mem, \"temp\", dtype=np.float32)\nwhile True:\n    if temps[0] &gt; 100:\n        send_alert()\n</code></pre>"},{"location":"getting-started/quick-start/#pattern-2-work-queue","title":"Pattern 2: Work Queue","text":"<p>Distribute tasks across worker processes:</p> <pre><code>// Manager\nzeroipc::Memory mem(\"/tasks\", 10*1024*1024);\nzeroipc::Queue&lt;Task&gt; tasks(mem, \"work\", 100);\n\nfor (auto&amp; task : all_tasks) {\n    tasks.enqueue(task);\n}\n</code></pre> <pre><code># Worker (many of these)\nmem = Memory(\"/tasks\")\ntasks = Queue(mem, \"work\", dtype=task_dtype)\n\nwhile True:\n    task = tasks.dequeue()\n    if task:\n        process(task)\n</code></pre>"},{"location":"getting-started/quick-start/#pattern-3-reactive-stream","title":"Pattern 3: Reactive Stream","text":"<p>Event-driven processing with transformations:</p> <pre><code>zeroipc::Memory mem(\"/events\", 10*1024*1024);\nzeroipc::Stream&lt;Event&gt; events(mem, \"stream\", 1000);\n\n// Create derived streams\nauto filtered = events.filter(mem, \"important\", \n    [](Event&amp; e) { return e.priority &gt; 5; });\n\nauto transformed = filtered.map(mem, \"alerts\",\n    [](Event&amp; e) { return Alert{e}; });\n</code></pre>"},{"location":"getting-started/quick-start/#whats-next","title":"What's Next?","text":"<p>You now know the basics! Here's where to go from here:</p> <ol> <li>Tutorial - Deep dive into all data structures</li> <li>CLI Tool Guide - Master the virtual filesystem interface</li> <li>API Reference - Complete API documentation</li> <li>Examples - Real-world usage patterns</li> <li>Best Practices - Tips and pitfalls</li> </ol>"},{"location":"getting-started/quick-start/#recommended-learning-path","title":"Recommended Learning Path","text":"<ul> <li>Beginners: Tutorial \u2192 Examples \u2192 Best Practices</li> <li>Intermediate: API Reference \u2192 CLI Tool \u2192 Advanced Topics</li> <li>Experts: Architecture \u2192 Lock-Free Patterns \u2192 Contributing</li> </ul> <p>Happy coding with ZeroIPC!</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>Welcome to the ZeroIPC tutorial! This hands-on guide will take you from basics to advanced usage through practical examples.</p>"},{"location":"tutorial/#what-youll-learn","title":"What You'll Learn","text":"<p>This tutorial covers:</p> <ol> <li>Your First Shared Memory - Create and access shared memory</li> <li>Working with Arrays - Store and access array data</li> <li>Using Queues and Stacks - FIFO and LIFO data structures</li> <li>Reactive Streams - Event-driven programming with streams</li> <li>Synchronization Primitives - Semaphores, barriers, and latches</li> <li>Advanced Patterns - Real-world usage patterns</li> </ol>"},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":"<p>Before starting:</p> <ul> <li>Completed: Installation</li> <li>Completed: Quick Start</li> <li>Understood: Basic Concepts</li> </ul>"},{"location":"tutorial/#tutorial-structure","title":"Tutorial Structure","text":"<p>Each lesson follows this structure:</p> <ol> <li>Concept - What you'll learn</li> <li>Code - Working examples in C++ and Python</li> <li>Explanation - How it works</li> <li>Exercise - Try it yourself</li> <li>Common Pitfalls - What to avoid</li> </ol>"},{"location":"tutorial/#learning-path","title":"Learning Path","text":""},{"location":"tutorial/#beginners","title":"Beginners","text":"<p>Start from the beginning and work through in order: 1. First Shared Memory 2. Working with Arrays 3. Using Queues and Stacks 4. Synchronization Primitives</p>"},{"location":"tutorial/#intermediate","title":"Intermediate","text":"<p>If you're familiar with shared memory: 1. Skim First Shared Memory 2. Focus on Reactive Streams 3. Study Advanced Patterns</p>"},{"location":"tutorial/#language-specific","title":"Language-Specific","text":"<p>C++ Developers: - Focus on template usage - Pay attention to memory ordering - Study lock-free implementations</p> <p>Python Developers: - Understand NumPy dtype mapping - Focus on duck typing examples - Note type consistency requirements</p>"},{"location":"tutorial/#example-project","title":"Example Project","text":"<p>Throughout the tutorial, we'll build a complete example: a real-time sensor monitoring system with:</p> <ul> <li>C++ sensor simulator (producer)</li> <li>Python data processor (consumer)</li> <li>Real-time visualization</li> <li>Alert generation</li> <li>Historical data storage</li> </ul> <p>By the end, you'll have a working multi-process application!</p>"},{"location":"tutorial/#code-examples","title":"Code Examples","text":"<p>All examples are available in the repository:</p> <pre><code>cd examples/tutorial/\nls -la\n# lesson01_first_memory/\n# lesson02_arrays/\n# lesson03_queues_stacks/\n# lesson04_streams/\n# lesson05_sync/\n# lesson06_advanced/\n</code></pre> <p>Each lesson includes: - Working C++ code - Working Python code - Build scripts - README with instructions</p>"},{"location":"tutorial/#tips-for-success","title":"Tips for Success","text":"<ol> <li>Type along - Don't just read, write the code yourself</li> <li>Experiment - Modify examples and see what happens</li> <li>Use the CLI - Inspect structures with <code>zeroipc</code> as you go</li> <li>Read errors - Error messages are helpful, not punishing</li> <li>Ask questions - Check GitHub Discussions if stuck</li> </ol>"},{"location":"tutorial/#common-questions","title":"Common Questions","text":"<p>Q: Do I need to know both C++ and Python?</p> <p>A: No! Pick one language and focus on those examples. The concepts apply to both.</p> <p>Q: Can I skip lessons?</p> <p>A: Yes, but each lesson builds on previous ones. Skipping may cause confusion.</p> <p>Q: How long does the tutorial take?</p> <p>A: About 2-3 hours for all lessons, depending on your pace.</p> <p>Q: What if I get stuck?</p> <p>A: Check the Common Pitfalls page and GitHub Issues.</p>"},{"location":"tutorial/#lets-begin","title":"Let's Begin!","text":"<p>Ready to start? Head to Your First Shared Memory to begin the tutorial!</p>"}]}